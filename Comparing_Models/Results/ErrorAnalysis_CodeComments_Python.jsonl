{"code": "def dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r'qualities\":({.+?}),\"' ) ) title = match1 ( html , r'\"video_title\"\\s*:\\s*\"([^\"]+)\"' ) or match1 ( html , r'\"title\"\\s*:\\s*\"([^\"]+)\"' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real_url = info [ quality ] [ 1 ] [ ] if real_url : break except KeyError : pass mime , ext , size = url_info ( real_url ) print_info ( <mask> , title , mime , size ) if not info_only : download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge )", "predicted": "info", "expected": "site_info"}
{"code": "def sina_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : if 'news.sina.com.cn/zxt' in url : sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) return vid = match1 ( url , r'vid=(\\d+)' ) if vid is None : video_page = get_content ( url ) vid = hd_vid = match1 ( video_page , r'hd_vid\\s*:\\s*\\'([^\\']+)\\'' ) if hd_vid == '0' : vids = match1 ( video_page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video_page , r'vid:\"?(\\d+)\"?' ) if vid : sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) else : vkey = match1 ( video_page , <mask> ) if vkey is None : vid = match1 ( url , r'#(\\d+)' ) sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) return title = match1 ( video_page , r'title\\s*:\\s*\"([^\"]+)\"' ) sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only )", "predicted": "url", "expected": "r'vkey\\s*:\\s*\"([^\"]+)\"'"}
{"code": "def sprint ( text , * colors ) : return . <mask> ( . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS_ANSI_TERMINAL and colors else text", "predicted": "colored", "expected": "format"}
{"code": "def e ( message , exit_code = None ) : print_log ( message , <mask> , BOLD ) if exit_code is not None : sys . exit ( exit_code )", "predicted": "e", "expected": "YELLOW"}
{"code": "def <mask> ( message , exit_code = 1 ) : print_log ( message , RED , BOLD ) if exit_code is not None : sys . exit ( exit_code )", "predicted": "exit", "expected": "wtf"}
{"code": "def detect_os ( ) : <mask> = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os", "predicted": "system", "expected": "syst"}
{"code": "def get_mgtv_real_url ( url ) : content = loads ( get_content ( url ) ) m3u_url = content [ 'info' ] split = urlsplit ( m3u_url ) base_url = <mask> format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2 ] ) ) content = get_content ( content [ 'info' ] ) segment_list = [ ] segments_size = 0 for i in content . split ( ) : if not i . startswith ( '#' ) : segment_list . append ( base_url + i ) elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : segments_size += int ( i [ i . rfind ( ':' ) + 1 : ] ) return m3u_url , segments_size , segment_list", "predicted": "url", "expected": "."}
{"code": "def post_content ( url , headers = { } , post_data = { } , decoded = True , ** kwargs ) : if kwargs . get ( 'post_data_raw' ) : logging . debug ( 'post_content: %s\\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) else : logging . debug ( 'post_content: %s\\npost_data: %s' % ( url , post_data ) ) req = <mask> . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) if kwargs . get ( 'post_data_raw' ) : post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) else : post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) response = urlopen_with_retry ( req , data = post_data_enc ) data = response . read ( ) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) if decoded : charset = match1 ( response . getheader ( 'Content-Type' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset ) else : data = data . decode ( 'utf-8' ) return data", "predicted": "requests", "expected": "request"}
{"code": "def showroom_get_roomid_by_room_url_key ( room_url_key ) : fake_headers_mobile = { <mask> : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' , 'Accept-Charset' : 'UTF-8,*;q=0.5' , 'Accept-Encoding' : 'gzip,deflate,sdch' , 'Accept-Language' : 'en-US,en;q=0.8' , 'User-Agent' : 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36' } webpage_url = 'https://www.showroom-live.com/' + room_url_key html = get_content ( webpage_url , headers = fake_headers_mobile ) roomid = match1 ( html , r'room\\?room_id\\=(\\d+)' ) assert roomid return roomid", "predicted": "''", "expected": "'Accept'"}
{"code": "def _get_aws_credentials ( self ) : if self . snowflake_conn_id : <mask> = self . get_connection ( self . snowflake_conn_id ) if 'aws_secret_access_key' in connection_object . extra_dejson : aws_access_key_id = connection_object . extra_dejson . get ( 'aws_access_key_id' ) aws_secret_access_key = connection_object . extra_dejson . get ( 'aws_secret_access_key' ) return aws_access_key_id , aws_secret_access_key", "predicted": "connection", "expected": "connection_object"}
{"code": "def _get_field ( self , field_name , default = None ) : <mask> = 'extra__grpc__{}' . format ( field_name ) if full_field_name in self . extras : return self . extras [ full_field_name ] else : return default", "predicted": "name", "expected": "full_field_name"}
{"code": "def get_conn ( self ) : conn = self . get_connection ( self . mysql_conn_id ) conn_config = { : conn . login , : conn . password or '' , : conn . host or 'localhost' , : self . schema or conn . schema or '' } if not conn . port : conn_config [ ] = 3306 else : conn_config [ ] = int ( conn . port ) if conn . extra_dejson . get ( 'charset' , False ) : conn_config [ ] = conn . extra_dejson [ ] if ( conn_config [ ] ) . lower ( ) == 'utf8' or ( conn_config [ ] ) . lower ( ) == 'utf-8' : conn_config [ ] = True if conn . extra_dejson . get ( 'cursor' , False ) : if ( conn . extra_dejson [ ] ) . lower ( ) == 'sscursor' : conn_config [ ] = MySQLdb . cursors . SSCursor elif ( conn . extra_dejson [ ] ) . lower ( ) == 'dictcursor' : conn_config [ ] = MySQLdb . cursors . DictCursor elif ( conn . extra_dejson [ ] ) . lower ( ) == <mask> : conn_config [ ] = MySQLdb . cursors . SSDictCursor local_infile = conn . extra_dejson . get ( 'local_infile' , False ) if conn . extra_dejson . get ( 'ssl' , False ) : dejson_ssl = conn . extra_dejson [ 'ssl' ] if isinstance ( dejson_ssl , six . string_types ) : dejson_ssl = json . loads ( dejson_ssl ) conn_config [ 'ssl' ] = dejson_ssl if conn . extra_dejson . get ( 'unix_socket' ) : conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] if local_infile : conn_config [ ] = 1 conn = MySQLdb . connect ( ** conn_config ) return conn", "predicted": "''", "expected": "'ssdictcursor'"}
{"code": "def create_instance ( self , body , <mask> = None ) : response = self . get_conn ( ) . instances ( ) . insert ( project = project_id , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )", "predicted": "timeout", "expected": "project_id"}
{"code": "def get_database ( self , instance , database , project_id = None ) : return self . get_conn ( ) . databases ( ) . get ( project = project_id , instance = instance , database = database ) . <mask> ( num_retries = self . num_retries )", "predicted": "get", "expected": "execute"}
{"code": "def get_proxy_version ( self ) : self . _download_sql_proxy_if_needed ( ) command_to_run = [ self . <mask> ] command_to_run . extend ( [ '--version' ] ) command_to_run . extend ( self . _get_credential_parameters ( ) ) result = subprocess . check_output ( command_to_run ) . decode ( 'utf-8' ) pattern = re . compile ( ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None", "predicted": "command", "expected": "sql_proxy_path"}
{"code": "def get_database_hook ( self ) : if self . <mask> == 'postgres' : self . db_hook = PostgresHook ( postgres_conn_id = self . db_conn_id , schema = self . database ) else : self . db_hook = MySqlHook ( mysql_conn_id = self . db_conn_id , schema = self . database ) return self . db_hook", "predicted": "database", "expected": "database_type"}
{"code": "def _normalize_mlengine_job_id ( job_id ) : match = re . search ( r'\\d|\\{{2}' , job_id ) if match and match . start ( ) == 0 : job = 'z_{}' . format ( job_id ) else : job = job_id tracker = 0 <mask> = '' for m in re . finditer ( r'\\{{2}.+?\\}{2}' , job ) : cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : m . start ( ) ] ) cleansed_job_id += job [ m . start ( ) : m . end ( ) ] tracker = m . end ( ) cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : ] ) return cleansed_job_id", "predicted": "result", "expected": "cleansed_job_id"}
{"code": "def set_dags_paused_state ( <mask> ) : session = settings . Session ( ) dms = session . query ( DagModel ) . filter ( DagModel . dag_id . in_ ( DAG_IDS ) ) for dm in dms : logging . info ( 'Setting DAG :: {} is_paused={}' . format ( dm , is_paused ) ) dm . is_paused = is_paused session . commit ( )", "predicted": "self", "expected": "is_paused"}
{"code": "def print_stats ( self ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) successful_tis = [ x for x in tis if x . state == State . SUCCESS ] ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . <mask> ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) print ( 'Performance Results' ) print ( '###################' ) for dag_id in DAG_IDS : print ( 'DAG {}' . format ( dag_id ) ) print ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) print ( '###################' ) if len ( tis ) > len ( successful_tis ) : print ( ) print ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) session . commit ( )", "predicted": "seconds", "expected": "total_seconds"}
{"code": "def get_conn ( self ) : conn = self . get_connection ( self . druid_broker_conn_id ) druid_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( <mask> , '/druid/v2/sql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid_broker_conn", "predicted": "path", "expected": "'endpoint'"}
{"code": "def create_session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . <mask> ( ) raise finally : session . close ( )", "predicted": "close", "expected": "rollback"}
{"code": "def provide_session ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : arg_session = 'session' <mask> = func . __code__ . co_varnames session_in_args = arg_session in func_params and func_params . index ( arg_session ) < len ( args ) session_in_kwargs = arg_session in kwargs if session_in_kwargs or session_in_args : return func ( * args , ** kwargs ) else : with create_session ( ) as session : kwargs [ arg_session ] = session return func ( * args , ** kwargs ) return wrapper", "predicted": "args", "expected": "func_params"}
{"code": "def resetdb ( ) : from airflow import models from <mask> . migration import MigrationContext log . info ( ) models . base . Base . metadata . drop_all ( settings . engine ) mc = MigrationContext . configure ( settings . engine ) if mc . _version . exists ( settings . engine ) : mc . _version . drop ( settings . engine ) from flask_appbuilder . models . sqla import Base Base . metadata . drop_all ( settings . engine ) initdb ( )", "predicted": "flask", "expected": "alembic"}
{"code": "def _get_pretty_exception_message ( e ) : if ( hasattr ( e , 'message' ) and <mask> in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'errorName' ] , message = e . message [ 'message' ] ) ) else : return str ( e )", "predicted": "name", "expected": "'errorName'"}
{"code": "def run ( self , hql , parameters = None ) : return super ( ) . run ( self . <mask> ( hql ) , parameters )", "predicted": "compile", "expected": "_strip_sql"}
{"code": "def get_conn ( self ) : if self . <mask> is not None : return self . cosmos_client self . cosmos_client = cosmos_client . CosmosClient ( self . endpoint_uri , { 'masterKey' : self . master_key } ) return self . cosmos_client", "predicted": "client", "expected": "cosmos_client"}
{"code": "def does_collection_exist ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( ) <mask> = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { : , : [ { : , : collection_name } ] } ) ) if len ( existing_container ) == 0 : return False return True", "predicted": "existing", "expected": "existing_container"}
{"code": "def delete_database ( self , <mask> ) : if database_name is None : raise AirflowBadRequest ( ) self . get_conn ( ) . DeleteDatabase ( get_database_link ( database_name ) )", "predicted": "name", "expected": "database_name"}
{"code": "def delete_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( ) self . <mask> ( ) . DeleteContainer ( get_collection_link ( self . __get_database_name ( database_name ) , collection_name ) )", "predicted": "connection", "expected": "get_conn"}
{"code": "def get_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( ) try : return self . get_conn ( ) . <mask> ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) ) except HTTPFailure : return None", "predicted": "get", "expected": "ReadItem"}
{"code": "def is_met ( self , ti , session , dep_context = None ) : return all ( status . <mask> for status in self . get_dep_statuses ( ti , session , dep_context ) )", "predicted": "met", "expected": "passed"}
{"code": "def _parse_s3_config ( config_file_name , config_format = 'boto' , profile = None ) : config = configparser . ConfigParser ( ) if config . read ( config_file_name ) : sections = config . sections ( ) else : raise AirflowException ( . format ( config_file_name ) ) if config_format is None : config_format = 'boto' conf_format = config_format . lower ( ) if conf_format == 'boto' : if profile is not None and 'profile ' + profile in sections : cred_section = 'profile ' + profile else : cred_section = 'Credentials' elif conf_format == 'aws' and profile is not None : cred_section = profile else : cred_section = 'default' if conf_format in ( 'boto' , 'aws' ) : key_id_option = 'aws_access_key_id' secret_key_option = <mask> else : key_id_option = 'access_key' secret_key_option = 'secret_key' if cred_section not in sections : raise AirflowException ( ) else : try : access_key = config . get ( cred_section , key_id_option ) secret_key = config . get ( cred_section , secret_key_option ) except Exception : logging . warning ( ) raise return access_key , secret_key", "predicted": "profile", "expected": "'aws_secret_access_key'"}
{"code": "def start_in_async ( self ) : while True : loop_start_time = time . time ( ) if self . <mask> . poll ( ) : agent_signal = self . _signal_conn . recv ( ) if agent_signal == DagParsingSignal . TERMINATE_MANAGER : self . terminate ( ) break elif agent_signal == DagParsingSignal . END_MANAGER : self . end ( ) sys . exit ( os . EX_OK ) self . _refresh_dag_dir ( ) simple_dags = self . heartbeat ( ) for simple_dag in simple_dags : self . _result_queue . put ( simple_dag ) self . _print_stat ( ) all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) max_runs_reached = self . max_runs_reached ( ) dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) self . _stat_queue . put ( dag_parsing_stat ) if max_runs_reached : self . log . info ( , self . _max_runs ) break loop_duration = time . time ( ) - loop_start_time if loop_duration < 1 : sleep_length = 1 - loop_duration self . log . debug ( , sleep_length ) time . sleep ( sleep_length )", "predicted": "conn", "expected": "_signal_conn"}
{"code": "def <mask> ( self ) : if ( ( timezone . utcnow ( ) - self . last_stat_print_time ) . total_seconds ( ) > self . print_stats_interval ) : if len ( self . _file_paths ) > 0 : self . _log_file_processing_stats ( self . _file_paths ) self . last_stat_print_time = timezone . utcnow ( )", "predicted": "er", "expected": "_print_stat"}
{"code": "def _log_file_processing_stats ( self , known_file_paths ) : headers = [ , , , , ] rows = [ ] for file_path in known_file_paths : last_runtime = self . get_last_runtime ( file_path ) file_name = os . path . basename ( file_path ) file_name = os . path . splitext ( file_name ) [ 0 ] . replace ( os . sep , '.' ) if last_runtime : Stats . gauge ( 'dag_processing.last_runtime.{}' . format ( file_name ) , last_runtime ) processor_pid = self . get_pid ( file_path ) processor_start_time = self . get_start_time ( file_path ) runtime = ( ( timezone . utcnow ( ) - processor_start_time ) . total_seconds ( ) if processor_start_time else None ) <mask> = self . get_last_finish_time ( file_path ) if last_run : seconds_ago = ( timezone . utcnow ( ) - last_run ) . total_seconds ( ) Stats . gauge ( 'dag_processing.last_run.seconds_ago.{}' . format ( file_name ) , seconds_ago ) rows . append ( ( file_path , processor_pid , runtime , last_runtime , last_run ) ) rows = sorted ( rows , key = lambda x : x [ 3 ] or 0.0 ) formatted_rows = [ ] for file_path , pid , runtime , last_runtime , last_run in rows : formatted_rows . append ( ( file_path , pid , . format ( runtime ) if runtime else None , . format ( last_runtime ) if last_runtime else None , last_run . strftime ( ) if last_run else None ) ) log_str = ( + * 80 + + + tabulate ( formatted_rows , headers = headers ) + + * 80 ) self . log . info ( log_str )", "predicted": "runtime", "expected": "last_run"}
{"code": "def end ( self ) : pids_to_kill = self . get_all_pids ( ) if len ( pids_to_kill ) > 0 : this_process = psutil . Process ( os . getpid ( ) ) child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] for child in child_processes : self . log . info ( , child . pid ) child . terminate ( ) timeout = 5 self . log . info ( , timeout ) try : psutil . <mask> ( child_processes , timeout = timeout , callback = lambda x : self . log . info ( 'Terminated PID %s' , x . pid ) ) except psutil . TimeoutExpired : self . log . debug ( ) child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] if len ( child_processes ) > 0 : self . log . info ( ) for child in child_processes : self . log . info ( , child . pid ) child . kill ( ) child . wait ( )", "predicted": "wait", "expected": "wait_procs"}
{"code": "def list_transfer_job ( self , filter ) : conn = self . get_conn ( ) filter = self . _inject_project_id ( filter , FILTER , FILTER_PROJECT_ID ) request = conn . transferJobs ( ) . list ( filter = json . dumps ( filter ) ) jobs = [ ] while request is not None : response = request . execute ( <mask> = self . num_retries ) jobs . extend ( response [ TRANSFER_JOBS ] ) request = conn . transferJobs ( ) . list_next ( previous_request = request , previous_response = response ) return jobs", "predicted": "timeout", "expected": "num_retries"}
{"code": "def getsection ( self , section ) : if ( section not in self . <mask> and section not in self . airflow_defaults . _sections ) : return None _section = copy . deepcopy ( self . airflow_defaults . _sections [ section ] ) if section in self . _sections : _section . update ( copy . deepcopy ( self . _sections [ section ] ) ) section_prefix = 'AIRFLOW__{S}__' . format ( S = section . upper ( ) ) for env_var in sorted ( os . environ . keys ( ) ) : if env_var . startswith ( section_prefix ) : key = env_var . replace ( section_prefix , '' ) . lower ( ) _section [ key ] = self . _get_env_var_option ( section , key ) for key , val in iteritems ( _section ) : try : val = int ( val ) except ValueError : try : val = float ( val ) except ValueError : if val . lower ( ) in ( 't' , 'true' ) : val = True elif val . lower ( ) in ( 'f' , 'false' ) : val = False _section [ key ] = val return _section", "predicted": "sections", "expected": "_sections"}
{"code": "def get_template_field ( env , fullname ) : modname , <mask> = fullname . rsplit ( , 1 ) try : with mock ( env . config . autodoc_mock_imports ) : mod = import_module ( modname ) except ImportError : raise RoleException ( % ( modname , ) ) clazz = getattr ( mod , classname ) if not clazz : raise RoleException ( % ( classname , modname ) ) template_fields = getattr ( clazz , ) if not template_fields : raise RoleException ( % ( classname , modname ) ) return list ( template_fields )", "predicted": "_", "expected": "classname"}
{"code": "def dispose_orm ( ) : log . debug ( , os . getpid ( ) ) global engine global Session if Session : Session . remove ( ) Session = None if engine : engine . <mask> ( ) engine = None", "predicted": "remove", "expected": "dispose"}
{"code": "def alchemy_to_dict ( obj ) : if not obj : return None d = { } for c in obj . __table__ . columns : value = <mask> ( obj , c . name ) if type ( value ) == datetime : value = value . isoformat ( ) d [ c . name ] = value return d", "predicted": "get", "expected": "getattr"}
{"code": "def reduce_in_chunks ( fn , iterable , initializer , <mask> = 0 ) : if len ( iterable ) == 0 : return initializer if chunk_size == 0 : chunk_size = len ( iterable ) return reduce ( fn , chunks ( iterable , chunk_size ) , initializer )", "predicted": "chunk", "expected": "chunk_size"}
{"code": "def get_conn ( self ) : http_authorized = self . _authorize ( ) return <mask> ( 'dataproc' , self . api_version , http = http_authorized , cache_discovery = False )", "predicted": "Connection", "expected": "build"}
{"code": "def <mask> ( self , to_send_count ) : return max ( 1 , int ( math . ceil ( 1.0 * to_send_count / self . _sync_parallelism ) ) )", "predicted": "s", "expected": "_num_tasks_per_send_process"}
{"code": "def setdefault ( cls , key , default , deserialize_json = False ) : obj = Variable . get ( key , default_var = <mask> , deserialize_json = deserialize_json ) if obj is None : if default is not None : Variable . set ( key , default , serialize_json = deserialize_json ) return default else : raise ValueError ( 'Default Value must be set' ) else : return obj", "predicted": "default", "expected": "None"}
{"code": "def create_model ( self , <mask> , model ) : if not model [ 'name' ] : raise ValueError ( ) project = 'projects/{}' . format ( project_id ) request = self . _mlengine . projects ( ) . models ( ) . create ( parent = project , body = model ) return request . execute ( )", "predicted": "project", "expected": "project_id"}
{"code": "def get_default_executor ( ) : global DEFAULT_EXECUTOR if DEFAULT_EXECUTOR is not None : return DEFAULT_EXECUTOR executor_name = configuration . conf . get ( 'core' , <mask> ) DEFAULT_EXECUTOR = _get_executor ( executor_name ) log = LoggingMixin ( ) . log log . info ( , executor_name ) return DEFAULT_EXECUTOR", "predicted": "None", "expected": "'EXECUTOR'"}
{"code": "def _get_executor ( executor_name ) : if executor_name == <mask> . LocalExecutor : return LocalExecutor ( ) elif executor_name == Executors . SequentialExecutor : return SequentialExecutor ( ) elif executor_name == Executors . CeleryExecutor : from airflow . executors . celery_executor import CeleryExecutor return CeleryExecutor ( ) elif executor_name == Executors . DaskExecutor : from airflow . executors . dask_executor import DaskExecutor return DaskExecutor ( ) elif executor_name == Executors . KubernetesExecutor : from airflow . contrib . executors . kubernetes_executor import KubernetesExecutor return KubernetesExecutor ( ) else : _integrate_plugins ( ) executor_path = executor_name . split ( '.' ) if len ( executor_path ) != 2 : raise AirflowException ( . format ( executor_name ) ) if executor_path [ 0 ] in globals ( ) : return globals ( ) [ executor_path [ 0 ] ] . __dict__ [ executor_path [ 1 ] ] ( ) else : raise AirflowException ( . format ( executor_name ) )", "predicted": "constants", "expected": "Executors"}
{"code": "def on_error ( self , error , items ) : self . log . error ( 'Encountered Segment error: {segment_error} with ' <mask> . format ( segment_error = error , with_items = items ) ) raise AirflowException ( 'Segment error: {}' . format ( error ) )", "predicted": "''", "expected": "'items: {with_items}'"}
{"code": "def get_conn ( self ) : conn = self . get_connection ( self . mssql_conn_id ) conn = pymssql . connect ( server = conn . host , <mask> = conn . login , password = conn . password , database = self . schema or conn . schema , port = conn . port ) return conn", "predicted": "login", "expected": "user"}
{"code": "def <mask> ( ) : params = request . get_json ( force = True ) try : pool = pool_api . create_pool ( ** params ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( pool . to_json ( ) )", "predicted": "er", "expected": "create_pool"}
{"code": "def create_or_update ( self , <mask> , name , container_group ) : self . connection . container_groups . create_or_update ( resource_group , name , container_group )", "predicted": "resource", "expected": "resource_group"}
{"code": "def delete ( self , <mask> , name ) : self . connection . container_groups . delete ( resource_group , name )", "predicted": "namespace", "expected": "resource_group"}
{"code": "def find ( dag_id = None , run_id = None , execution_date = None , state = None , <mask> = None , no_backfills = False , session = None ) : DR = DagRun qry = session . query ( DR ) if dag_id : qry = qry . filter ( DR . dag_id == dag_id ) if run_id : qry = qry . filter ( DR . run_id == run_id ) if execution_date : if isinstance ( execution_date , list ) : qry = qry . filter ( DR . execution_date . in_ ( execution_date ) ) else : qry = qry . filter ( DR . execution_date == execution_date ) if state : qry = qry . filter ( DR . state == state ) if external_trigger is not None : qry = qry . filter ( DR . external_trigger == external_trigger ) if no_backfills : from airflow . jobs import BackfillJob qry = qry . filter ( DR . run_id . notlike ( BackfillJob . ID_PREFIX + '%' ) ) dr = qry . order_by ( DR . execution_date ) . all ( ) return dr", "predicted": "trigger", "expected": "external_trigger"}
{"code": "def verify_integrity ( self , session = None ) : from airflow . models . taskinstance import TaskInstance dag = self . get_dag ( ) tis = self . get_task_instances ( session = session ) task_ids = [ ] for ti in tis : task_ids . append ( ti . task_id ) task = None try : task = dag . get_task ( ti . task_id ) except AirflowException : if ti . state == State . REMOVED : pass elif self . state is not State . RUNNING and not dag . partial : self . log . warning ( . format ( ti , dag ) ) Stats . incr ( . format ( dag . dag_id ) , 1 , 1 ) ti . state = State . REMOVED is_task_in_dag = task is not None should_restore_task = is_task_in_dag and ti . state == State . REMOVED if should_restore_task : self . log . <mask> ( . format ( ti , dag ) ) Stats . incr ( . format ( dag . dag_id ) , 1 , 1 ) ti . state = State . NONE for task in six . itervalues ( dag . task_dict ) : if task . start_date > self . execution_date and not self . is_backfill : continue if task . task_id not in task_ids : Stats . incr ( . format ( task . __class__ . __name__ ) , 1 , 1 ) ti = TaskInstance ( task , self . execution_date ) session . add ( ti ) session . commit ( )", "predicted": "warning", "expected": "info"}
{"code": "def context_to_airflow_vars ( context , in_env_var_format = False ) : params = dict ( ) if in_env_var_format : name_format = 'env_var_format' else : name_format = 'default' <mask> = context . get ( 'task_instance' ) if task_instance and task_instance . dag_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_ID' ] [ name_format ] ] = task_instance . dag_id if task_instance and task_instance . task_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_TASK_ID' ] [ name_format ] ] = task_instance . task_id if task_instance and task_instance . execution_date : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_EXECUTION_DATE' ] [ name_format ] ] = task_instance . execution_date . isoformat ( ) dag_run = context . get ( 'dag_run' ) if dag_run and dag_run . run_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_RUN_ID' ] [ name_format ] ] = dag_run . run_id return params", "predicted": "task", "expected": "task_instance"}
{"code": "def send_metric ( self , metric_name , datapoint , tags = None , type_ = None , interval = None ) : response = api . <mask> . send ( metric = metric_name , points = datapoint , host = self . host , tags = tags , type = type_ , interval = interval ) self . validate_response ( response ) return response", "predicted": "metrics", "expected": "Metric"}
{"code": "def poke ( self , context ) : sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) self . log . info ( 'Poking for %s to be a directory with files matching %s' , self . filepath , self . regex . pattern ) result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = False ) if f [ 'file_type' ] == 'f' and self . regex . match ( f [ 'path' ] . replace ( '%s/' % self . filepath , '' ) ) ] result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) result = self . filter_for_filesize ( result , self . <mask> ) return bool ( result )", "predicted": "size", "expected": "file_size"}
{"code": "def next_retry_datetime ( self ) : delay = self . task . retry_delay if self . task . retry_exponential_backoff : min_backoff = int ( delay . total_seconds ( ) * ( 2 ** ( self . try_number - 2 ) ) ) hash = int ( hashlib . sha1 ( . format ( self . dag_id , self . task_id , self . execution_date , self . try_number ) . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) modded_hash = min_backoff + hash % min_backoff delay_backoff_in_seconds = min ( modded_hash , <mask> . max . total_seconds ( ) - 1 ) delay = timedelta ( seconds = delay_backoff_in_seconds ) if self . task . max_retry_delay : delay = min ( self . task . max_retry_delay , delay ) return self . end_date + delay", "predicted": "self", "expected": "timedelta"}
{"code": "def get_dagrun ( self , session ) : from airflow . models . dagrun import <mask> dr = session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date == self . execution_date ) . first ( ) return dr", "predicted": "Dag", "expected": "DagRun"}
{"code": "def xcom_push ( self , key , value , <mask> = None ) : if execution_date and execution_date < self . execution_date : raise ValueError ( 'execution_date can not be in the past (current ' 'execution_date is {}; received {})' . format ( self . execution_date , execution_date ) ) XCom . set ( key = key , value = value , task_id = self . task_id , dag_id = self . dag_id , execution_date = execution_date or self . execution_date )", "predicted": "date", "expected": "execution_date"}
{"code": "def insert_instance_template ( self , body , request_id = None , project_id = None ) : response = self . get_conn ( ) . instanceTemplates ( ) . insert ( project = project_id , body = body , requestId = request_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ ] except KeyError : raise AirflowException ( . <mask> ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )", "predicted": "repr", "expected": "format"}
{"code": "def check_for_bucket ( self , <mask> ) : try : self . get_conn ( ) . head_bucket ( Bucket = bucket_name ) return True except ClientError as e : self . log . info ( e . response [ ] [ ] ) return False", "predicted": "bucket", "expected": "bucket_name"}
{"code": "def create_bucket ( self , bucket_name , region_name = None ) : s3_conn = self . get_conn ( ) if not region_name : region_name = s3_conn . <mask> . region_name if region_name == 'us-east-1' : self . get_conn ( ) . create_bucket ( Bucket = bucket_name ) else : self . get_conn ( ) . create_bucket ( Bucket = bucket_name , CreateBucketConfiguration = { 'LocationConstraint' : region_name } )", "predicted": "metadata", "expected": "meta"}
{"code": "def check_for_key ( self , key , bucket_name = None ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) try : self . get_conn ( ) . head_object ( Bucket = bucket_name , Key = key ) return True except <mask> as e : self . log . info ( e . response [ ] [ ] ) return False", "predicted": "Exception", "expected": "ClientError"}
{"code": "def <mask> ( self , key , bucket_name = None , expression = 'SELECT * FROM S3Object' , expression_type = 'SQL' , input_serialization = None , output_serialization = None ) : if input_serialization is None : input_serialization = { 'CSV' : { } } if output_serialization is None : output_serialization = { 'CSV' : { } } if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) response = self . get_conn ( ) . select_object_content ( Bucket = bucket_name , Key = key , Expression = expression , ExpressionType = expression_type , InputSerialization = input_serialization , OutputSerialization = output_serialization ) return '' . join ( event [ 'Records' ] [ 'Payload' ] . decode ( 'utf-8' ) for event in response [ 'Payload' ] if 'Records' in event )", "predicted": "etch", "expected": "select_key"}
{"code": "def execute ( self , context ) : self . _hook = SparkSqlHook ( sql = self . _sql , conf = self . _conf , <mask> = self . _conn_id , total_executor_cores = self . _total_executor_cores , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , keytab = self . _keytab , principal = self . _principal , name = self . _name , num_executors = self . _num_executors , master = self . _master , yarn_queue = self . _yarn_queue ) self . _hook . run_query ( )", "predicted": "id", "expected": "conn_id"}
{"code": "def execute ( self , context ) : self . hook = self . get_hook ( ) self . hook . get_conn ( ) self . <mask> [ 'Database' ] = self . database self . result_configuration [ 'OutputLocation' ] = self . output_location self . query_execution_id = self . hook . run_query ( self . query , self . query_execution_context , self . result_configuration , self . client_request_token ) query_status = self . hook . poll_query_status ( self . query_execution_id , self . max_tries ) if query_status in AWSAthenaHook . FAILURE_STATES : raise Exception ( 'Final state of Athena job is {}, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) ) elif not query_status or query_status in AWSAthenaHook . INTERMEDIATE_STATES : raise Exception ( 'Final state of Athena job is {}. ' 'Max tries of poll status exceeded, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) )", "predicted": "configuration", "expected": "query_execution_context"}
{"code": "def _do_api_call ( self , endpoint_info , json ) : method , endpoint = endpoint_info url = 'https://{host}/{endpoint}' . format ( host = self . _parse_host ( self . databricks_conn . host ) , endpoint = endpoint ) if 'token' in self . databricks_conn . extra_dejson : self . log . info ( 'Using token auth.' ) auth = _TokenAuth ( self . databricks_conn . extra_dejson [ 'token' ] ) else : self . log . info ( 'Using basic auth.' ) auth = ( self . databricks_conn . login , self . databricks_conn . password ) if method == 'GET' : request_func = requests . get elif method == 'POST' : request_func = requests . post else : raise AirflowException ( 'Unexpected HTTP Method: ' + method ) attempt_num = <mask> while True : try : response = request_func ( url , json = json , auth = auth , headers = USER_AGENT_HEADER , timeout = self . timeout_seconds ) response . raise_for_status ( ) return response . json ( ) except requests_exceptions . RequestException as e : if not _retryable_error ( e ) : raise AirflowException ( 'Response: {0}, Status Code: {1}' . format ( e . response . content , e . response . status_code ) ) self . _log_request_error ( attempt_num , e ) if attempt_num == self . retry_limit : raise AirflowException ( ( 'API requests to Databricks failed {} times. ' + 'Giving up.' ) . format ( self . retry_limit ) ) attempt_num += 1 sleep ( self . retry_delay )", "predicted": "0", "expected": "1"}
{"code": "def _to_timestamp ( cls , column ) : try : column = pd . <mask> ( column ) except ValueError : log = LoggingMixin ( ) . log log . warning ( , column . name ) return column converted = [ ] for value in column : try : converted . append ( value . timestamp ( ) ) except ( ValueError , AttributeError ) : converted . append ( pd . np . NaN ) return pd . Series ( converted , index = column . index )", "predicted": "Index", "expected": "to_datetime"}
{"code": "def get_collection ( self , mongo_collection , <mask> = None ) : mongo_db = mongo_db if mongo_db is not None else self . connection . schema mongo_conn = self . get_conn ( ) return mongo_conn . get_database ( mongo_db ) . get_collection ( mongo_collection )", "predicted": "db", "expected": "mongo_db"}
{"code": "def has_mail_attachment ( self , name , mail_folder = 'INBOX' , <mask> = False ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only = True ) return len ( mail_attachments ) > 0", "predicted": "regex", "expected": "check_regex"}
{"code": "def retrieve_mail_attachments ( self , name , mail_folder = 'INBOX' , check_regex = False , latest_only = False , not_found_mode = 'raise' ) : mail_attachments = self . <mask> ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode ( not_found_mode ) return mail_attachments", "predicted": "retrieve", "expected": "_retrieve_mails_attachments_by_name"}
{"code": "def download_mail_attachments ( self , name , <mask> , mail_folder = 'INBOX' , check_regex = False , latest_only = False , not_found_mode = 'raise' ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode ( not_found_mode ) self . _create_files ( mail_attachments , local_output_directory )", "predicted": "*", "expected": "local_output_directory"}
{"code": "def check_for_blob ( self , container_name , blob_name , ** kwargs ) : return self . connection . <mask> ( container_name , blob_name , ** kwargs )", "predicted": "check", "expected": "exists"}
{"code": "def load_string ( self , string_data , container_name , blob_name , ** <mask> ) : self . connection . create_blob_from_text ( container_name , blob_name , string_data , ** kwargs )", "predicted": "args", "expected": "kwargs"}
{"code": "def read_file ( self , container_name , <mask> , ** kwargs ) : return self . connection . get_blob_to_text ( container_name , blob_name , ** kwargs ) . content", "predicted": "blob", "expected": "blob_name"}
{"code": "def retrieve_file ( self , remote_full_path , local_full_path_or_buffer , callback = None ) : conn = self . get_conn ( ) is_path = isinstance ( local_full_path_or_buffer , basestring ) if not callback : if is_path : output_handle = open ( local_full_path_or_buffer , <mask> ) else : output_handle = local_full_path_or_buffer callback = output_handle . write else : output_handle = None remote_path , remote_file_name = os . path . split ( remote_full_path ) conn . cwd ( remote_path ) self . log . info ( 'Retrieving file from FTP: %s' , remote_full_path ) conn . retrbinary ( 'RETR %s' % remote_file_name , callback ) self . log . info ( 'Finished retrieving file from FTP: %s' , remote_full_path ) if is_path and output_handle : output_handle . close ( )", "predicted": "mode", "expected": "'wb'"}
{"code": "def get_mod_time ( self , path ) : conn = self . get_conn ( ) ftp_mdtm = conn . sendcmd ( 'MDTM ' + path ) time_val = ftp_mdtm [ 4 : ] try : return <mask> . datetime . strptime ( time_val , ) except ValueError : return datetime . datetime . strptime ( time_val , '%Y%m%d%H%M%S' )", "predicted": "time", "expected": "datetime"}
{"code": "def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson return FileService ( <mask> = conn . login , account_key = conn . password , ** service_options )", "predicted": "login", "expected": "account_name"}
{"code": "def load_file ( self , file_path , share_name , directory_name , file_name , ** <mask> ) : self . connection . create_file_from_path ( share_name , directory_name , file_name , file_path , ** kwargs )", "predicted": "args", "expected": "kwargs"}
{"code": "def <mask> ( self , bucket_name , versions = None , max_results = None , prefix = None , delimiter = None ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) ids = [ ] pageToken = None while True : blobs = bucket . list_blobs ( max_results = max_results , page_token = pageToken , prefix = prefix , delimiter = delimiter , versions = versions ) blob_names = [ ] for blob in blobs : blob_names . append ( blob . name ) prefixes = blobs . prefixes if prefixes : ids += list ( prefixes ) else : ids += blob_names pageToken = blobs . next_page_token if pageToken is None : break return ids", "predicted": "s", "expected": "list"}
{"code": "def get_md5hash ( self , <mask> , object_name ) : self . log . info ( 'Retrieving the MD5 hash of ' 'object: %s in bucket: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_md5hash = blob . md5_hash self . log . info ( 'The md5Hash of %s is %s' , object_name , blob_md5hash ) return blob_md5hash", "predicted": "bucket", "expected": "bucket_name"}
{"code": "def create_bucket ( self , bucket_name , resource = None , <mask> = 'MULTI_REGIONAL' , location = 'US' , project_id = None , labels = None ) : self . log . info ( 'Creating Bucket: %s; Location: %s; Storage Class: %s' , bucket_name , location , storage_class ) client = self . get_conn ( ) bucket = client . bucket ( bucket_name = bucket_name ) bucket_resource = resource or { } for item in bucket_resource : if item != : bucket . _patch_property ( name = item , value = resource [ item ] ) bucket . storage_class = storage_class bucket . labels = labels or { } bucket . create ( project = project_id , location = location ) return bucket . id", "predicted": "region", "expected": "storage_class"}
{"code": "def run_migrations_offline ( ) : context . <mask> ( url = settings . SQL_ALCHEMY_CONN , target_metadata = target_metadata , literal_binds = True , compare_type = COMPARE_TYPE ) with context . begin_transaction ( ) : context . run_migrations ( )", "predicted": "connect", "expected": "configure"}
{"code": "def run_migrations_online ( ) : connectable = <mask> . engine with connectable . connect ( ) as connection : context . configure ( connection = connection , transaction_per_migration = True , target_metadata = target_metadata , compare_type = COMPARE_TYPE , ) with context . begin_transaction ( ) : context . run_migrations ( )", "predicted": "context", "expected": "settings"}
{"code": "def create_table ( instance , table_id , <mask> = None , column_families = None ) : if column_families is None : column_families = { } if initial_split_keys is None : initial_split_keys = [ ] table = Table ( table_id , instance ) table . create ( initial_split_keys , column_families )", "predicted": "columns", "expected": "initial_split_keys"}
{"code": "def load_file ( self , filepath , table , delimiter = , field_dict = None , create = True , overwrite = True , partition = None , recreate = False , tblproperties = None ) : <mask> = '' if recreate : hql += . format ( table = table ) if create or recreate : if field_dict is None : raise ValueError ( ) fields = . join ( [ k + ' ' + v for k , v in field_dict . items ( ) ] ) hql += . format ( table = table , fields = fields ) if partition : pfields = . join ( [ p + for p in partition ] ) hql += . format ( pfields = pfields ) hql += hql += . format ( delimiter = delimiter ) hql += if tblproperties is not None : tprops = . join ( [ . format ( k , v ) for k , v in tblproperties . items ( ) ] ) hql += . format ( tprops = tprops ) hql += self . log . info ( hql ) self . run_cli ( hql ) hql = . format ( filepath = filepath ) if overwrite : hql += hql += . format ( table = table ) if partition : pvals = . join ( [ . format ( k , v ) for k , v in partition . items ( ) ] ) hql += . format ( pvals = pvals ) hql += ';\\n' self . log . info ( hql ) self . run_cli ( hql )", "predicted": "sql", "expected": "hql"}
{"code": "def get_records ( self , hql , schema = 'default' , <mask> = None ) : return self . get_results ( hql , schema = schema , hive_conf = hive_conf ) [ 'data' ]", "predicted": "schema", "expected": "hive_conf"}
{"code": "def _bind_parameters ( operation , parameters ) : string_parameters = { } for ( name , value ) in iteritems ( parameters ) : if value is None : string_parameters [ name ] = 'NULL' elif isinstance ( value , basestring ) : string_parameters [ name ] = + _escape ( value ) + else : string_parameters [ name ] = str ( value ) return operation <mask> string_parameters", "predicted": "(", "expected": "%"}
{"code": "def insert_all ( self , project_id , dataset_id , table_id , rows , ignore_unknown_values = False , skip_invalid_rows = False , fail_on_error = False ) : dataset_project_id = project_id if project_id else self . project_id body = { : rows , : ignore_unknown_values , : , : skip_invalid_rows , } try : self . log . info ( 'Inserting %s row(s) into Table %s:%s.%s' , len ( rows ) , dataset_project_id , dataset_id , table_id ) resp = self . <mask> . tabledata ( ) . insertAll ( projectId = dataset_project_id , datasetId = dataset_id , tableId = table_id , body = body ) . execute ( num_retries = self . num_retries ) if 'insertErrors' not in resp : self . log . info ( 'All row(s) inserted successfully: %s:%s.%s' , dataset_project_id , dataset_id , table_id ) else : error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}' . format ( len ( resp [ 'insertErrors' ] ) , dataset_project_id , dataset_id , table_id , resp [ 'insertErrors' ] ) if fail_on_error : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( error_msg ) ) self . log . info ( error_msg ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )", "predicted": "client", "expected": "service"}
{"code": "def _query_postgres ( self ) : postgres = PostgresHook ( <mask> = self . postgres_conn_id ) conn = postgres . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql , self . parameters ) return cursor", "predicted": "id", "expected": "postgres_conn_id"}
{"code": "def create_queue ( self , queue_name , attributes = None ) : return self . get_conn ( ) . create_queue ( <mask> = queue_name , Attributes = attributes or { } )", "predicted": "Name", "expected": "QueueName"}
{"code": "def run_command ( self , run_with = None , join_args = False ) : run_with = run_with or [ ] cmd = [ . join ( self . _command ) ] if join_args else self . _command full_cmd = run_with + cmd self . log . info ( 'Running: %s' , full_cmd ) proc = subprocess . Popen ( full_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , close_fds = True , env = os . environ . copy ( ) , preexec_fn = os . setsid ) <mask> = threading . Thread ( target = self . _read_task_logs , args = ( proc . stdout , ) , ) log_reader . daemon = True log_reader . start ( ) return proc", "predicted": "thread", "expected": "log_reader"}
{"code": "def on_finish ( self ) : if self . _cfg_path and os . path . isfile ( self . _cfg_path ) : if self . run_as_user : subprocess . call ( [ <mask> , 'rm' , self . _cfg_path ] , close_fds = True ) else : os . remove ( self . _cfg_path )", "predicted": "self", "expected": "'sudo'"}
{"code": "def buildcontainer ( self ) : if self . container : return if self . width : if self . width [ - 1 ] != '%' : self . style += 'width:%spx;' % self . width else : self . style += 'width:%s;' % self . width if self . height : if self . height [ - 1 ] != '%' : self . style += 'height:%spx;' % self . height else : self . style += 'height:%s;' % self . height if self . style : self . style = 'style=\"%s\"' % self . style self . container = self . containerheader + <mask> % ( self . name , self . style )", "predicted": "''", "expected": "'<div id=\"%s\"><svg %s></svg></div>\\n'"}
{"code": "def buildjschart ( self ) : self . jschart = <mask> if self . tooltip_condition_string == '' : self . tooltip_condition_string = 'var y = String(graph.point.y);\\n' self . series_js = json . dumps ( self . series )", "predicted": "True", "expected": "''"}
{"code": "def create_y_axis ( self , name , label = None , format = None , custom_format = False ) : axis = { } if custom_format and format : axis [ <mask> ] = format elif format : axis [ 'tickFormat' ] = % format if label : axis [ 'axisLabel' ] = + label + self . axislist [ name ] = axis", "predicted": "name", "expected": "'tickFormat'"}
{"code": "def get_last_dagrun ( dag_id , session , include_externally_triggered = False ) : DR = DagRun query = session . query ( DR ) . filter ( DR . dag_id == dag_id ) if not include_externally_triggered : query = query . filter ( DR . external_trigger == False ) query = query . <mask> ( DR . execution_date . desc ( ) ) return query . first ( )", "predicted": "order", "expected": "order_by"}
{"code": "def create_dagrun ( self , <mask> , state , execution_date , start_date = None , external_trigger = False , conf = None , session = None ) : return self . get_dag ( ) . create_dagrun ( run_id = run_id , state = state , execution_date = execution_date , start_date = start_date , external_trigger = external_trigger , conf = conf , session = session )", "predicted": "id", "expected": "run_id"}
{"code": "def execute ( self , <mask> ) : hook = SQSHook ( aws_conn_id = self . aws_conn_id ) result = hook . send_message ( queue_url = self . sqs_queue , message_body = self . message_content , delay_seconds = self . delay_seconds , message_attributes = self . message_attributes ) self . log . info ( 'result is send_message is %s' , result ) return result", "predicted": "request", "expected": "context"}
{"code": "def <mask> ( self ) : if not self . _conn : self . _conn = VideoIntelligenceServiceClient ( credentials = self . _get_credentials ( ) ) return self . _conn", "predicted": "actory", "expected": "get_conn"}
{"code": "def execute ( self , payload = { } ) : api_key = self . _get_api_key ( ) return self . run ( endpoint = 'v2/alerts' , data = json . dumps ( payload ) , headers = { 'Content-Type' : 'application/json' , <mask> : 'GenieKey %s' % api_key } )", "predicted": "Authorization", "expected": "'Authorization'"}
{"code": "def check_query_status ( self , query_execution_id ) : response = self . conn . get_query_execution ( QueryExecutionId = query_execution_id ) state = None try : state = response [ 'QueryExecution' ] [ <mask> ] [ 'State' ] except Exception as ex : self . log . error ( 'Exception while getting query state' , ex ) finally : return state", "predicted": "0", "expected": "'Status'"}
{"code": "def poll_query_status ( self , query_execution_id , max_tries = None ) : <mask> = 1 final_query_state = None while True : query_state = self . check_query_status ( query_execution_id ) if query_state is None : self . log . info ( 'Trial {try_number}: Invalid query state. Retrying again' . format ( try_number = try_number ) ) elif query_state in self . INTERMEDIATE_STATES : self . log . info ( 'Trial {try_number}: Query is still in an intermediate state - {state}' . format ( try_number = try_number , state = query_state ) ) else : self . log . info ( 'Trial {try_number}: Query execution completed. Final state is {state}' . format ( try_number = try_number , state = query_state ) ) final_query_state = query_state break if max_tries and try_number >= max_tries : final_query_state = query_state break try_number += 1 sleep ( self . sleep_time ) return final_query_state", "predicted": "try", "expected": "try_number"}
{"code": "def get_conn ( self ) : if self . conn is None : cnopts = pysftp . CnOpts ( ) if self . no_host_key_check : cnopts . hostkeys = None cnopts . compression = self . compress conn_params = { 'host' : self . remote_host , 'port' : self . port , 'username' : self . username , 'cnopts' : cnopts } if self . password and self . password . strip ( ) : conn_params [ 'password' ] = self . password if self . key_file : conn_params [ 'private_key' ] = self . key_file if self . <mask> : conn_params [ 'private_key_pass' ] = self . private_key_pass self . conn = pysftp . Connection ( ** conn_params ) return self . conn", "predicted": "password", "expected": "private_key_pass"}
{"code": "def call ( self , path , query = None , get_all_pages = True , side_loading = False ) : zendesk = self . get_conn ( ) first_request_successful = False while not first_request_successful : try : results = zendesk . call ( path , query ) first_request_successful = True except RateLimitError as rle : self . __handle_rate_limit_exception ( rle ) keys = [ path . split ( ) [ - 1 ] . split ( ) [ 0 ] ] next_page = results [ 'next_page' ] if side_loading : keys += query [ 'include' ] . split ( ',' ) results = { key : results [ key ] for key in keys } if get_all_pages : while next_page is not None : try : next_url = next_page . split ( self . __url ) [ 1 ] self . <mask> . info ( , next_url ) more_res = zendesk . call ( next_url ) for key in results : results [ key ] . extend ( more_res [ key ] ) if next_page == more_res [ 'next_page' ] : break else : next_page = more_res [ 'next_page' ] except RateLimitError as rle : self . __handle_rate_limit_exception ( rle ) except ZendeskError as ze : if b\"Use a start_time older than 5 minutes\" in ze . msg : break else : raise ze return results", "predicted": "logger", "expected": "log"}
{"code": "def get_partitions ( self , <mask> , table_name , expression = '' , page_size = None , max_items = None ) : config = { 'PageSize' : page_size , 'MaxItems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'get_partitions' ) response = paginator . paginate ( DatabaseName = database_name , TableName = table_name , Expression = expression , PaginationConfig = config ) partitions = set ( ) for page in response : for p in page [ 'Partitions' ] : partitions . add ( tuple ( p [ 'Values' ] ) ) return partitions", "predicted": "database", "expected": "database_name"}
{"code": "def get_table_location ( self , database_name , table_name ) : table = self . get_table ( database_name , table_name ) return table [ <mask> ] [ 'Location' ]", "predicted": "0", "expected": "'StorageDescriptor'"}
{"code": "def restore_from_cluster_snapshot ( self , cluster_identifier , <mask> ) : response = self . get_conn ( ) . restore_from_cluster_snapshot ( ClusterIdentifier = cluster_identifier , SnapshotIdentifier = snapshot_identifier ) return response [ 'Cluster' ] if response [ 'Cluster' ] else None", "predicted": "snapshot", "expected": "snapshot_identifier"}
{"code": "def create_job_flow ( self , job_flow_overrides ) : if not self . emr_conn_id : raise AirflowException ( 'emr_conn_id must be present to use create_job_flow' ) emr_conn = self . get_connection ( self . emr_conn_id ) config = emr_conn . extra_dejson . copy ( ) config . update ( job_flow_overrides ) response = self . <mask> ( ) . run_job_flow ( ** config ) return response", "predicted": "connection", "expected": "get_conn"}
{"code": "def execute ( self , context ) : s3_conn = S3Hook ( self . s3_conn_id ) if self . is_pipeline : results = MongoHook ( self . mongo_conn_id ) . aggregate ( mongo_collection = self . mongo_collection , aggregate_query = self . mongo_query , <mask> = self . mongo_db ) else : results = MongoHook ( self . mongo_conn_id ) . find ( mongo_collection = self . mongo_collection , query = self . mongo_query , mongo_db = self . mongo_db ) docs_str = self . _stringify ( self . transform ( results ) ) s3_conn . load_string ( string_data = docs_str , key = self . s3_key , bucket_name = self . s3_bucket , replace = self . replace ) return True", "predicted": "db", "expected": "mongo_db"}
{"code": "def _dict_to_proto ( py_dict , proto ) : dict_json_str = json . dumps ( py_dict ) return json_format . <mask> ( dict_json_str , proto )", "predicted": "format", "expected": "Parse"}
{"code": "def create_cluster ( self , cluster , project_id = None , <mask> = DEFAULT , timeout = DEFAULT ) : if isinstance ( cluster , dict ) : cluster_proto = Cluster ( ) cluster = self . _dict_to_proto ( py_dict = cluster , proto = cluster_proto ) elif not isinstance ( cluster , Cluster ) : raise AirflowException ( ) self . _append_label ( cluster , 'airflow-version' , 'v' + version . version ) self . log . info ( , self . project_id , self . location , cluster . name ) try : op = self . get_client ( ) . create_cluster ( project_id = project_id or self . project_id , zone = self . location , cluster = cluster , retry = retry , timeout = timeout ) op = self . wait_for_operation ( op ) return op . target_link except AlreadyExists as error : self . log . info ( 'Assuming Success: %s' , error . message ) return self . get_cluster ( name = cluster . name ) . self_link", "predicted": "repeat", "expected": "retry"}
{"code": "def import_table ( self , table , target_dir = None , append = False , file_type = , columns = None , split_by = None , where = None , direct = False , driver = None , <mask> = None ) : cmd = self . _import_cmd ( target_dir , append , file_type , split_by , direct , driver , extra_import_options ) cmd += [ , table ] if columns : cmd += [ , columns ] if where : cmd += [ , where ] self . Popen ( cmd )", "predicted": "extra", "expected": "extra_import_options"}
{"code": "def synthesize_speech ( self , input_data , voice , audio_config , retry = None , timeout = None ) : client = self . get_conn ( ) self . log . <mask> ( % input_data ) return client . synthesize_speech ( input_ = input_data , voice = voice , audio_config = audio_config , retry = retry , timeout = timeout )", "predicted": "debug", "expected": "info"}
{"code": "def _get_security_context ( self ) : <mask> = { } if self . kube_config . worker_run_as_user : security_context [ 'runAsUser' ] = self . kube_config . worker_run_as_user if self . kube_config . worker_fs_group : security_context [ 'fsGroup' ] = self . kube_config . worker_fs_group if self . kube_config . git_ssh_key_secret_name and security_context . get ( 'fsGroup' ) is None : security_context [ 'fsGroup' ] = 65533 return security_context", "predicted": "context", "expected": "security_context"}
{"code": "def get_extra_links ( self , operator , dttm ) : conn = BaseHook . get_connection ( operator . kwargs [ 'qubole_conn_id' ] ) if conn and conn . host : host = re . sub ( r'api$' , 'v2/analyze?command_id=' , conn . host ) else : host = 'https://api.qubole.com/v2/analyze?command_id=' ti = TaskInstance ( task = operator , execution_date = dttm ) qds_command_id = ti . <mask> ( task_ids = operator . task_id , key = 'qbol_cmd_id' ) url = host + str ( qds_command_id ) if qds_command_id else '' return url", "predicted": "get", "expected": "xcom_pull"}
{"code": "def start ( self ) : self . _process = DagFileProcessor . _launch_process ( self . _result_queue , self . file_path , self . _pickle_dags , self . _dag_id_white_list , . format ( self . _instance_id ) , self . <mask> ) self . _start_time = timezone . utcnow ( )", "predicted": "logger", "expected": "_zombies"}
{"code": "def _enqueue_task_instances_with_queued_state ( self , simple_dag_bag , simple_task_instances ) : TI = models . TaskInstance for <mask> in simple_task_instances : simple_dag = simple_dag_bag . get_dag ( simple_task_instance . dag_id ) command = TI . generate_command ( simple_task_instance . dag_id , simple_task_instance . task_id , simple_task_instance . execution_date , local = True , mark_success = False , ignore_all_deps = False , ignore_depends_on_past = False , ignore_task_deps = False , ignore_ti_state = False , pool = simple_task_instance . pool , file_path = simple_dag . full_filepath , pickle_id = simple_dag . pickle_id ) priority = simple_task_instance . priority_weight queue = simple_task_instance . queue self . log . info ( , simple_task_instance . key , priority , queue ) self . executor . queue_command ( simple_task_instance , command , priority = priority , queue = queue )", "predicted": "_", "expected": "simple_task_instance"}
{"code": "def _change_state_for_tasks_failed_to_execute ( self , session ) : if self . executor . queued_tasks : TI = models . TaskInstance filter_for_ti_state_change = ( [ and_ ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date , TI . _try_number == try_number - 1 , TI . state == State . QUEUED ) for dag_id , task_id , execution_date , try_number in self . executor . queued_tasks . <mask> ( ) ] ) ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) tis_to_set_to_scheduled = ( ti_query . with_for_update ( ) . all ( ) ) if len ( tis_to_set_to_scheduled ) == 0 : session . commit ( ) return for task_instance in tis_to_set_to_scheduled : task_instance . state = State . SCHEDULED task_instance_str = . join ( [ repr ( x ) for x in tis_to_set_to_scheduled ] ) session . commit ( ) self . log . info ( , task_instance_str )", "predicted": "items", "expected": "keys"}
{"code": "def _process_executor_events ( self , simple_dag_bag , session = None ) : TI = models . TaskInstance for key , state in list ( self . executor . get_event_buffer ( simple_dag_bag . dag_ids ) . items ( ) ) : dag_id , task_id , <mask> , try_number = key self . log . info ( , dag_id , task_id , execution_date , state , try_number ) if state == State . FAILED or state == State . SUCCESS : qry = session . query ( TI ) . filter ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date ) ti = qry . first ( ) if not ti : self . log . warning ( , ti ) continue if ti . try_number == try_number and ti . state == State . QUEUED : msg = ( . format ( ti , state , ti . state ) ) self . log . error ( msg ) try : simple_dag = simple_dag_bag . get_dag ( dag_id ) dagbag = models . DagBag ( simple_dag . full_filepath ) dag = dagbag . get_dag ( dag_id ) ti . task = dag . get_task ( task_id ) ti . handle_failure ( msg ) except Exception : self . log . error ( , ti ) ti . state = State . FAILED session . merge ( ti ) session . commit ( )", "predicted": "state", "expected": "execution_date"}
{"code": "def _apply_to_instance ( self , <mask> , instance_id , configuration_name , node_count , display_name , func ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id , configuration_name = configuration_name , node_count = node_count , display_name = display_name ) try : operation = func ( instance ) except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e if operation : result = operation . result ( ) self . log . info ( result )", "predicted": "project", "expected": "project_id"}
{"code": "def update_instance ( self , instance_id , <mask> , node_count , display_name , project_id = None ) : return self . _apply_to_instance ( project_id , instance_id , configuration_name , node_count , display_name , lambda x : x . update ( ) )", "predicted": "configuration", "expected": "configuration_name"}
{"code": "def <mask> ( self , instance_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id ) try : instance . delete ( ) return except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e", "predicted": "delete", "expected": "delete_instance"}
{"code": "def <mask> ( self , table , keys ) : keyspace = self . keyspace if '.' in table : keyspace , table = table . split ( '.' , 1 ) ks = . join ( . format ( key , key ) for key in keys . keys ( ) ) cql = . format ( keyspace = keyspace , table = table , keys = ks ) try : rs = self . get_conn ( ) . execute ( cql , keys ) return rs . one ( ) is not None except Exception : return False", "predicted": "etch", "expected": "record_exists"}
{"code": "def _build_track_driver_status_command ( self ) : connection_cmd = self . _get_spark_binary_path ( ) connection_cmd += [ , self . _connection [ 'master' ] ] if self . _driver_id : connection_cmd += [ , self . _driver_id ] else : raise <mask> ( + ) self . log . debug ( , connection_cmd ) return connection_cmd", "predicted": "Exception", "expected": "AirflowException"}
{"code": "def _process_spark_status_log ( self , itr ) : for line in itr : line = line . strip ( ) if in line : self . _driver_status = line . split ( <mask> ) [ 1 ] . replace ( ',' , '' ) . replace ( '\\\"' , '' ) . strip ( ) self . log . debug ( . format ( line ) )", "predicted": "''", "expected": "' : '"}
{"code": "def _write_local_schema_file ( self , cursor ) : schema_str = None schema_file_mime_type = 'application/json' tmp_schema_file_handle = NamedTemporaryFile ( delete = True ) if self . schema is not None and isinstance ( self . schema , string_types ) : schema_str = self . schema . encode ( 'utf-8' ) elif self . schema is not None and isinstance ( self . schema , list ) : schema_str = json . dumps ( self . schema ) . encode ( 'utf-8' ) else : schema = [ ] for field in cursor . description : field_name = field [ 0 ] field_type = self . type_map ( field [ 1 ] ) if field [ 6 ] or field_type == 'TIMESTAMP' : field_mode = 'NULLABLE' else : field_mode = 'REQUIRED' schema . append ( { <mask> : field_name , 'type' : field_type , 'mode' : field_mode , } ) schema_str = json . dumps ( schema , sort_keys = True ) . encode ( 'utf-8' ) tmp_schema_file_handle . write ( schema_str ) self . log . info ( 'Using schema for %s: %s' , self . schema_filename , schema_str ) schema_file_to_upload = { 'file_name' : self . schema_filename , 'file_handle' : tmp_schema_file_handle , 'file_mime_type' : schema_file_mime_type } return schema_file_to_upload", "predicted": "field", "expected": "'name'"}
{"code": "def create_perm_vm_for_all_dag ( self ) : for dag_vm in self . DAG_VMS : for perm in self . <mask> : self . _merge_perm ( permission_name = perm , view_menu_name = dag_vm )", "predicted": "permissions", "expected": "DAG_PERMS"}
{"code": "def get_conn ( self ) : connections = self . get_connections ( self . webhdfs_conn_id ) for connection in connections : try : self . log . debug ( 'Trying namenode %s' , connection . host ) client = self . <mask> ( connection ) client . status ( '/' ) self . log . debug ( 'Using namenode %s for hook' , connection . host ) return client except HdfsError as hdfs_error : self . log . debug ( 'Read operation on namenode %s failed with error: %s' , connection . host , hdfs_error ) hosts = [ connection . host for connection in connections ] error_message = 'Read operations failed on the namenodes below:\\n{hosts}' . format ( hosts = '\\n' . join ( hosts ) ) raise AirflowWebHDFSHookException ( error_message )", "predicted": "client", "expected": "_get_client"}
{"code": "def get_conn ( self ) : conn = self . get_connection ( self . pinot_broker_conn_id ) pinot_broker_conn = <mask> ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/pql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to pinot ' 'broker on {host}' . format ( host = conn . host ) ) return pinot_broker_conn", "predicted": "Connection", "expected": "connect"}
{"code": "def _convert_date_to_dict ( <mask> ) : return { DAY : field_date . day , MONTH : field_date . month , YEAR : field_date . year }", "predicted": "field", "expected": "field_date"}
{"code": "def get_pandas_df ( self , sql , parameters = None ) : import pandas . io . sql as psql with closing ( self . get_conn ( ) ) as conn : return psql . read_sql ( sql , <mask> = conn , params = parameters )", "predicted": "conn", "expected": "con"}
{"code": "def <mask> ( self , context ) : self . hook = SlackWebhookHook ( self . http_conn_id , self . webhook_token , self . message , self . attachments , self . channel , self . username , self . icon_emoji , self . link_names , self . proxy ) self . hook . execute ( )", "predicted": "er", "expected": "execute"}
{"code": "def fallback_to_default_project_id ( func ) : @ functools . wraps ( func ) def inner_wrapper ( self , * args , ** kwargs ) : if len ( args ) <mask> 0 : raise AirflowException ( ) if 'project_id' in kwargs : kwargs [ 'project_id' ] = self . _get_project_id ( kwargs [ 'project_id' ] ) else : kwargs [ 'project_id' ] = self . _get_project_id ( None ) if not kwargs [ 'project_id' ] : raise AirflowException ( ) return func ( self , * args , ** kwargs ) return inner_wrapper", "predicted": "==", "expected": ">"}
{"code": "def unfinished ( <mask> ) : return [ cls . NONE , cls . SCHEDULED , cls . QUEUED , cls . RUNNING , cls . SHUTDOWN , cls . UP_FOR_RETRY , cls . UP_FOR_RESCHEDULE ]", "predicted": "self", "expected": "cls"}
{"code": "def <mask> ( img , i , j , h , w , size , interpolation = Image . BILINEAR ) : assert _is_pil_image ( img ) , 'img should be PIL Image' img = crop ( img , i , j , h , w ) img = resize ( img , size , interpolation ) return img", "predicted": "crop", "expected": "resized_crop"}
{"code": "def vflip ( img ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) return img . transpose ( Image . <mask> )", "predicted": "trans", "expected": "FLIP_TOP_BOTTOM"}
{"code": "def adjust_hue ( img , <mask> ) : if not ( - 0.5 <= hue_factor <= 0.5 ) : raise ValueError ( 'hue_factor is not in [-0.5, 0.5].' . format ( hue_factor ) ) if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) input_mode = img . mode if input_mode in { 'L' , '1' , 'I' , 'F' } : return img h , s , v = img . convert ( 'HSV' ) . split ( ) np_h = np . array ( h , dtype = np . uint8 ) with np . errstate ( over = 'ignore' ) : np_h += np . uint8 ( hue_factor * 255 ) h = Image . fromarray ( np_h , 'L' ) img = Image . merge ( 'HSV' , ( h , s , v ) ) . convert ( input_mode ) return img", "predicted": "hue", "expected": "hue_factor"}
{"code": "def affine ( img , angle , translate , scale , shear , <mask> = 0 , fillcolor = None ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) assert isinstance ( translate , ( tuple , list ) ) and len ( translate ) == 2 , assert scale > 0.0 , output_size = img . size center = ( img . size [ 0 ] * 0.5 + 0.5 , img . size [ 1 ] * 0.5 + 0.5 ) matrix = _get_inverse_affine_matrix ( center , angle , translate , scale , shear ) kwargs = { : fillcolor } if PILLOW_VERSION [ 0 ] == '5' else { } return img . transform ( output_size , Image . AFFINE , matrix , resample , ** kwargs )", "predicted": "res", "expected": "resample"}
{"code": "def accuracy ( output , target , topk = ( 1 , ) ) : with torch . no_grad ( ) : maxk = max ( topk ) batch_size = target . size ( 0 ) _ , pred = output . topk ( maxk , 1 , True , True ) pred = pred . t ( ) correct = pred . eq ( target [ None ] ) res = [ ] for k in topk : <mask> = correct [ : k ] . flatten ( ) . sum ( dtype = torch . float32 ) res . append ( correct_k * ( 100.0 / batch_size ) ) return res", "predicted": "k", "expected": "correct_k"}
{"code": "def download_url ( url , root , filename = None , md5 = None ) : from six . moves import urllib root = os . path . expanduser ( root ) if not filename : filename = os . path . basename ( url ) fpath = os . path . join ( root , filename ) makedir_exist_ok ( root ) if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : print ( 'Using downloaded and verified file: ' + fpath ) else : try : print ( 'Downloading ' + url + ' to ' + fpath ) urllib . request . <mask> ( url , fpath , reporthook = gen_bar_updater ( ) ) except OSError : if url [ : 5 ] == 'https' : url = url . replace ( 'https:' , 'http:' ) print ( 'Failed download. Trying https -> http instead.' ' Downloading ' + url + ' to ' + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) )", "predicted": "download", "expected": "urlretrieve"}
{"code": "def <mask> ( x , data = None , summarize = None , message = None , name = None ) : with tf . compat . v2 . name_scope ( name or 'assert_finite' ) : x_ = tf . get_static_value ( x ) if x_ is not None : if ~ np . all ( np . isfinite ( x_ ) ) : raise ValueError ( message ) return x assertion = tf . compat . v1 . assert_equal ( tf . math . is_finite ( x ) , tf . ones_like ( x , tf . bool ) , data = data , summarize = summarize , message = message ) with tf . control_dependencies ( [ assertion ] ) : return tf . identity ( x )", "predicted": "inite", "expected": "assert_finite"}
{"code": "def _event_size ( event_shape , name = None ) : with tf . compat . v1 . name_scope ( name , 'event_size' , [ event_shape ] ) : event_shape = tf . convert_to_tensor ( value = event_shape , dtype = tf . int32 , name = 'event_shape' ) event_shape_const = tf . get_static_value ( event_shape ) if event_shape_const is not None : return <mask> . prod ( event_shape_const ) else : return tf . reduce_prod ( input_tensor = event_shape )", "predicted": "tf", "expected": "np"}
{"code": "def params_size ( num_components , <mask> , name = None ) : with tf . compat . v1 . name_scope ( name , 'MixtureSameFamily_params_size' , [ num_components , component_params_size ] ) : num_components = tf . convert_to_tensor ( value = num_components , name = 'num_components' , dtype_hint = tf . int32 ) component_params_size = tf . convert_to_tensor ( value = component_params_size , name = 'component_params_size' ) num_components = dist_util . prefer_static_value ( num_components ) component_params_size = dist_util . prefer_static_value ( component_params_size ) return num_components + num_components * component_params_size", "predicted": "value", "expected": "component_params_size"}
{"code": "def leapfrog_step ( leapfrog_step_state : LeapFrogStepState , step_size : FloatTensor , target_log_prob_fn : PotentialFn , kinetic_energy_fn : PotentialFn ) -> Tuple [ LeapFrogStepState , LeapFrogStepExtras ] : state = leapfrog_step_state . state <mask> = leapfrog_step_state . state_grads momentum = leapfrog_step_state . momentum step_size = maybe_broadcast_structure ( step_size , state ) state = tf . nest . map_structure ( tf . convert_to_tensor , state ) momentum = tf . nest . map_structure ( tf . convert_to_tensor , momentum ) state = tf . nest . map_structure ( tf . convert_to_tensor , state ) if state_grads is None : _ , _ , state_grads = call_and_grads ( target_log_prob_fn , state ) else : state_grads = tf . nest . map_structure ( tf . convert_to_tensor , state_grads ) momentum = tf . nest . map_structure ( lambda m , sg , s : m + 0.5 * sg * s , momentum , state_grads , step_size ) kinetic_energy , kinetic_energy_extra , momentum_grads = call_and_grads ( kinetic_energy_fn , momentum ) state = tf . nest . map_structure ( lambda x , mg , s : x + mg * s , state , momentum_grads , step_size ) target_log_prob , state_extra , state_grads = call_and_grads ( target_log_prob_fn , state ) momentum = tf . nest . map_structure ( lambda m , sg , s : m + 0.5 * sg * s , momentum , state_grads , step_size ) return LeapFrogStepState ( state , state_grads , momentum ) , LeapFrogStepExtras ( target_log_prob , state_extra , kinetic_energy , kinetic_energy_extra )", "predicted": "_", "expected": "state_grads"}
{"code": "def _expand_to_event_rank ( self , x ) : expanded_x = x for _ in range ( tensorshape_util . rank ( self . <mask> ) ) : expanded_x = tf . expand_dims ( expanded_x , - 1 ) return expanded_x", "predicted": "x", "expected": "event_shape"}
{"code": "def _cat_probs ( self , log_probs ) : which_softmax = tf . <mask> . log_softmax if log_probs else tf . nn . softmax cat_probs = which_softmax ( self . cat . logits ) cat_probs = tf . unstack ( cat_probs , num = self . num_components , axis = - 1 ) return cat_probs", "predicted": "losses", "expected": "nn"}
{"code": "def logistic_regression ( features ) : coeffs = ed . MultivariateNormalDiag ( loc = tf . zeros ( features . shape [ 1 ] ) , name = ) labels = ed . <mask> ( logits = tf . tensordot ( features , coeffs , [ [ 1 ] , [ 0 ] ] ) , name = ) return labels", "predicted": "Linear", "expected": "Bernoulli"}
{"code": "def cholesky_covariance ( x , sample_axis = <mask> , keepdims = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'cholesky_covariance' , values = [ x , sample_axis ] ) : sample_axis = tf . convert_to_tensor ( value = sample_axis , dtype = tf . int32 ) cov = covariance ( x , sample_axis = sample_axis , event_axis = - 1 , keepdims = keepdims ) return tf . linalg . cholesky ( cov )", "predicted": "None", "expected": "0"}
{"code": "def _inv_z ( self , z ) : with tf . name_scope ( ) : return z * self . scale + self . <mask>", "predicted": "scale", "expected": "loc"}
{"code": "def _machine_eps ( <mask> ) : if isinstance ( dtype , tf . DType ) : dtype = dtype . as_numpy_dtype ( ) return np . finfo ( dtype ) . eps", "predicted": "self", "expected": "dtype"}
{"code": "def _bracket_and_search ( value_and_gradients_function , init_interval , f_lim , max_iterations , shrinkage_param , expansion_param , <mask> , curvature_param ) : bracket_result = hzl . bracket ( value_and_gradients_function , init_interval , f_lim , max_iterations , expansion_param ) converged = init_interval . converged | _very_close ( bracket_result . left . x , bracket_result . right . x ) exhausted_iterations = ~ converged & tf . greater_equal ( bracket_result . iteration , max_iterations ) line_search_args = HagerZhangLineSearchResult ( converged = converged , failed = bracket_result . failed | exhausted_iterations , iterations = bracket_result . iteration , func_evals = bracket_result . num_evals , left = bracket_result . left , right = bracket_result . right ) return _line_search_after_bracketing ( value_and_gradients_function , line_search_args , init_interval . left , f_lim , max_iterations , sufficient_decrease_param , curvature_param , shrinkage_param )", "predicted": "None", "expected": "sufficient_decrease_param"}
{"code": "def _line_search_inner_bisection ( value_and_gradients_function , search_interval , active , f_lim ) : midpoint = ( search_interval . left . x + search_interval . right . x ) / 2 val_mid = value_and_gradients_function ( midpoint ) is_valid_mid = hzl . is_finite ( val_mid ) still_active = active & is_valid_mid new_failed = active & ~ is_valid_mid next_inteval = search_interval . _replace ( failed = search_interval . failed | new_failed , <mask> = search_interval . func_evals + 1 ) def _apply_update ( ) : update_result = hzl . update ( value_and_gradients_function , next_inteval . left , next_inteval . right , val_mid , f_lim , active = still_active ) return HagerZhangLineSearchResult ( converged = next_inteval . converged , failed = next_inteval . failed | update_result . failed , iterations = next_inteval . iterations + update_result . iteration , func_evals = next_inteval . func_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) return prefer_static . cond ( tf . reduce_any ( input_tensor = still_active ) , _apply_update , lambda : next_inteval )", "predicted": "iterations", "expected": "func_evals"}
{"code": "def _prepare_args ( value_and_gradients_function , initial_step_size , val_initial , val_0 , approximate_wolfe_threshold ) : eval_count = 0 if val_initial is None : if initial_step_size is not None : initial_step_size = tf . <mask> ( value = initial_step_size ) else : initial_step_size = tf . convert_to_tensor ( value = 1.0 , dtype = tf . float32 ) val_initial = value_and_gradients_function ( initial_step_size ) eval_count += 1 if val_0 is None : x_0 = tf . zeros_like ( val_initial . x ) val_0 = value_and_gradients_function ( x_0 ) eval_count += 1 f_lim = val_0 . f + ( approximate_wolfe_threshold * tf . abs ( val_0 . f ) ) return val_0 , val_initial , f_lim , tf . convert_to_tensor ( value = eval_count )", "predicted": "constant", "expected": "convert_to_tensor"}
{"code": "def _log_vector_matrix ( vs , ms ) : return tf . <mask> ( input_tensor = vs [ ... , tf . newaxis ] + ms , axis = - 2 )", "predicted": "Matrix", "expected": "reduce_logsumexp"}
{"code": "def _marginal_hidden_probs ( self ) : initial_log_probs = tf . broadcast_to ( self . _log_init , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . _num_states ] ] , axis = 0 ) ) if self . _num_steps > 1 : transition_log_probs = self . _log_trans def forward_step ( log_probs , _ ) : return _log_vector_matrix ( log_probs , transition_log_probs ) dummy_index = tf . zeros ( self . _num_steps - 1 , <mask> = tf . float32 ) forward_log_probs = tf . scan ( forward_step , dummy_index , initializer = initial_log_probs , name = ) forward_log_probs = tf . concat ( [ [ initial_log_probs ] , forward_log_probs ] , axis = 0 ) else : forward_log_probs = initial_log_probs [ tf . newaxis , ... ] return tf . exp ( forward_log_probs )", "predicted": "type", "expected": "dtype"}
{"code": "def _build_trainable_posterior ( param , initial_loc_fn ) : loc = tf . compat . v1 . get_variable ( param . name + '_loc' , initializer = lambda : initial_loc_fn ( param ) , <mask> = param . prior . dtype , use_resource = True ) scale = tf . nn . softplus ( tf . compat . v1 . get_variable ( param . name + '_scale' , initializer = lambda : - 4 * tf . ones_like ( initial_loc_fn ( param ) ) , dtype = param . prior . dtype , use_resource = True ) ) q = tfd . Normal ( loc = loc , scale = scale ) if ( param . prior . event_shape . ndims is None or param . prior . event_shape . ndims > 0 ) : q = tfd . Independent ( q , reinterpreted_batch_ndims = param . prior . event_shape . ndims ) return tfd . TransformedDistribution ( q , param . bijector )", "predicted": "type", "expected": "dtype"}
{"code": "def broadcast_batch_shape ( distributions ) : batch_shape = distributions [ 0 ] . batch_shape for distribution in distributions : batch_shape = tf . broadcast_static_shape ( batch_shape , distribution . batch_shape ) if batch_shape . is_fully_defined ( ) : return batch_shape . as_list ( ) batch_shape = distributions [ 0 ] . <mask> ( ) for distribution in distributions : batch_shape = tf . broadcast_dynamic_shape ( batch_shape , distribution . batch_shape_tensor ( ) ) return tf . convert_to_tensor ( value = batch_shape )", "predicted": "shape", "expected": "batch_shape_tensor"}
{"code": "def factored_joint_mvn ( distributions ) : graph_parents = [ tensor for distribution in distributions for tensor in distribution . _graph_parents ] with tf . compat . v1 . name_scope ( 'factored_joint_mvn' , values = graph_parents ) : <mask> = tf . debugging . assert_same_float_dtype ( distributions ) broadcast_ones = tf . ones ( broadcast_batch_shape ( distributions ) , dtype = dtype ) [ ... , tf . newaxis ] return MultivariateNormalLinearOperator ( loc = tf . concat ( [ mvn . mean ( ) * broadcast_ones for mvn in distributions ] , axis = - 1 ) , scale = tfl . LinearOperatorBlockDiag ( [ mvn . scale for mvn in distributions ] , is_square = True ) )", "predicted": "distributions", "expected": "dtype"}
{"code": "def empirical_statistics ( observed_time_series ) : with tf . compat . v1 . name_scope ( 'empirical_statistics' , values = [ observed_time_series ] ) : [ observed_time_series , mask ] = canonicalize_observed_time_series_with_mask ( observed_time_series ) squeezed_series = observed_time_series [ ... , 0 ] if mask is None : observed_mean , observed_variance = tf . nn . moments ( x = squeezed_series , axes = - 1 ) <mask> = squeezed_series [ ... , 0 ] else : broadcast_mask = tf . broadcast_to ( tf . cast ( mask , tf . bool ) , tf . shape ( input = squeezed_series ) ) observed_mean , observed_variance = ( missing_values_util . moments_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) try : observed_initial = ( missing_values_util . initial_value_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) except NotImplementedError : tf . compat . v1 . logging . warn ( 'Cannot compute initial values for a masked time series' 'with dynamic shape; using the mean instead. This will' 'affect heuristic priors and may change the results of' 'inference.' ) observed_initial = observed_mean observed_stddev = tf . sqrt ( observed_variance ) observed_initial_centered = observed_initial - observed_mean return observed_mean , observed_stddev , observed_initial_centered", "predicted": "mask", "expected": "observed_initial"}
{"code": "def _maybe_expand_trailing_dim ( observed_time_series_tensor ) : with tf . compat . v1 . name_scope ( 'maybe_expand_trailing_dim' , values = [ observed_time_series_tensor ] ) : if ( observed_time_series_tensor . shape . <mask> is not None and tf . compat . dimension_value ( observed_time_series_tensor . shape [ - 1 ] ) is not None ) : expanded_time_series = ( observed_time_series_tensor if observed_time_series_tensor . shape [ - 1 ] == 1 else observed_time_series_tensor [ ... , tf . newaxis ] ) else : expanded_time_series = tf . cond ( pred = tf . equal ( tf . shape ( input = observed_time_series_tensor ) [ - 1 ] , 1 ) , true_fn = lambda : observed_time_series_tensor , false_fn = lambda : observed_time_series_tensor [ ... , tf . newaxis ] ) return expanded_time_series", "predicted": "shape", "expected": "ndims"}
{"code": "def canonicalize_observed_time_series_with_mask ( maybe_masked_observed_time_series ) : with tf . compat . v1 . name_scope ( 'canonicalize_observed_time_series_with_mask' ) : if hasattr ( maybe_masked_observed_time_series , 'is_missing' ) : observed_time_series = ( maybe_masked_observed_time_series . time_series ) is_missing = maybe_masked_observed_time_series . is_missing else : observed_time_series = maybe_masked_observed_time_series is_missing = None observed_time_series = tf . convert_to_tensor ( value = observed_time_series , name = <mask> ) observed_time_series = _maybe_expand_trailing_dim ( observed_time_series ) if is_missing is not None : is_missing = tf . convert_to_tensor ( value = is_missing , name = 'is_missing' , dtype_hint = tf . bool ) return missing_values_util . MaskedTimeSeries ( observed_time_series , is_missing = is_missing )", "predicted": "name", "expected": "'observed_time_series'"}
{"code": "def range ( self , name = ) : with self . _name_scope ( name ) : return self . high <mask> self . low", "predicted": ",", "expected": "-"}
{"code": "def _make_summary_statistic ( <mask> ) : def _fn ( self ) : if any ( self . _dist_fn_args ) : raise ValueError ( 'Can only compute ' + attr + ' when all distributions are ' 'independent; {}' . format ( self . model ) ) return self . _unflatten ( getattr ( d ( ) , attr ) ( ) for d in self . _dist_fn_wrapped ) return _fn", "predicted": "self", "expected": "attr"}
{"code": "def _build ( self , model ) : if not <mask> ( model , collections . Sequence ) : raise TypeError ( '`model` must be `list`-like (saw: {}).' . format ( type ( model ) . __name__ ) ) self . _dist_fn = model self . _dist_fn_wrapped , self . _dist_fn_args = zip ( * [ _unify_call_signature ( i , dist_fn ) for i , dist_fn in enumerate ( model ) ] )", "predicted": "instance", "expected": "isinstance"}
{"code": "def _resolve_graph ( self , distribution_names = None , leaf_name = 'x' ) : if distribution_names is None or any ( self . _dist_fn_args ) : distribution_names = _resolve_distribution_names ( self . _dist_fn_args , distribution_names , leaf_name ) if len ( set ( distribution_names ) ) != len ( distribution_names ) : raise <mask> ( 'Distribution names must be unique: {}' . format ( distribution_names ) ) if len ( distribution_names ) != len ( self . _dist_fn_wrapped ) : raise ValueError ( 'Distribution names must be 1:1 with `rvs`.' ) return tuple ( zip ( distribution_names , tuple ( ( ) if a is None else a for a in self . _dist_fn_args ) ) )", "predicted": "Exception", "expected": "ValueError"}
{"code": "def <mask> ( f ) : @ functools . wraps ( f ) def _check_arg_and_apply_f ( * args , ** kwargs ) : dist = args [ 0 ] x = args [ 1 ] with tf . control_dependencies ( [ assert_util . assert_greater_equal ( x , dist . loc , message = ) ] if dist . validate_args else [ ] ) : return f ( * args , ** kwargs ) return _check_arg_and_apply_f", "predicted": "inner", "expected": "check_arg_in_support"}
{"code": "def visualize_reconstruction ( inputs , reconstruct , num = 3 , name = ) : reconstruct = tf . clip_by_value ( reconstruct , 0. , 1. ) inputs_and_reconstruct = tf . concat ( ( inputs [ : num ] , reconstruct [ : num ] ) , axis = <mask> ) image_summary ( inputs_and_reconstruct , name )", "predicted": "1", "expected": "0"}
{"code": "def zero_state ( self , sample_batch_shape = ( ) ) : h0 = tf . <mask> ( [ 1 , self . hidden_size ] ) c0 = tf . zeros ( [ 1 , self . hidden_size ] ) combined_shape = tf . concat ( ( tf . convert_to_tensor ( value = sample_batch_shape , dtype = tf . int32 ) , [ self . dimensions ] ) , axis = - 1 ) previous_output = tf . zeros ( combined_shape ) return previous_output , ( h0 , c0 )", "predicted": "ones", "expected": "zeros"}
{"code": "def reconstruct ( self , inputs , samples = 1 , sample_static = False , sample_dynamic = False , swap_static = False , swap_dynamic = False , fix_static = False , fix_dynamic = False ) : batch_size = tf . shape ( input = inputs ) [ - 5 ] length = len ( tf . unstack ( inputs , axis = - 4 ) ) features = self . compressor ( inputs ) if sample_static : static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) else : static_sample , _ = self . sample_static_posterior ( features , samples ) if swap_static : static_sample = tf . reverse ( static_sample , axis = [ 1 ] ) if sample_dynamic : <mask> , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) else : dynamic_sample , _ = self . sample_dynamic_posterior ( features , samples , static_sample ) if swap_dynamic : dynamic_sample = tf . reverse ( dynamic_sample , axis = [ 1 ] ) likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) return likelihood", "predicted": "samples", "expected": "dynamic_sample"}
{"code": "def make_state_space_model ( self , num_timesteps , param_vals = None , initial_state_prior = None , initial_step = <mask> ) : return self . _make_state_space_model ( num_timesteps = num_timesteps , param_map = self . _canonicalize_param_vals_as_map ( param_vals ) , initial_state_prior = initial_state_prior , initial_step = initial_step )", "predicted": "None", "expected": "0"}
{"code": "def prior_sample ( self , num_timesteps , initial_step = 0 , params_sample_shape = ( ) , <mask> = ( ) , seed = None ) : seed = distributions . SeedStream ( seed , salt = 'StructuralTimeSeries_prior_sample' ) with tf . compat . v1 . name_scope ( 'prior_sample' , values = [ num_timesteps , params_sample_shape , trajectories_sample_shape ] ) : param_samples = [ p . prior . sample ( params_sample_shape , seed = seed ( ) , name = p . name ) for p in self . parameters ] model = self . make_state_space_model ( num_timesteps = num_timesteps , initial_step = initial_step , param_vals = param_samples ) return model . sample ( trajectories_sample_shape , seed = seed ( ) ) , param_samples", "predicted": "trajectory", "expected": "trajectories_sample_shape"}
{"code": "def _argsort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise <mask> ( 'Unrecognized direction: {}.' . format ( direction ) ) return np . argsort ( values , axis , kind = 'stable' if stable else 'quicksort' )", "predicted": "Exception", "expected": "ValueError"}
{"code": "def erfinv ( x , name = ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = ) if dtype_util . as_numpy_dtype ( x . dtype ) not in [ np . float32 , np . float64 ] : raise TypeError ( . format ( dtype_util . name ( x . dtype ) ) ) return ndtri ( ( x + 1. ) <mask> 2. ) / np . sqrt ( 2. )", "predicted": "**", "expected": "/"}
{"code": "def text_messages_joint_log_prob ( count_data , lambda_1 , lambda_2 , tau ) : alpha = ( 1. / tf . reduce_mean ( input_tensor = count_data ) ) rv_lambda = <mask> . Exponential ( rate = alpha ) rv_tau = tfd . Uniform ( ) lambda_ = tf . gather ( [ lambda_1 , lambda_2 ] , indices = tf . cast ( tau * tf . cast ( tf . size ( input = count_data ) , dtype = tf . float32 ) <= tf . cast ( tf . range ( tf . size ( input = count_data ) ) , dtype = tf . float32 ) , dtype = tf . int32 ) ) rv_observation = tfd . Poisson ( rate = lambda_ ) return ( rv_lambda . log_prob ( lambda_1 ) + rv_lambda . log_prob ( lambda_2 ) + rv_tau . log_prob ( tau ) + tf . reduce_sum ( input_tensor = rv_observation . log_prob ( count_data ) ) )", "predicted": "tf", "expected": "tfd"}
{"code": "def get_marginal_distribution ( self , index_points = None ) : with self . _name_scope ( 'get_marginal_distribution' ) : index_points = self . _get_index_points ( index_points ) covariance = self . _compute_covariance ( index_points ) loc = self . _mean_fn ( index_points ) if self . _is_univariate_marginal ( index_points ) : scale = tf . sqrt ( covariance ) loc = tf . squeeze ( loc , axis = - 1 ) return normal . Normal ( loc = loc , scale = scale , validate_args = self . _validate_args , <mask> = self . _allow_nan_stats , name = 'marginal_distribution' ) else : scale = tf . linalg . LinearOperatorLowerTriangular ( tf . linalg . cholesky ( _add_diagonal_shift ( covariance , self . jitter ) ) , is_non_singular = True , name = 'GaussianProcessScaleLinearOperator' ) return mvn_linear_operator . MultivariateNormalLinearOperator ( loc = loc , scale = scale , validate_args = self . _validate_args , allow_nan_stats = self . _allow_nan_stats , name = 'marginal_distribution' )", "predicted": "allow", "expected": "allow_nan_stats"}
{"code": "def decompose_forecast_by_component ( model , forecast_dist , parameter_samples ) : with tf . compat . v1 . name_scope ( 'decompose_forecast_by_component' ) : try : forecast_lgssm = forecast_dist . components_distribution forecast_latent_mean , _ = forecast_lgssm . _joint_mean ( ) <mask> , _ = forecast_lgssm . _joint_covariances ( ) except AttributeError as e : raise ValueError ( 'Forecast distribution must be a MixtureSameFamily of' 'LinearGaussianStateSpaceModel distributions, such as returned by' '`tfp.sts.forecast()`. (saw exception: {})' . format ( e ) ) forecast_latent_mean = dist_util . move_dimension ( forecast_latent_mean , source_idx = - 3 , dest_idx = 0 ) forecast_latent_covs = dist_util . move_dimension ( forecast_latent_covs , source_idx = - 4 , dest_idx = 0 ) return _decompose_from_posterior_marginals ( model , forecast_latent_mean , forecast_latent_covs , parameter_samples )", "predicted": "_", "expected": "forecast_latent_covs"}
{"code": "def <mask> ( self ) : if self . _value is None : try : self . _value = self . distribution . sample ( self . sample_shape_tensor ( ) ) except NotImplementedError : raise NotImplementedError ( . format ( self . distribution . __class__ . __name__ ) ) return self . _value", "predicted": "sample", "expected": "value"}
{"code": "def real_nvp_default_template ( hidden_layers , shift_only = False , activation = tf . nn . relu , name = None , * args , ** kwargs ) : with tf . compat . v2 . name_scope ( name or ) : def _fn ( x , output_units , ** <mask> ) : if condition_kwargs : raise NotImplementedError ( ) if tensorshape_util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , ... ] reshape_output = lambda x : x [ 0 ] else : reshape_output = lambda x : x for units in hidden_layers : x = tf . compat . v1 . layers . dense ( inputs = x , units = units , activation = activation , * args , ** kwargs ) x = tf . compat . v1 . layers . dense ( inputs = x , units = ( 1 if shift_only else 2 ) * output_units , activation = None , * args , ** kwargs ) if shift_only : return reshape_output ( x ) , None shift , log_scale = tf . split ( x , 2 , axis = - 1 ) return reshape_output ( shift ) , reshape_output ( log_scale ) return tf . compat . v1 . make_template ( , _fn )", "predicted": "args", "expected": "condition_kwargs"}
{"code": "def _effective_sample_size_single_state ( states , filter_beyond_lag , filter_threshold ) : with tf . compat . v1 . name_scope ( 'effective_sample_size_single_state' , values = [ states , filter_beyond_lag , filter_threshold ] ) : states = tf . convert_to_tensor ( value = states , name = 'states' ) dt = states . dtype auto_corr = stats . auto_correlation ( states , axis = 0 , max_lags = filter_beyond_lag ) if filter_threshold is not None : filter_threshold = tf . convert_to_tensor ( value = filter_threshold , dtype = dt , name = 'filter_threshold' ) mask = auto_corr < filter_threshold mask = tf . cast ( mask , dtype = dt ) mask = tf . cumsum ( mask , axis = 0 ) mask = tf . maximum ( 1. - mask , 0. ) auto_corr *= mask n = _axis_size ( states , axis = 0 ) k = tf . range ( 0. , _axis_size ( auto_corr , axis = 0 ) ) <mask> = ( n - k ) / n if auto_corr . shape . ndims is not None : new_shape = [ - 1 ] + [ 1 ] * ( auto_corr . shape . ndims - 1 ) else : new_shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto_corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) nk_factor = tf . reshape ( nk_factor , new_shape ) return n / ( - 1 + 2 * tf . reduce_sum ( input_tensor = nk_factor * auto_corr , axis = 0 ) )", "predicted": "n", "expected": "nk_factor"}
{"code": "def _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'left_doubling_increments' , [ batch_shape , max_doublings , step_size ] ) : step_size = tf . convert_to_tensor ( value = step_size ) dtype = step_size . dtype . base_dtype output_shape = tf . concat ( ( [ max_doublings + 1 ] , batch_shape ) , axis = 0 ) expand_left = distributions . Bernoulli ( 0.5 , dtype = dtype ) . <mask> ( sample_shape = output_shape , seed = seed ) width_multipliers = tf . cast ( 2 ** tf . range ( 0 , max_doublings + 1 ) , dtype = dtype ) widths_shape = tf . concat ( ( [ max_doublings + 1 ] , tf . ones_like ( batch_shape ) ) , axis = 0 ) width_multipliers = tf . reshape ( width_multipliers , shape = widths_shape ) widths = width_multipliers * step_size left_increments = tf . cumsum ( widths * expand_left , exclusive = True , axis = 0 ) return left_increments , widths", "predicted": "expand", "expected": "sample"}
{"code": "def serialize_function ( func ) : if isinstance ( func , types . LambdaType ) : return generic_utils . func_dump ( func ) , 'lambda' return func . <mask> , 'function'", "predicted": "func", "expected": "__name__"}
{"code": "def _validate_block_sizes ( <mask> , bijectors , validate_args ) : block_sizes_shape = block_sizes . shape if tensorshape_util . is_fully_defined ( block_sizes_shape ) : if ( tensorshape_util . rank ( block_sizes_shape ) != 1 or ( tensorshape_util . num_elements ( block_sizes_shape ) != len ( bijectors ) ) ) : raise ValueError ( '`block_sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block_sizes_shape , len ( bijectors ) ) ) return block_sizes elif validate_args : message = ( '`block_sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) with tf . control_dependencies ( [ assert_util . assert_equal ( tf . size ( input = block_sizes ) , len ( bijectors ) , message = message ) , assert_util . assert_equal ( tf . rank ( block_sizes ) , 1 ) ] ) : return tf . identity ( block_sizes ) else : return block_sizes", "predicted": "self", "expected": "block_sizes"}
{"code": "def poisson ( x , layer_fn = tf . compat . v1 . layers . dense , log_rate_fn = lambda x : x , name = None ) : with tf . compat . v1 . name_scope ( name , 'poisson' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) log_rate = log_rate_fn ( tf . squeeze ( layer_fn ( x , <mask> ) , axis = - 1 ) ) return tfd . Poisson ( log_rate = log_rate )", "predicted": "name", "expected": "1"}
{"code": "def _maybe_call_volatility_fn_and_grads ( volatility_fn , state , volatility_fn_results = None , grads_volatility_fn = None , sample_shape = None , parallel_iterations = 10 ) : <mask> = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] needs_volatility_fn_gradients = grads_volatility_fn is None if volatility_fn_results is None : volatility_fn_results = volatility_fn ( * state_parts ) volatility_fn_results = ( list ( volatility_fn_results ) if mcmc_util . is_list_like ( volatility_fn_results ) else [ volatility_fn_results ] ) if len ( volatility_fn_results ) == 1 : volatility_fn_results *= len ( state_parts ) if len ( state_parts ) != len ( volatility_fn_results ) : raise ValueError ( '`volatility_fn` should return a tensor or a list ' 'of the same length as `current_state`.' ) volatility_fn_results = _maybe_broadcast_volatility ( volatility_fn_results , state_parts ) if grads_volatility_fn is None : [ _ , grads_volatility_fn , ] = diag_jacobian ( xs = state_parts , ys = volatility_fn_results , sample_shape = sample_shape , parallel_iterations = parallel_iterations , fn = volatility_fn ) if needs_volatility_fn_gradients : grads_volatility_fn = [ 2. * g * volatility if g is not None else tf . zeros_like ( fn_arg , dtype = fn_arg . dtype . base_dtype ) for g , volatility , fn_arg in zip ( grads_volatility_fn , volatility_fn_results , state_parts ) ] return volatility_fn_results , grads_volatility_fn", "predicted": "state", "expected": "state_parts"}
{"code": "def size ( dtype ) : dtype = <mask> . as_dtype ( dtype ) if hasattr ( dtype , 'size' ) : return dtype . size return np . dtype ( dtype ) . itemsize", "predicted": "np", "expected": "tf"}
{"code": "def _assert_same_base_type ( items , expected_type = None ) : r original_expected_type = expected_type mismatch = False for item in items : if item is not None : item_type = base_dtype ( item . dtype ) if not expected_type : expected_type = item_type elif expected_type != item_type : mismatch = True break if mismatch : expected_type = original_expected_type <mask> = None get_name = lambda x : x . name if hasattr ( x , 'name' ) else str ( x ) for item in items : if item is not None : item_type = base_dtype ( item . dtype ) if not expected_type : expected_type = item_type original_item_str = get_name ( item ) elif expected_type != item_type : raise ValueError ( '{}, type={}, must be of the same type ({}){}.' . format ( get_name ( item ) , item_type , expected_type , ( ( ' as {}' . format ( original_item_str ) ) if original_item_str else '' ) ) ) return expected_type else : return expected_type", "predicted": "item", "expected": "original_item_str"}
{"code": "def assert_same_float_dtype ( tensors = None , dtype = None ) : if tensors : dtype = _assert_same_base_type ( tensors , dtype ) if not dtype : dtype = <mask> . float32 elif not is_floating ( dtype ) : raise ValueError ( 'Expected floating point type, got {}.' . format ( dtype ) ) return dtype", "predicted": "np", "expected": "tf"}
{"code": "def _accept_reflected_fn ( simplex , objective_values , worst_index , reflected , <mask> ) : def _replace_worst_with_reflected ( ) : next_simplex = _replace_at_index ( simplex , worst_index , reflected ) next_objective_values = _replace_at_index ( objective_values , worst_index , objective_at_reflected ) return False , next_simplex , next_objective_values , 0 return _replace_worst_with_reflected", "predicted": "0", "expected": "objective_at_reflected"}
{"code": "def _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) : best_vertex = simplex [ best_index ] <mask> = best_vertex + shrinkage * ( simplex - best_vertex ) objective_at_shrunk_simplex , evals = _evaluate_objective_multiple ( objective_function , shrunk_simplex , batch_evaluate_objective ) return ( False , shrunk_simplex , objective_at_shrunk_simplex , evals )", "predicted": "shrunk", "expected": "shrunk_simplex"}
{"code": "def _check_convergence ( simplex , best_vertex , best_objective , worst_objective , func_tolerance , position_tolerance ) : objective_convergence = tf . abs ( worst_objective - best_objective ) < func_tolerance simplex_degeneracy = tf . <mask> ( input_tensor = tf . abs ( simplex - best_vertex ) ) < position_tolerance return objective_convergence | simplex_degeneracy", "predicted": "square", "expected": "reduce_max"}
{"code": "def _prepare_args ( objective_function , initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , batch_evaluate_objective ) : if objective_at_initial_simplex is not None and initial_simplex is None : raise <mask> ( '`objective_at_initial_simplex` specified but the' '`initial_simplex` was not.' ) if objective_at_initial_vertex is not None and initial_vertex is None : raise ValueError ( '`objective_at_initial_vertex` specified but the' '`initial_vertex` was not.' ) if initial_simplex is not None : if initial_vertex is not None : raise ValueError ( 'Both `initial_simplex` and `initial_vertex` specified.' ' Only one of the two should be specified.' ) if step_sizes is not None : raise ValueError ( '`step_sizes` must not be specified when an' ' `initial_simplex` has been specified.' ) return _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) if initial_vertex is None : raise ValueError ( 'One of `initial_simplex` or `initial_vertex`' ' must be supplied' ) if step_sizes is None : step_sizes = _default_step_sizes ( initial_vertex ) return _prepare_args_with_initial_vertex ( objective_function , initial_vertex , step_sizes , objective_at_initial_vertex , batch_evaluate_objective )", "predicted": "Error", "expected": "ValueError"}
{"code": "def _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) : initial_simplex = tf . convert_to_tensor ( value = initial_simplex ) num_vertices = tf . shape ( input = initial_simplex ) [ 0 ] dim = num_vertices - 1 num_evaluations = 0 if objective_at_initial_simplex is None : objective_at_initial_simplex , <mask> = _evaluate_objective_multiple ( objective_function , initial_simplex , batch_evaluate_objective ) num_evaluations += n_evals objective_at_initial_simplex = tf . convert_to_tensor ( value = objective_at_initial_simplex ) return ( dim , num_vertices , initial_simplex , objective_at_initial_simplex , num_evaluations )", "predicted": "evaluations", "expected": "n_evals"}
{"code": "def _evaluate_objective_multiple ( objective_function , arg_batch , <mask> ) : n_points = tf . shape ( input = arg_batch ) [ 0 ] if batch_evaluate_objective : return objective_function ( arg_batch ) , n_points return tf . map_fn ( objective_function , arg_batch ) , n_points", "predicted": "_", "expected": "batch_evaluate_objective"}
{"code": "def assign_log_moving_mean_exp ( log_mean_exp_var , log_value , decay , name = None ) : with tf . compat . v1 . name_scope ( name , , [ log_mean_exp_var , log_value , decay ] ) : with tf . compat . v1 . colocate_with ( log_mean_exp_var ) : base_dtype = log_mean_exp_var . dtype . base_dtype if not base_dtype . is_floating : raise TypeError ( . format ( base_dtype . name ) ) log_value = tf . <mask> ( value = log_value , dtype = base_dtype , name = ) decay = tf . convert_to_tensor ( value = decay , dtype = base_dtype , name = ) delta = ( log_value - log_mean_exp_var ) [ tf . newaxis , ... ] x = tf . concat ( [ tf . math . log ( decay ) * tf . ones_like ( delta ) , delta + tf . math . log1p ( - decay ) ] , axis = 0 ) x = tf . reduce_logsumexp ( input_tensor = x , axis = 0 ) return log_mean_exp_var . assign_add ( x )", "predicted": "constant", "expected": "convert_to_tensor"}
{"code": "def random_rademacher ( shape , <mask> = tf . float32 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'random_rademacher' , [ shape , seed ] ) : generation_dtype = tf . int64 if tf . as_dtype ( dtype ) != tf . int32 else tf . int32 random_bernoulli = tf . random . uniform ( shape , minval = 0 , maxval = 2 , dtype = generation_dtype , seed = seed ) return tf . cast ( 2 * random_bernoulli - 1 , dtype )", "predicted": "type", "expected": "dtype"}
{"code": "def _finish_log_prob_for_one_fiber ( self , y , x , ildj , event_ndims , ** distribution_kwargs ) : x = self . _maybe_rotate_dims ( x , rotate_right = True ) <mask> = self . distribution . log_prob ( x , ** distribution_kwargs ) if self . _is_maybe_event_override : log_prob = tf . reduce_sum ( input_tensor = log_prob , axis = self . _reduce_event_indices ) log_prob += tf . cast ( ildj , log_prob . dtype ) if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : tensorshape_util . set_shape ( log_prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) return log_prob", "predicted": "x", "expected": "log_prob"}
{"code": "def _undo_batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , name = None ) : r with tf . compat . <mask> . name_scope ( name or ) : rescale = tf . sqrt ( variance + variance_epsilon ) if scale is not None : rescale /= scale batch_unnormalized = x * rescale + ( mean - offset * rescale if offset is not None else mean ) return batch_unnormalized", "predicted": "tf", "expected": "v2"}
{"code": "def _apply_slice_sequence ( dist , <mask> , slice_overrides_seq ) : for slices , overrides in slice_overrides_seq : dist = _apply_single_step ( dist , params_event_ndims , slices , overrides ) return dist", "predicted": "params", "expected": "params_event_ndims"}
{"code": "def convergence_criteria_small_relative_norm_weights_change ( tolerance = 1e-5 , norm_order = 2 ) : def convergence_criteria_fn ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous , model_coefficients_next , predicted_linear_response_next , response , model , dispersion ) : relative_euclidean_norm = ( tf . norm ( tensor = model_coefficients_previous <mask> model_coefficients_next , ord = norm_order , axis = - 1 ) / ( 1. + tf . norm ( tensor = model_coefficients_previous , ord = norm_order , axis = - 1 ) ) ) return ( iter_ > 0 ) & tf . reduce_all ( input_tensor = relative_euclidean_norm < tolerance ) return convergence_criteria_fn", "predicted": "+", "expected": "-"}
{"code": "def _name_scope ( self , name = None , default_name = None , values = None ) : with tf . compat . v1 . name_scope ( self . name ) : with tf . compat . v1 . name_scope ( name , default_name , values = values or [ ] ) as scope : <mask> scope", "predicted": "with", "expected": "yield"}
{"code": "def is_diagonal_scale ( scale ) : if not <mask> ( scale , tf . linalg . LinearOperator ) : raise TypeError ( % scale ) return ( isinstance ( scale , tf . linalg . LinearOperatorIdentity ) or isinstance ( scale , tf . linalg . LinearOperatorScaledIdentity ) or isinstance ( scale , tf . linalg . LinearOperatorDiag ) )", "predicted": "instance", "expected": "isinstance"}
{"code": "def _smallest_integer_by_dtype ( dt ) : if not _is_known_dtype ( dt ) : raise TypeError ( . format ( dt . name ) ) if <mask> ( dt ) : return 0 return - 1 * _largest_integer_by_dtype ( dt )", "predicted": "empty", "expected": "_is_known_unsigned_by_dtype"}
{"code": "def _is_integer_like_by_dtype ( dt ) : if not <mask> ( dt ) : raise TypeError ( . format ( dt . name ) ) return dt . is_integer or dt . base_dtype == tf . bool", "predicted": "type", "expected": "_is_known_dtype"}
{"code": "def embed_check_categorical_event_shape ( categorical_param , name = ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = categorical_param , name = ) x_dtype = dtype_util . base_dtype ( x . dtype ) max_event_size = ( _largest_integer_by_dtype ( x_dtype ) if dtype_util . <mask> ( x_dtype ) else 0 ) if max_event_size is 0 : raise TypeError ( . format ( dtype_util . name ( x_dtype ) ) ) try : x_shape_static = tensorshape_util . with_rank_at_least ( x . shape , 1 ) except ValueError : raise ValueError ( ) event_size = tf . compat . dimension_value ( x_shape_static [ - 1 ] ) if event_size is not None : if event_size < 2 : raise ValueError ( ) if event_size > max_event_size : raise ValueError ( . format ( dtype_util . name ( x_dtype ) , event_size , max_event_size ) ) return x else : event_size = tf . shape ( input = x , out_type = tf . int64 , name = ) [ - 1 ] return with_dependencies ( [ assert_util . assert_rank_at_least ( x , 1 , message = ( ) ) , assert_util . assert_greater_equal ( tf . shape ( input = x ) [ - 1 ] , 2 , message = ( ) ) , assert_util . assert_less_equal ( event_size , tf . convert_to_tensor ( max_event_size , dtype = tf . int64 ) , message = . format ( dtype_util . name ( x_dtype ) , max_event_size ) ) , ] , x )", "predicted": "name", "expected": "is_floating"}
{"code": "def log_combinations ( n , counts , name = ) : with tf . name_scope ( name ) : n = tf . convert_to_tensor ( value = n , name = ) counts = tf . convert_to_tensor ( value = counts , name = ) total_permutations = tf . math . lgamma ( n + 1 ) counts_factorial = tf . math . lgamma ( counts + 1 ) <mask> = tf . reduce_sum ( input_tensor = counts_factorial , axis = [ - 1 ] ) return total_permutations - redundant_permutations", "predicted": "redundant", "expected": "redundant_permutations"}
{"code": "def process_quadrature_grid_and_probs ( quadrature_grid_and_probs , dtype , validate_args , name = None ) : with tf . name_scope ( name or ) : if quadrature_grid_and_probs is None : grid , probs = np . <mask> . hermite . hermgauss ( deg = 8 ) grid = grid . astype ( dtype_util . as_numpy_dtype ( dtype ) ) probs = probs . astype ( dtype_util . as_numpy_dtype ( dtype ) ) probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) grid = tf . convert_to_tensor ( value = grid , name = , dtype = dtype ) probs = tf . convert_to_tensor ( value = probs , name = , dtype = dtype ) return grid , probs grid , probs = tuple ( quadrature_grid_and_probs ) grid = tf . convert_to_tensor ( value = grid , name = , dtype = dtype ) probs = tf . convert_to_tensor ( value = probs , name = , dtype = dtype ) probs /= tf . norm ( tensor = probs , ord = 1 , axis = - 1 , keepdims = True , name = ) def _static_event_size ( x ) : return tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ - 1 ] ) m , n = _static_event_size ( probs ) , _static_event_size ( grid ) if m is not None and n is not None : if m != n : raise ValueError ( . format ( m , n ) ) elif validate_args : assertions = [ assert_util . assert_equal ( dimension_size ( probs , axis = - 1 ) , dimension_size ( grid , axis = - 1 ) , message = ( ) ) , ] with tf . control_dependencies ( assertions ) : grid = tf . identity ( grid ) probs = tf . identity ( probs ) return grid , probs", "predicted": "random", "expected": "polynomial"}
{"code": "def parent_frame_arguments ( ) : arg_names , variable_arg_name , keyword_arg_name , <mask> = ( tf_inspect . _inspect . getargvalues ( tf_inspect . _inspect . stack ( ) [ 1 ] [ 0 ] ) ) local_vars . pop ( variable_arg_name , { } ) keyword_args = local_vars . pop ( keyword_arg_name , { } ) final_args = { } for arg_name in arg_names : final_args [ arg_name ] = local_vars . pop ( arg_name ) final_args . update ( keyword_args ) return final_args", "predicted": "_", "expected": "local_vars"}
{"code": "def expand_to_vector ( x , tensor_name = None , op_name = None , validate_args = False ) : with tf . name_scope ( op_name or ) : x = tf . convert_to_tensor ( value = x , name = ) ndims = tensorshape_util . rank ( x . shape ) if ndims is None : if validate_args : x = with_dependencies ( [ assert_util . assert_rank_at_most ( x , 1 , message = ) ] , x ) ndims = tf . rank ( x ) expanded_shape = pick_vector ( tf . equal ( ndims , 0 ) , np . array ( [ 1 ] , dtype = np . int32 ) , tf . shape ( input = x ) ) return tf . reshape ( x , expanded_shape ) elif ndims == 0 : <mask> = tf . get_static_value ( x ) if x_const is not None : return tf . convert_to_tensor ( value = dtype_util . as_numpy_dtype ( x . dtype ) ( [ x_const ] ) , name = tensor_name ) else : return tf . reshape ( x , [ 1 ] ) elif ndims != 1 : raise ValueError ( ) return x", "predicted": "x", "expected": "x_const"}
{"code": "def _event_shape ( self , shape , static_perm_to_shape ) : rightmost_ = tf . get_static_value ( self . rightmost_transposed_ndims ) if tensorshape_util . rank ( shape ) is <mask> or rightmost_ is None : return tf . TensorShape ( None ) if tensorshape_util . rank ( shape ) < rightmost_ : raise ValueError ( 'Invalid shape: min event ndims={} but got {}' . format ( rightmost_ , shape ) ) perm_ = tf . get_static_value ( self . perm , partial = True ) if perm_ is None : return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( [ None ] * int ( rightmost_ ) ) if sum ( p is None for p in perm_ ) == 1 : present = np . argsort ( [ - 1 if p is None else p for p in perm_ ] ) for i , p in enumerate ( present [ 1 : ] ) : if i != p : perm_ = [ i if p is None else p for p in perm_ ] break return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( static_perm_to_shape ( shape [ tensorshape_util . rank ( shape ) - rightmost_ : ] , perm_ ) )", "predicted": "0", "expected": "None"}
{"code": "def <mask> ( x , other ) : return type ( x ) ( tf . TensorShape ( x ) . merge_with ( other ) )", "predicted": "merge", "expected": "merge_with"}
{"code": "def _check_equal_shape ( name , static_shape , dynamic_shape , static_target_shape , dynamic_target_shape = None ) : static_target_shape = tf . TensorShape ( static_target_shape ) if tensorshape_util . <mask> ( static_shape ) and tensorshape_util . is_fully_defined ( static_target_shape ) : if static_shape != static_target_shape : raise ValueError ( . format ( name , static_target_shape , static_shape ) ) return None else : if dynamic_target_shape is None : if tensorshape_util . is_fully_defined ( static_target_shape ) : dynamic_target_shape = tensorshape_util . as_list ( static_target_shape ) else : raise ValueError ( . format ( name , static_target_shape ) ) return assert_util . assert_equal ( dynamic_shape , dynamic_target_shape , message = ( . format ( name , static_target_shape ) ) )", "predicted": "any", "expected": "is_fully_defined"}
{"code": "def build_backward_pass_step ( get_transition_matrix_for_timestep ) : def backward_pass_step ( state , filtered_parameters ) : ( filtered_mean , filtered_cov , predicted_mean , predicted_cov ) = filtered_parameters transition_matrix = get_transition_matrix_for_timestep ( state . timestep ) next_posterior_mean = state . backward_mean next_posterior_cov = state . backward_cov posterior_mean , <mask> = backward_smoothing_update ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , next_posterior_mean , next_posterior_cov , transition_matrix ) return BackwardPassState ( backward_mean = posterior_mean , backward_cov = posterior_cov , timestep = state . timestep - 1 ) return backward_pass_step", "predicted": "cov", "expected": "posterior_cov"}
{"code": "def backward_smoothing_update ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , next_posterior_mean , next_posterior_cov , transition_matrix ) : tmp_gain_cov = transition_matrix . matmul ( filtered_cov ) predicted_cov_chol = tf . linalg . cholesky ( predicted_cov ) <mask> = tf . linalg . cholesky_solve ( predicted_cov_chol , tmp_gain_cov ) posterior_mean = ( filtered_mean + tf . linalg . matmul ( gain_transpose , next_posterior_mean - predicted_mean , adjoint_a = True ) ) posterior_cov = ( filtered_cov + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( next_posterior_cov - predicted_cov , gain_transpose ) , adjoint_a = True ) ) return ( posterior_mean , posterior_cov )", "predicted": "gain", "expected": "gain_transpose"}
{"code": "def build_kalman_mean_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : def mean_step ( <mask> , t ) : previous_latent_mean , _ = previous_means latent_mean = _propagate_mean ( previous_latent_mean , get_transition_matrix_for_timestep ( t - 1 ) , get_transition_noise_for_timestep ( t - 1 ) ) observation_mean = _propagate_mean ( latent_mean , get_observation_matrix_for_timestep ( t ) , get_observation_noise_for_timestep ( t ) ) return ( latent_mean , observation_mean ) return mean_step", "predicted": "_", "expected": "previous_means"}
{"code": "def _joint_mean ( self ) : with tf . name_scope ( ) : with tf . control_dependencies ( self . runtime_assertions ) : initial_latent_mean = _broadcast_to_shape ( self . initial_state_prior . mean ( ) [ ... , tf . newaxis ] , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , 1 ] ] , axis = 0 ) ) initial_observation_mean = _propagate_mean ( initial_latent_mean , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) mean_step = build_kalman_mean_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) ( <mask> , observation_means ) = tf . scan ( mean_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent_mean , initial_observation_mean ) ) latent_means = tf . concat ( [ initial_latent_mean [ tf . newaxis , ... ] , latent_means ] , axis = 0 ) observation_means = tf . concat ( [ initial_observation_mean [ tf . newaxis , ... ] , observation_means ] , axis = 0 ) latent_means = tf . squeeze ( latent_means , - 1 ) latent_means = distribution_util . move_dimension ( latent_means , 0 , - 2 ) observation_means = tf . squeeze ( observation_means , - 1 ) observation_means = distribution_util . move_dimension ( observation_means , 0 , - 2 ) return latent_means , observation_means", "predicted": "means", "expected": "latent_means"}
{"code": "def _joint_covariances ( self ) : with tf . name_scope ( ) : with tf . control_dependencies ( self . runtime_assertions ) : <mask> = _broadcast_to_shape ( self . initial_state_prior . covariance ( ) , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , self . latent_size ] ] , axis = 0 ) ) initial_observation_cov = _propagate_cov ( initial_latent_cov , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) cov_step = build_kalman_cov_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) ( latent_covs , observation_covs ) = tf . scan ( cov_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent_cov , initial_observation_cov ) ) latent_covs = tf . concat ( [ initial_latent_cov [ tf . newaxis , ... ] , latent_covs ] , axis = 0 ) observation_covs = tf . concat ( [ initial_observation_cov [ tf . newaxis , ... ] , observation_covs ] , axis = 0 ) latent_covs = distribution_util . move_dimension ( latent_covs , 0 , - 3 ) observation_covs = distribution_util . move_dimension ( observation_covs , 0 , - 3 ) return latent_covs , observation_covs", "predicted": "shape", "expected": "initial_latent_cov"}
{"code": "def latents_to_observations ( self , latent_means , latent_covs ) : with tf . name_scope ( ) : pushforward_latents_step = build_pushforward_latents_step ( self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) latent_means = distribution_util . move_dimension ( latent_means , source_idx = - 2 , dest_idx = 0 ) latent_means = latent_means [ ... , tf . newaxis ] latent_covs = distribution_util . move_dimension ( latent_covs , source_idx = - 3 , dest_idx = 0 ) ( initial_observation_mean , initial_observation_cov ) = pushforward_latents_step ( _ = None , latent_t_mean_cov = ( self . <mask> , latent_means [ self . initial_step ] , latent_covs [ self . initial_step ] ) ) timesteps = tf . range ( self . initial_step , self . initial_step + self . num_timesteps ) observation_means , observation_covs = tf . scan ( pushforward_latents_step , elems = ( timesteps , latent_means , latent_covs ) , initializer = ( initial_observation_mean , initial_observation_cov ) , parallel_iterations = 10000 ) observation_means = distribution_util . move_dimension ( observation_means [ ... , 0 ] , source_idx = 0 , dest_idx = - 2 ) observation_covs = distribution_util . move_dimension ( observation_covs , source_idx = 0 , dest_idx = - 3 ) return observation_means , observation_covs", "predicted": "T", "expected": "initial_step"}
{"code": "def _mode ( self ) : return ( self . <mask> + tf . zeros_like ( self . concentration ) [ ... , tf . newaxis ] )", "predicted": "concentration", "expected": "mean_direction"}
{"code": "def is_namedtuple_like ( x ) : try : for fn in x . _fields : _ = getattr ( x , fn ) return True except <mask> : return False", "predicted": "Exception", "expected": "AttributeError"}
{"code": "def make_innermost_setter ( setter ) : @ functools . wraps ( setter ) def _new_setter ( <mask> , * args , ** kwargs ) : results_stack = [ ] while hasattr ( kernel_results , 'inner_results' ) : results_stack . append ( kernel_results ) kernel_results = kernel_results . inner_results new_kernel_results = setter ( kernel_results , * args , ** kwargs ) for outer_results in reversed ( results_stack ) : new_kernel_results = outer_results . _replace ( inner_results = new_kernel_results ) return new_kernel_results return _new_setter", "predicted": "self", "expected": "kernel_results"}
{"code": "def get_initial_state_args ( value_and_gradients_function , initial_position , grad_tolerance , control_inputs = None ) : if control_inputs : with tf . control_dependencies ( control_inputs ) : f0 , df0 = value_and_gradients_function ( initial_position ) else : f0 , df0 = value_and_gradients_function ( initial_position ) <mask> = norm ( df0 , dims = 1 ) < grad_tolerance return dict ( converged = converged , failed = tf . zeros_like ( converged ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , num_objective_evaluations = tf . convert_to_tensor ( value = 1 ) , position = initial_position , objective_value = f0 , objective_gradient = df0 )", "predicted": "rejected", "expected": "converged"}
{"code": "def _check_convergence ( <mask> , next_position , current_objective , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : grad_converged = norm ( next_gradient , dims = 1 ) <= grad_tolerance x_converged = norm ( next_position - current_position , dims = 1 ) <= x_tolerance f_converged = ( norm ( next_objective - current_objective , dims = 0 ) <= f_relative_tolerance * current_objective ) return grad_converged | x_converged | f_converged", "predicted": "self", "expected": "current_position"}
{"code": "def _get_field ( <mask> , field_name ) : if hasattr ( kernel_results , field_name ) : return getattr ( kernel_results , field_name ) if hasattr ( kernel_results , 'accepted_results' ) : return getattr ( kernel_results . accepted_results , field_name ) raise TypeError ( 'Cannot extract %s from %s' % ( field_name , kernel_results ) )", "predicted": "self", "expected": "kernel_results"}
{"code": "def _secant2_inner_update ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : new_failed = initial_args . active & ~ is_finite ( val_c ) active = initial_args . active & ~ new_failed failed = initial_args . failed | new_failed found_wolfe = active & _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) <mask> = val_where ( found_wolfe , val_c , initial_args . left ) val_right = val_where ( found_wolfe , val_c , initial_args . right ) converged = initial_args . converged | found_wolfe active = active & ~ found_wolfe def _apply_update ( ) : update_result = update ( value_and_gradients_function , val_left , val_right , val_c , f_lim , active = active ) return _Secant2Result ( active = tf . zeros_like ( active ) , converged = converged , failed = failed | update_result . failed , num_evals = initial_args . num_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) def _default ( ) : return _Secant2Result ( active = active , converged = converged , failed = failed , num_evals = initial_args . num_evals , left = val_left , right = val_right ) return prefer_static . cond ( tf . reduce_any ( input_tensor = active ) , _apply_update , _default )", "predicted": "left", "expected": "val_left"}
{"code": "def is_finite ( val_1 , <mask> = None ) : val_1_finite = tf . math . is_finite ( val_1 . f ) & tf . math . is_finite ( val_1 . df ) if val_2 is not None : return val_1_finite & tf . math . is_finite ( val_2 . f ) & tf . math . is_finite ( val_2 . df ) return val_1_finite", "predicted": "beta", "expected": "val_2"}
{"code": "def make_decoder ( num_topics , num_words ) : topics_words_logits = tf . compat . v1 . get_variable ( , shape = [ num_topics , num_words ] , initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) topics_words = tf . nn . softmax ( topics_words_logits , axis = - 1 ) def decoder ( topics ) : word_probs = tf . matmul ( topics , topics_words ) return <mask> . OneHotCategorical ( probs = word_probs , name = ) return decoder , topics_words", "predicted": "tf", "expected": "tfd"}
{"code": "def make_prior ( num_topics , initial_value ) : def _softplus_inverse ( x ) : return np . log ( np . expm1 ( x ) ) logit_concentration = tf . compat . <mask> . get_variable ( , shape = [ 1 , num_topics ] , initializer = tf . compat . v1 . initializers . constant ( _softplus_inverse ( initial_value ) ) ) concentration = _clip_dirichlet_parameters ( tf . nn . softplus ( logit_concentration ) ) def prior ( ) : return tfd . Dirichlet ( concentration = concentration , name = ) prior_variables = [ logit_concentration ] return prior , prior_variables", "predicted": "train", "expected": "v1"}
{"code": "def _registered_kl ( type_a , type_b ) : hierarchy_a = tf_inspect . getmro ( type_a ) hierarchy_b = tf_inspect . getmro ( type_b ) dist_to_children = None kl_fn = None for mro_to_a , parent_a in enumerate ( hierarchy_a ) : for mro_to_b , parent_b in enumerate ( hierarchy_b ) : candidate_dist = mro_to_a + mro_to_b <mask> = _DIVERGENCES . get ( ( parent_a , parent_b ) , None ) if not kl_fn or ( candidate_kl_fn and candidate_dist < dist_to_children ) : dist_to_children = candidate_dist kl_fn = candidate_kl_fn return kl_fn", "predicted": "_", "expected": "candidate_kl_fn"}
{"code": "def create_random_seq ( character , action_metadata , direction , length = 8 ) : start = tf . random . uniform ( [ ] , <mask> = action_metadata [ 1 ] , dtype = tf . int32 ) return create_seq ( character , action_metadata , direction , length , start )", "predicted": "size", "expected": "maxval"}
{"code": "def _flatten_summand_list ( kernels ) : flattened = [ ] for k in kernels : if <mask> ( k , _SumKernel ) : flattened += k . kernels else : flattened . append ( k ) return flattened", "predicted": "is", "expected": "isinstance"}
{"code": "def histogram ( x , edges , axis = None , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'histogram' , values = [ x , edges , axis ] ) : in_dtype = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_dtype ) edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_dtype ) if axis is None : x = tf . reshape ( x , shape = [ - 1 ] ) else : x_ndims = _get_static_ndims ( x , expect_static = True , expect_ndims_at_least = 1 ) axis = _make_static_axis_non_negative_list ( axis , x_ndims ) if not axis : raise ValueError ( '`axis` cannot be empty.  Found: {}' . format ( axis ) ) x = _move_dims_to_flat_end ( x , axis , x_ndims , right_end = False ) bins = find_bins ( x , edges = edges , extend_lower_interval = extend_lower_interval , extend_upper_interval = extend_upper_interval , dtype = tf . int32 ) counts = count_integers ( bins , minlength = tf . shape ( input = edges ) [ 0 ] - 1 , <mask> = tf . shape ( input = edges ) [ 0 ] - 1 , axis = 0 , dtype = dtype or in_dtype ) n_edges = tf . compat . dimension_value ( edges . shape [ 0 ] ) if n_edges is not None : counts . set_shape ( tf . TensorShape ( [ n_edges - 1 ] ) . concatenate ( counts . shape [ 1 : ] ) ) return counts", "predicted": "max", "expected": "maxlength"}
{"code": "def amari_alpha ( logu , alpha = 1. , self_normalized = False , name = None ) : with tf . compat . v1 . <mask> ( name , , [ logu ] ) : if alpha is None or tf . is_tensor ( alpha ) : raise TypeError ( ) if ( self_normalized is None or tf . is_tensor ( self_normalized ) ) : raise TypeError ( ) logu = tf . convert_to_tensor ( value = logu , name = ) if alpha == 0. : f = - logu elif alpha == 1. : f = tf . exp ( logu ) * logu else : f = tf . math . expm1 ( alpha * logu ) / ( alpha * ( alpha - 1. ) ) if not self_normalized : return f if alpha == 0. : return f + tf . math . expm1 ( logu ) elif alpha == 1. : return f - tf . math . expm1 ( logu ) else : return f - tf . math . expm1 ( logu ) / ( alpha - 1. )", "predicted": "placeholder", "expected": "name_scope"}
{"code": "def kl_reverse ( logu , <mask> = False , name = None ) : with tf . compat . v1 . name_scope ( name , , [ logu ] ) : return amari_alpha ( logu , alpha = 0. , self_normalized = self_normalized )", "predicted": "normalized", "expected": "self_normalized"}
{"code": "def triangular ( <mask> , name = None ) : with tf . compat . v1 . name_scope ( name , , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ) return pearson ( logu ) / ( 1. + tf . exp ( logu ) )", "predicted": "self", "expected": "logu"}
{"code": "def jeffreys ( logu , name = None ) : with tf . compat . <mask> . name_scope ( name , , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ) return 0.5 * tf . math . expm1 ( logu ) * logu", "predicted": "tf", "expected": "v1"}
{"code": "def modified_gan ( <mask> , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ) y = tf . nn . softplus ( logu ) - logu if self_normalized : y += 0.5 * tf . math . expm1 ( logu ) return y", "predicted": "value", "expected": "logu"}
{"code": "def symmetrized_csiszar_function ( logu , <mask> , name = None ) : with tf . compat . v1 . name_scope ( name , , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ) return 0.5 * ( csiszar_function ( logu ) + dual_csiszar_function ( logu , csiszar_function ) )", "predicted": "*", "expected": "csiszar_function"}
{"code": "def monte_carlo_csiszar_f_divergence ( f , p_log_prob , q , num_draws , use_reparametrization = None , seed = None , name = None ) : reparameterization_types = tf . nest . flatten ( q . reparameterization_type ) with tf . compat . v1 . name_scope ( name , , [ num_draws ] ) : if use_reparametrization is None : use_reparametrization = all ( reparameterization_type == tfd . FULLY_REPARAMETERIZED for reparameterization_type in reparameterization_types ) elif ( use_reparametrization and any ( reparameterization_type != tfd . FULLY_REPARAMETERIZED for reparameterization_type in reparameterization_types ) ) : raise ValueError ( ) if not callable ( p_log_prob ) : raise TypeError ( ) return monte_carlo . expectation ( f = lambda <mask> : f ( p_log_prob ( q_samples ) - q . log_prob ( q_samples ) ) , samples = q . sample ( num_draws , seed = seed ) , log_prob = q . log_prob , use_reparametrization = use_reparametrization )", "predicted": "q", "expected": "q_samples"}
{"code": "def _assert_ndims_statically ( x , <mask> = None , expect_ndims_at_least = None , expect_static = False ) : ndims = x . shape . ndims if ndims is None : if expect_static : raise ValueError ( 'Expected static ndims. Found: {}' . format ( x ) ) return if expect_ndims is not None and ndims != expect_ndims : raise ValueError ( 'ndims must be {}.  Found: {}' . format ( expect_ndims , ndims ) ) if expect_ndims_at_least is not None and ndims < expect_ndims_at_least : raise ValueError ( 'ndims must be at least {}. Found {}' . format ( expect_ndims_at_least , ndims ) )", "predicted": "message", "expected": "expect_ndims"}
