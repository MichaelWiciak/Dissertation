{"code": "def dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r'qualities\":({.+?}),\"' ) ) title = match1 ( html , r'\"video_title\"\\s*:\\s*\"([^\"]+)\"' ) or match1 ( html , r'\"title\"\\s*:\\s*\"([^\"]+)\"' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real_url = info [ quality ] [ 1 ] [ ] if real_url : break except KeyError : pass mime , ext , size = url_info ( real_url ) print_info ( <mask> , title , mime , size ) if not info_only : download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge )", "predicted": "url", "expected": "site_info"}
{"code": "def sina_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : if 'news.sina.com.cn/zxt' in url : sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) return vid = match1 ( url , r'vid=(\\d+)' ) if vid is None : video_page = get_content ( url ) vid = hd_vid = match1 ( video_page , r'hd_vid\\s*:\\s*\\'([^\\']+)\\'' ) if hd_vid == '0' : vids = match1 ( video_page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video_page , r'vid:\"?(\\d+)\"?' ) if vid : sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) else : vkey = match1 ( video_page , <mask> ) if vkey is None : vid = match1 ( url , r'#(\\d+)' ) sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) return title = match1 ( video_page , r'title\\s*:\\s*\"([^\"]+)\"' ) sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only )", "predicted": "url", "expected": "r'vkey\\s*:\\s*\"([^\"]+)\"'"}
{"code": "def sprint ( text , * colors ) : return . <mask> ( . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS_ANSI_TERMINAL and colors else text", "predicted": "join", "expected": "format"}
{"code": "def print_log ( text , * colors ) : sys <mask> stderr . write ( sprint ( . format ( script_name , text ) , * colors ) + )", "predicted": "+", "expected": "."}
{"code": "def e ( message , exit_code = None ) : print_log ( message , <mask> , BOLD ) if exit_code is not None : sys . exit ( exit_code )", "predicted": "message", "expected": "YELLOW"}
{"code": "def <mask> ( message , exit_code = 1 ) : print_log ( message , RED , BOLD ) if exit_code is not None : sys . exit ( exit_code )", "predicted": "print", "expected": "wtf"}
{"code": "def detect_os ( ) : <mask> = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os", "predicted": "os", "expected": "syst"}
{"code": "def ckplayer_get_info_by_xml ( ckinfo ) : e = ET . XML ( ckinfo ) video_dict = { 'title' : '' , 'links' : [ <mask> , 'size' : 0 , 'flashvars' : '' , } dictified = dictify ( e ) [ 'ckplayer' ] if 'info' in dictified : if '_text' in dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] : video_dict [ 'title' ] = dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] [ '_text' ] . strip ( ) if '_text' in dictified [ 'video' ] [ 0 ] [ 'size' ] [ 0 ] : video_dict [ 'size' ] = sum ( [ int ( i [ 'size' ] [ 0 ] [ '_text' ] ) for i in dictified [ 'video' ] ] ) if '_text' in dictified [ 'video' ] [ 0 ] [ 'file' ] [ 0 ] : video_dict [ 'links' ] = [ i [ 'file' ] [ 0 ] [ '_text' ] . strip ( ) for i in dictified [ 'video' ] ] if '_text' in dictified [ 'flashvars' ] [ 0 ] : video_dict [ 'flashvars' ] = dictified [ 'flashvars' ] [ 0 ] [ '_text' ] . strip ( ) return video_dict", "predicted": "0", "expected": "]"}
{"code": "def get_mgtv_real_url ( url ) : content = loads ( get_content ( url ) ) m3u_url = content [ 'info' ] split = urlsplit ( m3u_url ) base_url = <mask> format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2 ] ) ) content = get_content ( content [ 'info' ] ) segment_list = [ ] segments_size = 0 for i in content . split ( ) : if not i . startswith ( '#' ) : segment_list . append ( base_url + i ) elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : segments_size += int ( i [ i . rfind ( ':' ) + 1 : ] ) return m3u_url , segments_size , segment_list", "predicted": "url", "expected": "."}
{"code": "def matchall ( text , patterns ) : ret = [ ] for pattern in patterns : match = re . findall ( pattern , text ) ret <mask> match return ret", "predicted": ":", "expected": "+="}
{"code": "def parse_query_param ( url , param ) : try : return parse <mask> parse_qs ( parse . urlparse ( url ) . query ) [ param ] [ 0 ] except : return None", "predicted": ":", "expected": "."}
{"code": "def get_content ( url , headers = { } , decoded = True ) : logging . debug ( 'get_content: %s' % url ) req = request . Request ( url , headers = headers ) if <mask> : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) response = urlopen_with_retry ( req ) data = response . read ( ) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) if decoded : charset = match1 ( response . getheader ( 'Content-Type' , '' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset , 'ignore' ) else : data = data . decode ( 'utf-8' , 'ignore' ) return data", "predicted": "headers", "expected": "cookies"}
{"code": "def post_content ( url , headers = { } , post_data = { } , decoded = True , ** kwargs ) : if kwargs . get ( 'post_data_raw' ) : logging . debug ( 'post_content: %s\\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) else : logging . debug ( 'post_content: %s\\npost_data: %s' % ( url , post_data ) ) req = <mask> . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) if kwargs . get ( 'post_data_raw' ) : post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) else : post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) response = urlopen_with_retry ( req , data = post_data_enc ) data = response . read ( ) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) if decoded : charset = match1 ( response . getheader ( 'Content-Type' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset ) else : data = data . decode ( 'utf-8' ) return data", "predicted": "req", "expected": "request"}
{"code": "def showroom_get_roomid_by_room_url_key ( room_url_key ) : fake_headers_mobile = { <mask> : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' , 'Accept-Charset' : 'UTF-8,*;q=0.5' , 'Accept-Encoding' : 'gzip,deflate,sdch' , 'Accept-Language' : 'en-US,en;q=0.8' , 'User-Agent' : 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36' } webpage_url = 'https://www.showroom-live.com/' + room_url_key html = get_content ( webpage_url , headers = fake_headers_mobile ) roomid = match1 ( html , r'room\\?room_id\\=(\\d+)' ) assert roomid return roomid", "predicted": "text", "expected": "'Accept'"}
{"code": "def _wanmen_get_title_by_json_topic_part ( json_content , tIndex , pIndex ) : return '_' . join ( <mask> json_content [ 0 ] [ 'name' ] , json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'name' ] , json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'name' ] ] )", "predicted": "", "expected": "["}
{"code": "def _get_aws_credentials ( self ) : if self . snowflake_conn_id : <mask> = self . get_connection ( self . snowflake_conn_id ) if 'aws_secret_access_key' in connection_object . extra_dejson : aws_access_key_id = connection_object . extra_dejson . get ( 'aws_access_key_id' ) aws_secret_access_key = connection_object . extra_dejson . get ( 'aws_secret_access_key' ) return aws_access_key_id , aws_secret_access_key", "predicted": "connection", "expected": "connection_object"}
{"code": "def _get_field ( self , field_name , default = None ) : <mask> = 'extra__grpc__{}' . format ( field_name ) if full_field_name in self . extras : return self . extras [ full_field_name ] else : return default", "predicted": "default", "expected": "full_field_name"}
{"code": "def execute ( self , context ) : hook <mask> GoogleCloudStorageHook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) hook . upload ( bucket_name = self . bucket , object_name = self . dst , mime_type = self . mime_type , filename = self . src , gzip = self . gzip , )", "predicted": ":", "expected": "="}
{"code": "def get_conn ( self ) : conn = self . get_connection ( self . mysql_conn_id ) conn_config = { : conn . login , : conn . password or '' , : conn . host or 'localhost' , : self . schema or conn . schema or '' } if not conn . port : conn_config [ ] = 3306 else : conn_config [ ] = int ( conn . port ) if conn . extra_dejson . get ( 'charset' , False ) : conn_config [ ] = conn . extra_dejson [ ] if ( conn_config [ ] ) . lower ( ) == 'utf8' or ( conn_config [ ] ) . lower ( ) == 'utf-8' : conn_config [ ] = True if conn . extra_dejson . get ( 'cursor' , False ) : if ( conn . extra_dejson [ ] ) . lower ( ) == 'sscursor' : conn_config [ ] = MySQLdb . cursors . SSCursor elif ( conn . extra_dejson [ ] ) . lower ( ) == 'dictcursor' : conn_config [ ] = MySQLdb . cursors . DictCursor elif ( conn . extra_dejson [ ] ) . lower ( ) == <mask> : conn_config [ ] = MySQLdb . cursors . SSDictCursor local_infile = conn . extra_dejson . get ( 'local_infile' , False ) if conn . extra_dejson . get ( 'ssl' , False ) : dejson_ssl = conn . extra_dejson [ 'ssl' ] if isinstance ( dejson_ssl , six . string_types ) : dejson_ssl = json . loads ( dejson_ssl ) conn_config [ 'ssl' ] = dejson_ssl if conn . extra_dejson . get ( 'unix_socket' ) : conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] if local_infile : conn_config [ ] = 1 conn = MySQLdb . connect ( ** conn_config ) return conn", "predicted": "''", "expected": "'ssdictcursor'"}
{"code": "def get_conn ( self ) : if not self <mask> _client : self . _client = Client ( credentials = self . _get_credentials ( ) ) return self . _client", "predicted": "?", "expected": "."}
{"code": "def create_instance ( self , body , <mask> = None ) : response = self . get_conn ( ) . instances ( ) . insert ( project = project_id , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )", "predicted": "data", "expected": "project_id"}
{"code": "def get_database ( self , instance , database , project_id = None ) : return self . get_conn ( ) . databases ( ) . get ( project = project_id , instance = instance , database = database ) . <mask> ( num_retries = self . num_retries )", "predicted": "get", "expected": "execute"}
{"code": "def start_proxy ( self ) : self . _download_sql_proxy_if_needed ( ) <mask> self . sql_proxy_process : raise AirflowException ( . format ( self . sql_proxy_process ) ) else : command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( self . command_line_parameters ) try : self . log . info ( , self . cloud_sql_proxy_socket_directory ) os . makedirs ( self . cloud_sql_proxy_socket_directory ) except OSError : pass command_to_run . extend ( self . _get_credential_parameters ( ) ) self . log . info ( , . join ( command_to_run ) ) self . sql_proxy_process = Popen ( command_to_run , stdin = PIPE , stdout = PIPE , stderr = PIPE ) self . log . info ( , self . sql_proxy_process . pid ) while True : line = self . sql_proxy_process . stderr . readline ( ) . decode ( 'utf-8' ) return_code = self . sql_proxy_process . poll ( ) if line == '' and return_code is not None : self . sql_proxy_process = None raise AirflowException ( . format ( return_code ) ) if line != '' : self . log . info ( line ) if in line or in line : self . stop_proxy ( ) raise AirflowException ( . format ( line ) ) if in line : return", "predicted": "?", "expected": "if"}
{"code": "def get_proxy_version ( self ) : self . _download_sql_proxy_if_needed ( ) command_to_run = [ self . <mask> ] command_to_run . extend ( [ '--version' ] ) command_to_run . extend ( self . _get_credential_parameters ( ) ) result = subprocess . check_output ( command_to_run ) . decode ( 'utf-8' ) pattern = re . compile ( ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None", "predicted": "command", "expected": "sql_proxy_path"}
{"code": "def get_sqlproxy_runner ( self ) : if not self . use_proxy : raise AirflowException ( ) return CloudSqlProxyRunner ( path_prefix = self . sql_proxy_unique_path <mask> instance_specification = self . _get_sqlproxy_instance_specification ( ) , project_id = self . project_id , sql_proxy_version = self . sql_proxy_version , sql_proxy_binary_path = self . sql_proxy_binary_path )", "predicted": "),", "expected": ","}
{"code": "def get_database_hook ( self ) : if self . <mask> == 'postgres' : self . db_hook = PostgresHook ( postgres_conn_id = self . db_conn_id , schema = self . database ) else : self . db_hook = MySqlHook ( mysql_conn_id = self . db_conn_id , schema = self . database ) return self . db_hook", "predicted": "database", "expected": "database_type"}
{"code": "def _normalize_mlengine_job_id ( job_id ) : match = re . search ( r'\\d|\\{{2}' , job_id ) if match and match . start ( ) == 0 : job = 'z_{}' . format ( job_id ) else : job = job_id tracker = 0 <mask> = '' for m in re . finditer ( r'\\{{2}.+?\\}{2}' , job ) : cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : m . start ( ) ] ) cleansed_job_id += job [ m . start ( ) : m . end ( ) ] tracker = m . end ( ) cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : ] ) return cleansed_job_id", "predicted": "]", "expected": "cleansed_job_id"}
{"code": "def clear_dag_task_instances ( ) : session = settings <mask> Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) for ti in tis : logging . info ( 'Deleting TaskInstance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( )", "predicted": ":", "expected": "."}
{"code": "def set_dags_paused_state ( <mask> ) : session = settings . Session ( ) dms = session . query ( DagModel ) . filter ( DagModel . dag_id . in_ ( DAG_IDS ) ) for dm in dms : logging . info ( 'Setting DAG :: {} is_paused={}' . format ( dm , is_paused ) ) dm . is_paused = is_paused session . commit ( )", "predicted": "session", "expected": "is_paused"}
{"code": "def print_stats ( self ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) successful_tis = [ x for x in tis if x . state == State . SUCCESS ] ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . <mask> ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) print ( 'Performance Results' ) print ( '###################' ) for dag_id in DAG_IDS : print ( 'DAG {}' . format ( dag_id ) ) print ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) print ( '###################' ) if len ( tis ) > len ( successful_tis ) : print ( ) print ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) session . commit ( )", "predicted": "total", "expected": "total_seconds"}
{"code": "def _convert_to_float_if_possible ( s ) : <mask> : ret = float ( s ) except ( ValueError , TypeError ) : ret = s return ret", "predicted": "float", "expected": "try"}
{"code": "def make_aware ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE if is_localized ( value ) : raise ValueError ( % value ) if hasattr ( value , 'fold' ) : value = value . replace ( <mask> = 1 ) if hasattr ( timezone , 'localize' ) : return timezone . localize ( value ) elif hasattr ( timezone , 'convert' ) : return timezone . convert ( value ) else : return value . replace ( tzinfo = timezone )", "predicted": "value", "expected": "fold"}
{"code": "def get_conn ( self ) : conn = self . get_connection ( self . druid_broker_conn_id ) druid_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( <mask> , '/druid/v2/sql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid_broker_conn", "predicted": "self", "expected": "'endpoint'"}
{"code": "def get_conn ( self , headers = None ) : session = requests . Session ( ) <mask> self . http_conn_id : conn = self . get_connection ( self . http_conn_id ) if in conn . host : self . base_url = conn . host else : schema = conn . schema if conn . schema else self . base_url = schema + + conn . host if conn . port : self . base_url = self . base_url + + str ( conn . port ) if conn . login : session . auth = ( conn . login , conn . password ) if conn . extra : try : session . headers . update ( conn . extra_dejson ) except TypeError : self . log . warn ( 'Connection to %s has invalid extra field.' , conn . host ) if headers : session . headers . update ( headers ) return session", "predicted": "?", "expected": "if"}
{"code": "def create_session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . <mask> ( ) raise finally : session . close ( )", "predicted": "close", "expected": "rollback"}
{"code": "def provide_session ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : arg_session = 'session' <mask> = func . __code__ . co_varnames session_in_args = arg_session in func_params and func_params . index ( arg_session ) < len ( args ) session_in_kwargs = arg_session in kwargs if session_in_kwargs or session_in_args : return func ( * args , ** kwargs ) else : with create_session ( ) as session : kwargs [ arg_session ] = session return func ( * args , ** kwargs ) return wrapper", "predicted": "]", "expected": "func_params"}
{"code": "def resetdb ( ) : from airflow import models from <mask> . migration import MigrationContext log . info ( ) models . base . Base . metadata . drop_all ( settings . engine ) mc = MigrationContext . configure ( settings . engine ) if mc . _version . exists ( settings . engine ) : mc . _version . drop ( settings . engine ) from flask_appbuilder . models . sqla import Base Base . metadata . drop_all ( settings . engine ) initdb ( )", "predicted": "flask", "expected": "alembic"}
{"code": "def _get_pretty_exception_message ( e ) : if ( hasattr ( e , 'message' ) and <mask> in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'errorName' ] , message = e . message [ 'message' ] ) ) else : return str ( e )", "predicted": "name", "expected": "'errorName'"}
{"code": "def run ( self , hql , parameters = None ) : return super ( ) . run ( self . <mask> ( hql ) , parameters )", "predicted": "run", "expected": "_strip_sql"}
{"code": "def insert_rows ( self , table , rows , target_fields = <mask> ) : super ( ) . insert_rows ( table , rows , target_fields , 0 )", "predicted": "0", "expected": "None"}
{"code": "def get_conn ( self ) : if self . <mask> is not None : return self . cosmos_client self . cosmos_client = cosmos_client . CosmosClient ( self . endpoint_uri , { 'masterKey' : self . master_key } ) return self . cosmos_client", "predicted": "client", "expected": "cosmos_client"}
{"code": "def does_collection_exist ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( ) <mask> = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { : , : [ { : , : collection_name } ] } ) ) if len ( existing_container ) == 0 : return False return True", "predicted": ")", "expected": "existing_container"}
{"code": "def delete_database ( self , <mask> ) : if database_name is None : raise AirflowBadRequest ( ) self . get_conn ( ) . DeleteDatabase ( get_database_link ( database_name ) )", "predicted": "database", "expected": "database_name"}
{"code": "def delete_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( ) self . <mask> ( ) . DeleteContainer ( get_collection_link ( self . __get_database_name ( database_name ) , collection_name ) )", "predicted": "type", "expected": "get_conn"}
{"code": "def delete_document ( self , document_id , database_name = None , collection_name = None ) <mask> if document_id is None : raise AirflowBadRequest ( ) self . get_conn ( ) . DeleteItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) )", "predicted": "", "expected": ":"}
{"code": "def get_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( ) try : return self . get_conn ( ) . <mask> ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) ) except HTTPFailure : return None", "predicted": "execute", "expected": "ReadItem"}
{"code": "def upload_function_zip ( self , location , zip_path , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . generateUploadUrl ( parent = self . _full_location ( project_id , location ) ) . execute ( num_retries = self . num_retries ) upload_url = response . get ( 'uploadUrl' ) with open ( zip_path , 'rb' ) <mask> fp : requests . put ( url = upload_url , data = fp , headers = { 'Content-type' : 'application/zip' , 'x-goog-content-length-range' : '0,104857600' , } ) return upload_url", "predicted": "?", "expected": "as"}
{"code": "def delete_function ( self , name ) : response <mask> self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . delete ( name = name ) . execute ( num_retries = self . num_retries ) operation_name = response [ ] self . _wait_for_operation_to_complete ( operation_name = operation_name )", "predicted": "[", "expected": "="}
{"code": "def is_met ( self , ti , session , dep_context = None ) : return all ( status . <mask> for status in self . get_dep_statuses ( ti , session , dep_context ) )", "predicted": "status", "expected": "passed"}
{"code": "def get_failure_reasons ( self , ti , session , dep_context = None ) : <mask> dep_status in self . get_dep_statuses ( ti , session , dep_context ) : if not dep_status . passed : yield dep_status . reason", "predicted": "yield", "expected": "for"}
{"code": "def _parse_s3_config ( config_file_name , config_format = 'boto' , profile = None ) : config = configparser . ConfigParser ( ) if config . read ( config_file_name ) : sections = config . sections ( ) else : raise AirflowException ( . format ( config_file_name ) ) if config_format is None : config_format = 'boto' conf_format = config_format . lower ( ) if conf_format == 'boto' : if profile is not None and 'profile ' + profile in sections : cred_section = 'profile ' + profile else : cred_section = 'Credentials' elif conf_format == 'aws' and profile is not None : cred_section = profile else : cred_section = 'default' if conf_format in ( 'boto' , 'aws' ) : key_id_option = 'aws_access_key_id' secret_key_option = <mask> else : key_id_option = 'access_key' secret_key_option = 'secret_key' if cred_section not in sections : raise AirflowException ( ) else : try : access_key = config . get ( cred_section , key_id_option ) secret_key = config . get ( cred_section , secret_key_option ) except Exception : logging . warning ( ) raise return access_key , secret_key", "predicted": "None", "expected": "'aws_secret_access_key'"}
{"code": "def start_in_async ( self ) : while True : loop_start_time = time . time ( ) if self . <mask> . poll ( ) : agent_signal = self . _signal_conn . recv ( ) if agent_signal == DagParsingSignal . TERMINATE_MANAGER : self . terminate ( ) break elif agent_signal == DagParsingSignal . END_MANAGER : self . end ( ) sys . exit ( os . EX_OK ) self . _refresh_dag_dir ( ) simple_dags = self . heartbeat ( ) for simple_dag in simple_dags : self . _result_queue . put ( simple_dag ) self . _print_stat ( ) all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) max_runs_reached = self . max_runs_reached ( ) dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) self . _stat_queue . put ( dag_parsing_stat ) if max_runs_reached : self . log . info ( , self . _max_runs ) break loop_duration = time . time ( ) - loop_start_time if loop_duration < 1 : sleep_length = 1 - loop_duration self . log . debug ( , sleep_length ) time . sleep ( sleep_length )", "predicted": "self", "expected": "_signal_conn"}
{"code": "def <mask> ( self ) : if ( ( timezone . utcnow ( ) - self . last_stat_print_time ) . total_seconds ( ) > self . print_stats_interval ) : if len ( self . _file_paths ) > 0 : self . _log_file_processing_stats ( self . _file_paths ) self . last_stat_print_time = timezone . utcnow ( )", "predicted": "print", "expected": "_print_stat"}
{"code": "def _log_file_processing_stats ( self , known_file_paths ) : headers = [ , , , , ] rows = [ ] for file_path in known_file_paths : last_runtime = self . get_last_runtime ( file_path ) file_name = os . path . basename ( file_path ) file_name = os . path . splitext ( file_name ) [ 0 ] . replace ( os . sep , '.' ) if last_runtime : Stats . gauge ( 'dag_processing.last_runtime.{}' . format ( file_name ) , last_runtime ) processor_pid = self . get_pid ( file_path ) processor_start_time = self . get_start_time ( file_path ) runtime = ( ( timezone . utcnow ( ) - processor_start_time ) . total_seconds ( ) if processor_start_time else None ) <mask> = self . get_last_finish_time ( file_path ) if last_run : seconds_ago = ( timezone . utcnow ( ) - last_run ) . total_seconds ( ) Stats . gauge ( 'dag_processing.last_run.seconds_ago.{}' . format ( file_name ) , seconds_ago ) rows . append ( ( file_path , processor_pid , runtime , last_runtime , last_run ) ) rows = sorted ( rows , key = lambda x : x [ 3 ] or 0.0 ) formatted_rows = [ ] for file_path , pid , runtime , last_runtime , last_run in rows : formatted_rows . append ( ( file_path , pid , . format ( runtime ) if runtime else None , . format ( last_runtime ) if last_runtime else None , last_run . strftime ( ) if last_run else None ) ) log_str = ( + * 80 + + + tabulate ( formatted_rows , headers = headers ) + + * 80 ) self . log . info ( log_str )", "predicted": "runtime", "expected": "last_run"}
{"code": "def end ( self ) : pids_to_kill = self . get_all_pids ( ) if len ( pids_to_kill ) > 0 : this_process = psutil . Process ( os . getpid ( ) ) child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] for child in child_processes : self . log . info ( , child . pid ) child . terminate ( ) timeout = 5 self . log . info ( , timeout ) try : psutil . <mask> ( child_processes , timeout = timeout , callback = lambda x : self . log . info ( 'Terminated PID %s' , x . pid ) ) except psutil . TimeoutExpired : self . log . debug ( ) child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] if len ( child_processes ) > 0 : self . log . info ( ) for child in child_processes : self . log . info ( , child . pid ) child . kill ( ) child . wait ( )", "predicted": "Process", "expected": "wait_procs"}
{"code": "def list_transfer_job ( self , filter ) : conn = self . get_conn ( ) filter = self . _inject_project_id ( filter , FILTER , FILTER_PROJECT_ID ) request = conn . transferJobs ( ) . list ( filter = json . dumps ( filter ) ) jobs = [ ] while request is not None : response = request . execute ( <mask> = self . num_retries ) jobs . extend ( response [ TRANSFER_JOBS ] ) request = conn . transferJobs ( ) . list_next ( previous_request = request , previous_response = response ) return jobs", "predicted": "retries", "expected": "num_retries"}
{"code": "<mask> pause_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferOperations ( ) . pause ( name = operation_name ) . execute ( num_retries = self . num_retries )", "predicted": "function", "expected": "def"}
{"code": "def find_for_task_instance ( task_instance , session ) : <mask> = TaskReschedule return ( session . query ( TR ) . filter ( TR . dag_id == task_instance . dag_id , TR . task_id == task_instance . task_id , TR . execution_date == task_instance . execution_date , TR . try_number == task_instance . try_number ) . order_by ( asc ( TR . id ) ) . all ( ) )", "predicted": "result", "expected": "TR"}
{"code": "def open_slots ( self , session ) : from airflow . models . taskinstance import TaskInstance as TI used_slots = session . query ( func . count ( ) ) . filter ( TI . pool == self . pool ) . filter ( TI . <mask> . in_ ( [ State . RUNNING , State . QUEUED ] ) ) . scalar ( ) return self . slots - used_slots", "predicted": "pool", "expected": "state"}
{"code": "def remove_option ( self , section , option , remove_default = True ) : if super ( ) <mask> has_option ( section , option ) : super ( ) . remove_option ( section , option ) if self . airflow_defaults . has_option ( section , option ) and remove_default : self . airflow_defaults . remove_option ( section , option )", "predicted": "?", "expected": "."}
{"code": "def getsection ( self , section ) : if ( section not in self . <mask> and section not in self . airflow_defaults . _sections ) : return None _section = copy . deepcopy ( self . airflow_defaults . _sections [ section ] ) if section in self . _sections : _section . update ( copy . deepcopy ( self . _sections [ section ] ) ) section_prefix = 'AIRFLOW__{S}__' . format ( S = section . upper ( ) ) for env_var in sorted ( os . environ . keys ( ) ) : if env_var . startswith ( section_prefix ) : key = env_var . replace ( section_prefix , '' ) . lower ( ) _section [ key ] = self . _get_env_var_option ( section , key ) for key , val in iteritems ( _section ) : try : val = int ( val ) except ValueError : try : val = float ( val ) except ValueError : if val . lower ( ) in ( 't' , 'true' ) : val = True elif val . lower ( ) in ( 'f' , 'false' ) : val = False _section [ key ] = val return _section", "predicted": "sections", "expected": "_sections"}
{"code": "def get_hostname ( ) : try : callable_path = conf . get ( 'core' , 'hostname_callable' ) except AirflowConfigException : callable_path = None <mask> not callable_path : return socket . getfqdn ( ) module_path , attr_name = callable_path . split ( ':' ) module = importlib . import_module ( module_path ) callable = getattr ( module , attr_name ) return callable ( )", "predicted": ",", "expected": "if"}
{"code": "def dispose_orm ( ) : log . debug ( , os . getpid ( ) ) global engine global Session if Session : Session . remove ( ) Session = None if engine : engine . <mask> ( ) engine = None", "predicted": "remove", "expected": "dispose"}
{"code": "def prepare_classpath ( ) : if DAGS_FOLDER not in sys . path : sys . path . append ( DAGS_FOLDER ) config_path = os . path . <mask> ( AIRFLOW_HOME , 'config' ) if config_path not in sys . path : sys . path . append ( config_path ) if PLUGINS_FOLDER not in sys . path : sys . path . append ( PLUGINS_FOLDER )", "predicted": "append", "expected": "join"}
{"code": "def detect_conf_var ( ) : ticket_cache = configuration . conf . get ( 'kerberos' , 'ccache' ) with open ( ticket_cache , 'rb' ) <mask> f : return b'X-CACHECONF:' in f . read ( )", "predicted": "?", "expected": "as"}
{"code": "def reduce_in_chunks ( fn , iterable , initializer , <mask> = 0 ) : if len ( iterable ) == 0 : return initializer if chunk_size == 0 : chunk_size = len ( iterable ) return reduce ( fn , chunks ( iterable , chunk_size ) , initializer )", "predicted": "chunks", "expected": "chunk_size"}
{"code": "def chain ( * tasks ) <mask> for up_task , down_task in zip ( tasks [ : - 1 ] , tasks [ 1 : ] ) : up_task . set_downstream ( down_task )", "predicted": "#", "expected": ":"}
{"code": "def pprinttable ( rows ) : if not rows : return if hasattr ( rows [ 0 ] , '_fields' ) : headers = rows [ 0 ] <mask> _fields else : headers = [ . format ( i ) for i in range ( len ( rows [ 0 ] ) ) ] lens = [ len ( s ) for s in headers ] for row in rows : for i in range ( len ( rows [ 0 ] ) ) : slenght = len ( . format ( row [ i ] ) ) if slenght > lens [ i ] : lens [ i ] = slenght formats = [ ] hformats = [ ] for i in range ( len ( rows [ 0 ] ) ) : if isinstance ( rows [ 0 ] [ i ] , int ) : formats . append ( % lens [ i ] ) else : formats . append ( % lens [ i ] ) hformats . append ( % lens [ i ] ) pattern = . join ( formats ) hpattern = . join ( hformats ) separator = . join ( [ '-' * n for n in lens ] ) s = s += separator + '\\n' s += ( hpattern % tuple ( headers ) ) + '\\n' s += separator + '\\n' def f ( t ) : return . format ( t ) if isinstance ( t , basestring ) else t for line in rows : s += pattern % tuple ( f ( t ) for t in line ) + '\\n' s += separator + '\\n' return s", "predicted": "if", "expected": "."}
{"code": "def get_conn ( self ) : http_authorized = self . _authorize ( ) return <mask> ( 'dataproc' , self . api_version , http = http_authorized , cache_discovery = False )", "predicted": "self", "expected": "build"}
{"code": "def run_cli ( self , pig , verbose = True ) : with TemporaryDirectory ( prefix = 'airflow_pigop_' ) as tmp_dir : with NamedTemporaryFile ( dir = tmp_dir ) as f : f . write ( pig . <mask> ( 'utf-8' ) ) f . flush ( ) fname = f . name pig_bin = 'pig' cmd_extra = [ ] pig_cmd = [ pig_bin , '-f' , fname ] + cmd_extra if self . pig_properties : pig_properties_list = self . pig_properties . split ( ) pig_cmd . extend ( pig_properties_list ) if verbose : self . log . info ( , . join ( pig_cmd ) ) sp = subprocess . Popen ( pig_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , cwd = tmp_dir , close_fds = True ) self . sp = sp stdout = '' for line in iter ( sp . stdout . readline , b'' ) : stdout += line . decode ( 'utf-8' ) if verbose : self . log . info ( line . strip ( ) ) sp . wait ( ) if sp . returncode : raise AirflowException ( stdout ) return stdout", "predicted": "decode", "expected": "encode"}
{"code": "def <mask> ( self , to_send_count ) : return max ( 1 , int ( math . ceil ( 1.0 * to_send_count / self . _sync_parallelism ) ) )", "predicted": "max", "expected": "_num_tasks_per_send_process"}
{"code": "def setdefault ( cls , key , default , deserialize_json = False ) : obj = Variable . get ( key , default_var = <mask> , deserialize_json = deserialize_json ) if obj is None : if default is not None : Variable . set ( key , default , serialize_json = deserialize_json ) return default else : raise ValueError ( 'Default Value must be set' ) else : return obj", "predicted": "default", "expected": "None"}
{"code": "def get_conn ( self ) : authed_http = self . _authorize ( ) <mask> build ( 'ml' , 'v1' , http = authed_http , cache_discovery = False )", "predicted": ":", "expected": "return"}
{"code": "def create_model ( self , <mask> , model ) : if not model [ 'name' ] : raise ValueError ( ) project = 'projects/{}' . format ( project_id ) request = self . _mlengine . projects ( ) . models ( ) . create ( parent = project , body = model ) return request . execute ( )", "predicted": "project", "expected": "project_id"}
{"code": "def get_default_executor ( ) : global DEFAULT_EXECUTOR if DEFAULT_EXECUTOR is not None : return DEFAULT_EXECUTOR executor_name = configuration . conf . get ( 'core' , <mask> ) DEFAULT_EXECUTOR = _get_executor ( executor_name ) log = LoggingMixin ( ) . log log . info ( , executor_name ) return DEFAULT_EXECUTOR", "predicted": ")", "expected": "'EXECUTOR'"}
{"code": "def on_error ( self , error , items ) : self . log . error ( 'Encountered Segment error: {segment_error} with ' <mask> . format ( segment_error = error , with_items = items ) ) raise AirflowException ( 'Segment error: {}' . format ( error ) )", "predicted": "'", "expected": "'items: {with_items}'"}
{"code": "def get_conn ( self ) : conn = self . get_connection ( self . mssql_conn_id ) conn = pymssql . connect ( server = conn . host , <mask> = conn . login , password = conn . password , database = self . schema or conn . schema , port = conn . port ) return conn", "predicted": "login", "expected": "user"}
{"code": "def delete_dag ( dag_id ) : try : count = delete <mask> delete_dag ( dag_id ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = . format ( err ) ) response . status_code = err . status_code return response return jsonify ( message = . format ( count ) , count = count )", "predicted": "|", "expected": "."}
{"code": "def task_info ( dag_id , task_id ) : try : info <mask> get_task ( dag_id , task_id ) except AirflowException as err : _log . info ( err ) response = jsonify ( error = . format ( err ) ) response . status_code = err . status_code return response fields = { k : str ( v ) for k , v in vars ( info ) . items ( ) if not k . startswith ( '_' ) } return jsonify ( fields )", "predicted": ":", "expected": "="}
{"code": "def get_pools ( ) : try : pools = pool_api . get_pools ( ) except AirflowException <mask> err : _log . error ( err ) response = jsonify ( error = . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( [ p . to_json ( ) for p in pools ] )", "predicted": "?", "expected": "as"}
{"code": "def <mask> ( ) : params = request . get_json ( force = True ) try : pool = pool_api . create_pool ( ** params ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( pool . to_json ( ) )", "predicted": "create", "expected": "create_pool"}
{"code": "def create_or_update ( self , <mask> , name , container_group ) : self . connection . container_groups . create_or_update ( resource_group , name , container_group )", "predicted": "connection", "expected": "resource_group"}
{"code": "def delete ( self , <mask> , name ) : self . connection . container_groups . delete ( resource_group , name )", "predicted": "group", "expected": "resource_group"}
{"code": "def find ( dag_id = None , run_id = None , execution_date = None , state = None , <mask> = None , no_backfills = False , session = None ) : DR = DagRun qry = session . query ( DR ) if dag_id : qry = qry . filter ( DR . dag_id == dag_id ) if run_id : qry = qry . filter ( DR . run_id == run_id ) if execution_date : if isinstance ( execution_date , list ) : qry = qry . filter ( DR . execution_date . in_ ( execution_date ) ) else : qry = qry . filter ( DR . execution_date == execution_date ) if state : qry = qry . filter ( DR . state == state ) if external_trigger is not None : qry = qry . filter ( DR . external_trigger == external_trigger ) if no_backfills : from airflow . jobs import BackfillJob qry = qry . filter ( DR . run_id . notlike ( BackfillJob . ID_PREFIX + '%' ) ) dr = qry . order_by ( DR . execution_date ) . all ( ) return dr", "predicted": "qry", "expected": "external_trigger"}
{"code": "def get_previous_scheduled_dagrun ( self , session = None ) : dag = self . get_dag ( ) return session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date == dag . previous_schedule ( self . execution_date ) ) . <mask> ( )", "predicted": "join", "expected": "first"}
{"code": "def verify_integrity ( self , session = None ) : from airflow . models . taskinstance import TaskInstance dag = self . get_dag ( ) tis = self . get_task_instances ( session = session ) task_ids = [ ] for ti in tis : task_ids . append ( ti . task_id ) task = None try : task = dag . get_task ( ti . task_id ) except AirflowException : if ti . state == State . REMOVED : pass elif self . state is not State . RUNNING and not dag . partial : self . log . warning ( . format ( ti , dag ) ) Stats . incr ( . format ( dag . dag_id ) , 1 , 1 ) ti . state = State . REMOVED is_task_in_dag = task is not None should_restore_task = is_task_in_dag and ti . state == State . REMOVED if should_restore_task : self . log . <mask> ( . format ( ti , dag ) ) Stats . incr ( . format ( dag . dag_id ) , 1 , 1 ) ti . state = State . NONE for task in six . itervalues ( dag . task_dict ) : if task . start_date > self . execution_date and not self . is_backfill : continue if task . task_id not in task_ids : Stats . incr ( . format ( task . __class__ . __name__ ) , 1 , 1 ) ti = TaskInstance ( task , self . execution_date ) session . add ( ti ) session . commit ( )", "predicted": "warning", "expected": "info"}
{"code": "def context_to_airflow_vars ( context , in_env_var_format = False ) : params = dict ( ) if in_env_var_format : name_format = 'env_var_format' else : name_format = 'default' <mask> = context . get ( 'task_instance' ) if task_instance and task_instance . dag_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_ID' ] [ name_format ] ] = task_instance . dag_id if task_instance and task_instance . task_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_TASK_ID' ] [ name_format ] ] = task_instance . task_id if task_instance and task_instance . execution_date : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_EXECUTION_DATE' ] [ name_format ] ] = task_instance . execution_date . isoformat ( ) dag_run = context . get ( 'dag_run' ) if dag_run and dag_run . run_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_RUN_ID' ] [ name_format ] ] = dag_run . run_id return params", "predicted": "]", "expected": "task_instance"}
{"code": "def send_metric ( self , metric_name , datapoint , tags = None , type_ = None , interval = None ) : response = api . <mask> . send ( metric = metric_name , points = datapoint , host = self . host , tags = tags , type = type_ , interval = interval ) self . validate_response ( response ) return response", "predicted": "self", "expected": "Metric"}
{"code": "def bag_dag ( <mask> , dag , parent_dag , root_dag ) : dag . test_cycle ( ) dag . resolve_template_files ( ) dag . last_loaded = timezone . utcnow ( ) for task in dag . tasks : settings . policy ( task ) subdags = dag . subdags try : for subdag in subdags : subdag . full_filepath = dag . full_filepath subdag . parent_dag = dag subdag . is_subdag = True self . bag_dag ( subdag , parent_dag = dag , root_dag = root_dag ) self . dags [ dag . dag_id ] = dag self . log . debug ( 'Loaded DAG %s' , dag ) except AirflowDagCycleException as cycle_exception : self . log . exception ( 'Exception bagging dag: %s' , dag . dag_id ) if dag == root_dag : for subdag in subdags : if subdag . dag_id in self . dags : del self . dags [ subdag . dag_id ] raise cycle_exception", "predicted": "dag", "expected": "self"}
{"code": "def poke ( self , context ) : sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) self . log . info ( 'Poking for %s to be a directory with files matching %s' , self . filepath , self . regex . pattern ) result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = False ) if f [ 'file_type' ] == 'f' and self . regex . match ( f [ 'path' ] . replace ( '%s/' % self . filepath , '' ) ) ] result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) result = self . filter_for_filesize ( result , self . <mask> ) return bool ( result )", "predicted": "filepath", "expected": "file_size"}
{"code": "def poke ( self , context ) : sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = True ) ] result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) result = self . filter_for_filesize ( result , self . file_size ) if self . be_empty : self . log . info ( 'Poking for filepath %s to a empty directory' , self . filepath ) <mask> len ( result ) == 1 and result [ 0 ] [ 'path' ] == self . filepath else : self . log . info ( 'Poking for filepath %s to a non empty directory' , self . filepath ) result . pop ( 0 ) return bool ( result ) and result [ 0 ] [ 'file_type' ] == 'f'", "predicted": "if", "expected": "return"}
{"code": "def are_dependents_done ( self , session = None ) : task = self . task if not task . downstream_task_ids : <mask> True ti = session . query ( func . count ( TaskInstance . task_id ) ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . task_id . in_ ( task . downstream_task_ids ) , TaskInstance . execution_date == self . execution_date , TaskInstance . state == State . SUCCESS , ) count = ti [ 0 ] [ 0 ] return count == len ( task . downstream_task_ids )", "predicted": "True", "expected": "return"}
{"code": "def next_retry_datetime ( self ) : delay = self . task . retry_delay if self . task . retry_exponential_backoff : min_backoff = int ( delay . total_seconds ( ) * ( 2 ** ( self . try_number - 2 ) ) ) hash = int ( hashlib . sha1 ( . format ( self . dag_id , self . task_id , self . execution_date , self . try_number ) . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) modded_hash = min_backoff + hash % min_backoff delay_backoff_in_seconds = min ( modded_hash , <mask> . max . total_seconds ( ) - 1 ) delay = timedelta ( seconds = delay_backoff_in_seconds ) if self . task . max_retry_delay : delay = min ( self . task . max_retry_delay , delay ) return self . end_date + delay", "predicted": "self", "expected": "timedelta"}
{"code": "def get_dagrun ( self , session ) : from airflow . models . dagrun import <mask> dr = session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date == self . execution_date ) . first ( ) return dr", "predicted": "", "expected": "DagRun"}
{"code": "def xcom_push ( self , key , value , <mask> = None ) : if execution_date and execution_date < self . execution_date : raise ValueError ( 'execution_date can not be in the past (current ' 'execution_date is {}; received {})' . format ( self . execution_date , execution_date ) ) XCom . set ( key = key , value = value , task_id = self . task_id , dag_id = self . dag_id , execution_date = execution_date or self . execution_date )", "predicted": "date", "expected": "execution_date"}
{"code": "def start_instance ( self , zone , resource_id , project_id = None ) : <mask> = self . get_conn ( ) . instances ( ) . start ( project = project_id , zone = zone , instance = resource_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ ] except KeyError : raise AirflowException ( . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )", "predicted": "client", "expected": "response"}
{"code": "def insert_instance_template ( self , body , request_id = None , project_id = None ) : response = self . get_conn ( ) . instanceTemplates ( ) . insert ( project = project_id , body = body , requestId = request_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ ] except KeyError : raise AirflowException ( . <mask> ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )", "predicted": "error", "expected": "format"}
{"code": "def check_for_bucket ( self , <mask> ) : try : self . get_conn ( ) . head_bucket ( Bucket = bucket_name ) return True except ClientError as e : self . log . info ( e . response [ ] [ ] ) return False", "predicted": "bucket", "expected": "bucket_name"}
{"code": "def create_bucket ( self , bucket_name , region_name = None ) : s3_conn = self . get_conn ( ) if not region_name : region_name = s3_conn . <mask> . region_name if region_name == 'us-east-1' : self . get_conn ( ) . create_bucket ( Bucket = bucket_name ) else : self . get_conn ( ) . create_bucket ( Bucket = bucket_name , CreateBucketConfiguration = { 'LocationConstraint' : region_name } )", "predicted": "'.'", "expected": "meta"}
{"code": "def check_for_key ( self , key , bucket_name = None ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) try : self . get_conn ( ) . head_object ( Bucket = bucket_name , Key = key ) return True except <mask> as e : self . log . info ( e . response [ ] [ ] ) return False", "predicted": "Exception", "expected": "ClientError"}
{"code": "def <mask> ( self , key , bucket_name = None , expression = 'SELECT * FROM S3Object' , expression_type = 'SQL' , input_serialization = None , output_serialization = None ) : if input_serialization is None : input_serialization = { 'CSV' : { } } if output_serialization is None : output_serialization = { 'CSV' : { } } if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) response = self . get_conn ( ) . select_object_content ( Bucket = bucket_name , Key = key , Expression = expression , ExpressionType = expression_type , InputSerialization = input_serialization , OutputSerialization = output_serialization ) return '' . join ( event [ 'Records' ] [ 'Payload' ] . decode ( 'utf-8' ) for event in response [ 'Payload' ] if 'Records' in event )", "predicted": "fetch", "expected": "select_key"}
{"code": "def _query_cassandra ( self ) : self <mask> hook = CassandraHook ( cassandra_conn_id = self . cassandra_conn_id ) session = self . hook . get_conn ( ) cursor = session . execute ( self . cql ) return cursor", "predicted": ":", "expected": "."}
{"code": "def execute ( self , context ) : self . _hook = SparkSqlHook ( sql = self . _sql , conf = self . _conf , <mask> = self . _conn_id , total_executor_cores = self . _total_executor_cores , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , keytab = self . _keytab , principal = self . _principal , name = self . _name , num_executors = self . _num_executors , master = self . _master , yarn_queue = self . _yarn_queue ) self . _hook . run_query ( )", "predicted": "id", "expected": "conn_id"}
{"code": "def execute ( self , context ) : self . hook = self . get_hook ( ) self . hook . get_conn ( ) self . <mask> [ 'Database' ] = self . database self . result_configuration [ 'OutputLocation' ] = self . output_location self . query_execution_id = self . hook . run_query ( self . query , self . query_execution_context , self . result_configuration , self . client_request_token ) query_status = self . hook . poll_query_status ( self . query_execution_id , self . max_tries ) if query_status in AWSAthenaHook . FAILURE_STATES : raise Exception ( 'Final state of Athena job is {}, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) ) elif not query_status or query_status in AWSAthenaHook . INTERMEDIATE_STATES : raise Exception ( 'Final state of Athena job is {}. ' 'Max tries of poll status exceeded, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) )", "predicted": "database", "expected": "query_execution_context"}
{"code": "def _query_mssql ( self ) : mssql = MsSqlHook ( mssql_conn_id = self . mssql_conn_id ) conn = mssql . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql ) <mask> cursor", "predicted": "+", "expected": "return"}
{"code": "def _parse_host ( host ) : urlparse_host <mask> urlparse . urlparse ( host ) . hostname if urlparse_host : return urlparse_host else : return host", "predicted": ":", "expected": "="}
{"code": "def _do_api_call ( self , endpoint_info , json ) : method , endpoint = endpoint_info url = 'https://{host}/{endpoint}' . format ( host = self . _parse_host ( self . databricks_conn . host ) , endpoint = endpoint ) if 'token' in self . databricks_conn . extra_dejson : self . log . info ( 'Using token auth.' ) auth = _TokenAuth ( self . databricks_conn . extra_dejson [ 'token' ] ) else : self . log . info ( 'Using basic auth.' ) auth = ( self . databricks_conn . login , self . databricks_conn . password ) if method == 'GET' : request_func = requests . get elif method == 'POST' : request_func = requests . post else : raise AirflowException ( 'Unexpected HTTP Method: ' + method ) attempt_num = <mask> while True : try : response = request_func ( url , json = json , auth = auth , headers = USER_AGENT_HEADER , timeout = self . timeout_seconds ) response . raise_for_status ( ) return response . json ( ) except requests_exceptions . RequestException as e : if not _retryable_error ( e ) : raise AirflowException ( 'Response: {0}, Status Code: {1}' . format ( e . response . content , e . response . status_code ) ) self . _log_request_error ( attempt_num , e ) if attempt_num == self . retry_limit : raise AirflowException ( ( 'API requests to Databricks failed {} times. ' + 'Giving up.' ) . format ( self . retry_limit ) ) attempt_num += 1 sleep ( self . retry_delay )", "predicted": "0", "expected": "1"}
{"code": "def _to_timestamp ( cls , column ) : try : column = pd . <mask> ( column ) except ValueError : log = LoggingMixin ( ) . log log . warning ( , column . name ) return column converted = [ ] for value in column : try : converted . append ( value . timestamp ( ) ) except ( ValueError , AttributeError ) : converted . append ( pd . np . NaN ) return pd . Series ( converted , index = column . index )", "predicted": "column", "expected": "to_datetime"}
{"code": "def get_collection ( self , mongo_collection , <mask> = None ) : mongo_db = mongo_db if mongo_db is not None else self . connection . schema mongo_conn = self . get_conn ( ) return mongo_conn . get_database ( mongo_db ) . get_collection ( mongo_collection )", "predicted": "db", "expected": "mongo_db"}
{"code": "def replace_many ( self , mongo_collection , docs , filter_docs = None , mongo_db = None , upsert = False , collation = None , ** kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) if not filter_docs : filter_docs = [ { '_id' : doc [ '_id' ] } for doc in docs ] requests = [ ReplaceOne ( filter_docs [ <mask> ] , docs [ i ] , upsert = upsert , collation = collation ) for i in range ( len ( docs ) ) ] return collection . bulk_write ( requests , ** kwargs )", "predicted": "docs", "expected": "i"}
{"code": "def has_mail_attachment ( self , name , mail_folder = 'INBOX' , <mask> = False ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only = True ) return len ( mail_attachments ) > 0", "predicted": "check", "expected": "check_regex"}
{"code": "def retrieve_mail_attachments ( self , name , mail_folder = 'INBOX' , check_regex = False , latest_only = False , not_found_mode = 'raise' ) : mail_attachments = self . <mask> ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode ( not_found_mode ) return mail_attachments", "predicted": "retrieve", "expected": "_retrieve_mails_attachments_by_name"}
{"code": "def download_mail_attachments ( self , name , <mask> , mail_folder = 'INBOX' , check_regex = False , latest_only = False , not_found_mode = 'raise' ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode ( not_found_mode ) self . _create_files ( mail_attachments , local_output_directory )", "predicted": "name", "expected": "local_output_directory"}
{"code": "def get_file ( self ) : return self . part . get_filename ( ) , self . part . get_payload ( decode = <mask> )", "predicted": "False", "expected": "True"}
{"code": "def _get_dep_statuses ( self , ti , session , dep_context ) : if dep_context <mask> ignore_in_reschedule_period : yield self . _passing_status ( reason = ) return if ti . state not in self . RESCHEDULEABLE_STATES : yield self . _passing_status ( reason = ) return task_reschedules = TaskReschedule . find_for_task_instance ( task_instance = ti ) if not task_reschedules : yield self . _passing_status ( reason = ) return now = timezone . utcnow ( ) next_reschedule_date = task_reschedules [ - 1 ] . reschedule_date if now >= next_reschedule_date : yield self . _passing_status ( reason = ) return yield self . _failing_status ( reason = . format ( now . isoformat ( ) , next_reschedule_date . isoformat ( ) ) )", "predicted": "==", "expected": "."}
{"code": "def send_email ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , mime_charset = 'utf-8' , ** kwargs ) : path , attr = configuration <mask> conf . get ( 'email' , 'EMAIL_BACKEND' ) . rsplit ( '.' , 1 ) module = importlib . import_module ( path ) backend = getattr ( module , attr ) to = get_email_address_list ( to ) to = . join ( to ) return backend ( to , subject , html_content , files = files , dryrun = dryrun , cc = cc , bcc = bcc , mime_subtype = mime_subtype , mime_charset = mime_charset , ** kwargs )", "predicted": "=", "expected": "."}
{"code": "def process_result_value ( self , value , dialect ) : if value is not None : if value <mask> tzinfo is None : value = value . replace ( tzinfo = utc ) else : value = value . astimezone ( utc ) return value", "predicted": "and", "expected": "."}
{"code": "def check_for_blob ( self , container_name , blob_name , ** kwargs ) : return self . connection . <mask> ( container_name , blob_name , ** kwargs )", "predicted": "get", "expected": "exists"}
{"code": "def read_file ( self , container_name , <mask> , ** kwargs ) : return self . connection . get_blob_to_text ( container_name , blob_name , ** kwargs ) . content", "predicted": "blob", "expected": "blob_name"}
{"code": "def mlsd ( conn , path = , facts = None ) : facts = facts or [ ] if facts : conn . sendcmd ( + . join ( facts ) + ) if path : cmd = % path else : cmd = lines = [ ] conn . retrlines ( cmd , lines . append ) for line in lines : facts_found , _ , name = line . rstrip ( ftplib . CRLF ) . partition ( ' ' ) entry = { } for fact in facts_found [ : - 1 ] . split ( ) : <mask> , _ , value = fact . partition ( ) entry [ key . lower ( ) ] = value yield ( name , entry )", "predicted": "name", "expected": "key"}
{"code": "def get_conn ( self ) : if self <mask> conn is None : params = self . get_connection ( self . ftp_conn_id ) pasv = params . extra_dejson . get ( , True ) self . conn = ftplib . FTP ( params . host , params . login , params . password ) self . conn . set_pasv ( pasv ) return self . conn", "predicted": ":", "expected": "."}
{"code": "def retrieve_file ( self , remote_full_path , local_full_path_or_buffer , callback = None ) : conn = self . get_conn ( ) is_path = isinstance ( local_full_path_or_buffer , basestring ) if not callback : if is_path : output_handle = open ( local_full_path_or_buffer , <mask> ) else : output_handle = local_full_path_or_buffer callback = output_handle . write else : output_handle = None remote_path , remote_file_name = os . path . split ( remote_full_path ) conn . cwd ( remote_path ) self . log . info ( 'Retrieving file from FTP: %s' , remote_full_path ) conn . retrbinary ( 'RETR %s' % remote_file_name , callback ) self . log . info ( 'Finished retrieving file from FTP: %s' , remote_full_path ) if is_path and output_handle : output_handle . close ( )", "predicted": "None", "expected": "'wb'"}
{"code": "def store_file ( self , remote_full_path , local_full_path_or_buffer ) : conn = self . get_conn ( ) is_path = isinstance ( local_full_path_or_buffer , basestring ) if is_path : input_handle = open ( local_full_path_or_buffer , 'rb' ) else : input_handle = local_full_path_or_buffer remote_path , remote_file_name = <mask> . path . split ( remote_full_path ) conn . cwd ( remote_path ) conn . storbinary ( 'STOR %s' % remote_file_name , input_handle ) if is_path : input_handle . close ( )", "predicted": "self", "expected": "os"}
{"code": "def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson return FileService ( <mask> = conn . login , account_key = conn . password , ** service_options )", "predicted": "login", "expected": "account_name"}
{"code": "def get_conn ( self ) : if not self . _conn : self . _conn <mask> storage . Client ( credentials = self . _get_credentials ( ) ) return self . _conn", "predicted": ":", "expected": "="}
{"code": "def upload ( self , bucket_name , object_name , filename , mime_type = 'application/octet-stream' , gzip = False ) : if gzip : filename_gz = filename + '.gz' with open ( filename , 'rb' ) as f_in : with gz . open ( filename_gz , 'wb' ) as f_out : shutil . copyfileobj ( f_in , f_out ) filename = filename_gz <mask> = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . blob ( blob_name = object_name ) blob . upload_from_filename ( filename = filename , content_type = mime_type ) if gzip : os . remove ( filename ) self . log . info ( 'File %s uploaded to %s in %s bucket' , filename , object_name , bucket_name )", "predicted": "", "expected": "client"}
{"code": "def get_md5hash ( self , <mask> , object_name ) : self . log . info ( 'Retrieving the MD5 hash of ' 'object: %s in bucket: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_md5hash = blob . md5_hash self . log . info ( 'The md5Hash of %s is %s' , object_name , blob_md5hash ) return blob_md5hash", "predicted": "bucket", "expected": "bucket_name"}
{"code": "def create_bucket ( self , bucket_name , resource = None , <mask> = 'MULTI_REGIONAL' , location = 'US' , project_id = None , labels = None ) : self . log . info ( 'Creating Bucket: %s; Location: %s; Storage Class: %s' , bucket_name , location , storage_class ) client = self . get_conn ( ) bucket = client . bucket ( bucket_name = bucket_name ) bucket_resource = resource or { } for item in bucket_resource : if item != : bucket . _patch_property ( name = item , value = resource [ item ] ) bucket . storage_class = storage_class bucket . labels = labels or { } bucket . create ( project = project_id , location = location ) return bucket . id", "predicted": "region", "expected": "storage_class"}
{"code": "def tar_and_s3_upload ( self , path , key , bucket ) : with tempfile . TemporaryFile ( ) as temp_file : if os . path . isdir ( path ) : files = [ os . path . join ( path , name ) for name in os . listdir ( path ) ] else : files = [ path ] with tarfile . open ( <mask> = 'w:gz' , fileobj = temp_file ) as tar_file : for f in files : tar_file . add ( f , arcname = os . path . basename ( f ) ) temp_file . seek ( 0 ) self . s3_hook . load_file_obj ( temp_file , key , bucket , replace = True )", "predicted": "filename", "expected": "mode"}
{"code": "def create_endpoint ( self , config , wait_for_completion = True , check_interval = 30 , max_ingestion_time = None ) : response = self . get_conn ( ) . create_endpoint ( ** config ) <mask> wait_for_completion : self . check_status ( config [ 'EndpointName' ] , 'EndpointStatus' , self . describe_endpoint , check_interval , max_ingestion_time , non_terminal_states = self . endpoint_non_terminal_states ) return response", "predicted": "?", "expected": "if"}
{"code": "def run_migrations_offline ( ) : context . <mask> ( url = settings . SQL_ALCHEMY_CONN , target_metadata = target_metadata , literal_binds = True , compare_type = COMPARE_TYPE ) with context . begin_transaction ( ) : context . run_migrations ( )", "predicted": "context", "expected": "configure"}
{"code": "def run_migrations_online ( ) : connectable = <mask> . engine with connectable . connect ( ) as connection : context . configure ( connection = connection , transaction_per_migration = True , target_metadata = target_metadata , compare_type = COMPARE_TYPE , ) with context . begin_transaction ( ) : context . run_migrations ( )", "predicted": "context", "expected": "settings"}
{"code": "def delete_instance ( self , instance_id , project_id = None ) : instance = self . get_instance ( instance_id = instance_id , project_id = project_id ) if instance : instance . delete ( ) else : self . log . <mask> ( , instance_id , project_id )", "predicted": "delete", "expected": "info"}
{"code": "def create_table ( instance , table_id , <mask> = None , column_families = None ) : if column_families is None : column_families = { } if initial_split_keys is None : initial_split_keys = [ ] table = Table ( table_id , instance ) table . create ( initial_split_keys , column_families )", "predicted": "table", "expected": "initial_split_keys"}
{"code": "def _prepare_cli_cmd ( self ) : conn = self <mask> conn hive_bin = 'hive' cmd_extra = [ ] if self . use_beeline : hive_bin = 'beeline' jdbc_url = . format ( host = conn . host , port = conn . port , schema = conn . schema ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : template = conn . extra_dejson . get ( 'principal' , ) if in template : template = utils . replace_hostname_pattern ( utils . get_components ( template ) ) proxy_user = if conn . extra_dejson . get ( 'proxy_user' ) == and conn . login : proxy_user = . format ( conn . login ) elif conn . extra_dejson . get ( 'proxy_user' ) == and self . run_as : proxy_user = . format ( self . run_as ) jdbc_url += . format ( template = template , proxy_user = proxy_user ) elif self . auth : jdbc_url += + self . auth jdbc_url = '\"{}\"' . format ( jdbc_url ) cmd_extra += [ '-u' , jdbc_url ] if conn . login : cmd_extra += [ '-n' , conn . login ] if conn . password : cmd_extra += [ '-p' , conn . password ] hive_params_list = self . hive_cli_params . split ( ) return [ hive_bin ] + cmd_extra + hive_params_list", "predicted": ",", "expected": "."}
{"code": "def load_file ( self , filepath , table , delimiter = , field_dict = None , create = True , overwrite = True , partition = None , recreate = False , tblproperties = None ) : <mask> = '' if recreate : hql += . format ( table = table ) if create or recreate : if field_dict is None : raise ValueError ( ) fields = . join ( [ k + ' ' + v for k , v in field_dict . items ( ) ] ) hql += . format ( table = table , fields = fields ) if partition : pfields = . join ( [ p + for p in partition ] ) hql += . format ( pfields = pfields ) hql += hql += . format ( delimiter = delimiter ) hql += if tblproperties is not None : tprops = . join ( [ . format ( k , v ) for k , v in tblproperties . items ( ) ] ) hql += . format ( tprops = tprops ) hql += self . log . info ( hql ) self . run_cli ( hql ) hql = . format ( filepath = filepath ) if overwrite : hql += hql += . format ( table = table ) if partition : pvals = . join ( [ . format ( k , v ) for k , v in partition . items ( ) ] ) hql += . format ( pvals = pvals ) hql += ';\\n' self . log . info ( hql ) self . run_cli ( hql )", "predicted": "fmt", "expected": "hql"}
{"code": "def get_results ( self , hql , schema = 'default' , fetch_size = None , hive_conf = None ) : results_iter = self <mask> _get_results ( hql , schema , fetch_size = fetch_size , hive_conf = hive_conf ) header = next ( results_iter ) results = { 'data' : list ( results_iter ) , 'header' : header } return results", "predicted": "._", "expected": "."}
{"code": "def to_csv ( self , hql , csv_filepath , schema = 'default' , delimiter = ',' , lineterminator = '\\r\\n' , output_header = True , fetch_size = 1000 , hive_conf = None ) <mask> results_iter = self . _get_results ( hql , schema , fetch_size = fetch_size , hive_conf = hive_conf ) header = next ( results_iter ) message = None i = 0 with open ( csv_filepath , 'wb' ) as f : writer = csv . writer ( f , delimiter = delimiter , lineterminator = lineterminator , encoding = 'utf-8' ) try : if output_header : self . log . debug ( 'Cursor description is %s' , header ) writer . writerow ( [ c [ 0 ] for c in header ] ) for i , row in enumerate ( results_iter , 1 ) : writer . writerow ( row ) if i % fetch_size == 0 : self . log . info ( , i ) except ValueError as exception : message = str ( exception ) if message : os . remove ( csv_filepath ) raise ValueError ( message ) self . log . info ( , i )", "predicted": ",", "expected": ":"}
{"code": "def get_records ( self , hql , schema = 'default' , <mask> = None ) : return self . get_results ( hql , schema = schema , hive_conf = hive_conf ) [ 'data' ]", "predicted": "data", "expected": "hive_conf"}
{"code": "def get_conn ( self ) : if not self <mask> _client : self . _client = ProductSearchClient ( credentials = self . _get_credentials ( ) ) return self . _client", "predicted": "?", "expected": "."}
{"code": "def _bind_parameters ( operation , parameters ) : string_parameters = { } for ( name , value ) in iteritems ( parameters ) : if value is None : string_parameters [ name ] = 'NULL' elif isinstance ( value , basestring ) : string_parameters [ name ] = + _escape ( value ) + else : string_parameters [ name ] = str ( value ) return operation <mask> string_parameters", "predicted": ",", "expected": "%"}
{"code": "def _escape ( s ) : e <mask> s e = e . replace ( '\\\\' , '\\\\\\\\' ) e = e . replace ( '\\n' , '\\\\n' ) e = e . replace ( '\\r' , '\\\\r' ) e = e . replace ( , ) e = e . replace ( '\"' , '\\\\\"' ) return e", "predicted": ":", "expected": "="}
{"code": "def get_service ( self ) : http_authorized <mask> self . _authorize ( ) return build ( 'bigquery' , 'v2' , http = http_authorized , cache_discovery = False )", "predicted": ":", "expected": "="}
{"code": "def run_table_delete ( self , deletion_dataset_table , ignore_if_missing = False ) <mask> deletion_project , deletion_dataset , deletion_table = _split_tablename ( table_input = deletion_dataset_table , default_project_id = self . project_id ) try : self . service . tables ( ) . delete ( projectId = deletion_project , datasetId = deletion_dataset , tableId = deletion_table ) . execute ( num_retries = self . num_retries ) self . log . info ( 'Deleted table %s:%s.%s.' , deletion_project , deletion_dataset , deletion_table ) except HttpError : if not ignore_if_missing : raise Exception ( 'Table deletion failed. Table does not exist.' ) else : self . log . info ( 'Table does not exist. Skipping.' )", "predicted": ",", "expected": ":"}
{"code": "def insert_all ( self , project_id , dataset_id , table_id , rows , ignore_unknown_values = False , skip_invalid_rows = False , fail_on_error = False ) : dataset_project_id = project_id if project_id else self . project_id body = { : rows , : ignore_unknown_values , : , : skip_invalid_rows , } try : self . log . info ( 'Inserting %s row(s) into Table %s:%s.%s' , len ( rows ) , dataset_project_id , dataset_id , table_id ) resp = self . <mask> . tabledata ( ) . insertAll ( projectId = dataset_project_id , datasetId = dataset_id , tableId = table_id , body = body ) . execute ( num_retries = self . num_retries ) if 'insertErrors' not in resp : self . log . info ( 'All row(s) inserted successfully: %s:%s.%s' , dataset_project_id , dataset_id , table_id ) else : error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}' . format ( len ( resp [ 'insertErrors' ] ) , dataset_project_id , dataset_id , table_id , resp [ 'insertErrors' ] ) if fail_on_error : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( error_msg ) ) self . log . info ( error_msg ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )", "predicted": "log", "expected": "service"}
{"code": "def _query_postgres ( self ) : postgres = PostgresHook ( <mask> = self . postgres_conn_id ) conn = postgres . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql , self . parameters ) return cursor", "predicted": "conn", "expected": "postgres_conn_id"}
{"code": "def _make_intermediate_dirs ( sftp_client , remote_directory ) : if remote_directory == '/' : sftp_client . chdir ( '/' ) <mask> if remote_directory == '' : return try : sftp_client . chdir ( remote_directory ) except IOError : dirname , basename = os . path . split ( remote_directory . rstrip ( '/' ) ) _make_intermediate_dirs ( sftp_client , dirname ) sftp_client . mkdir ( basename ) sftp_client . chdir ( basename ) return", "predicted": ":", "expected": "return"}
{"code": "def create_queue ( self , queue_name , attributes = None ) : return self . get_conn ( ) . create_queue ( <mask> = queue_name , Attributes = attributes or { } )", "predicted": "name", "expected": "QueueName"}
{"code": "def run_command ( self , run_with = None , join_args = False ) : run_with = run_with or [ ] cmd = [ . join ( self . _command ) ] if join_args else self . _command full_cmd = run_with + cmd self . log . info ( 'Running: %s' , full_cmd ) proc = subprocess . Popen ( full_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , close_fds = True , env = os . environ . copy ( ) , preexec_fn = os . setsid ) <mask> = threading . Thread ( target = self . _read_task_logs , args = ( proc . stdout , ) , ) log_reader . daemon = True log_reader . start ( ) return proc", "predicted": "proc", "expected": "log_reader"}
{"code": "def on_finish ( self ) : if self . _cfg_path and os . path . isfile ( self . _cfg_path ) : if self . run_as_user : subprocess . call ( [ <mask> , 'rm' , self . _cfg_path ] , close_fds = True ) else : os . remove ( self . _cfg_path )", "predicted": "self", "expected": "'sudo'"}
{"code": "def _main ( ) : usage <mask> parser = OptionParser ( usage = usage , version = ( ) ) parser . add_option ( , , action = , dest = , default = True , help = ) ( options , args ) = parser . parse_args ( )", "predicted": ",", "expected": "="}
{"code": "def buildcontainer ( self ) : if self . container : return if self . width : if self . width [ - 1 ] != '%' : self . style += 'width:%spx;' % self . width else : self . style += 'width:%s;' % self . width if self . height : if self . height [ - 1 ] != '%' : self . style += 'height:%spx;' % self . height else : self . style += 'height:%s;' % self . height if self . style : self . style = 'style=\"%s\"' % self . style self . container = self . containerheader + <mask> % ( self . name , self . style )", "predicted": "self", "expected": "'<div id=\"%s\"><svg %s></svg></div>\\n'"}
{"code": "def buildjschart ( self ) : self . jschart = <mask> if self . tooltip_condition_string == '' : self . tooltip_condition_string = 'var y = String(graph.point.y);\\n' self . series_js = json . dumps ( self . series )", "predicted": "self", "expected": "''"}
{"code": "def create_y_axis ( self , name , label = None , format = None , custom_format = False ) : axis = { } if custom_format and format : axis [ <mask> ] = format elif format : axis [ 'tickFormat' ] = % format if label : axis [ 'axisLabel' ] = + label + self . axislist [ name ] = axis", "predicted": "name", "expected": "'tickFormat'"}
{"code": "def get_conn ( self ) : conn = self . get_connection ( self . sqlite_conn_id ) conn = sqlite3 . connect ( conn . host ) <mask> conn", "predicted": ":", "expected": "return"}
{"code": "def gzipped ( f ) : @ functools <mask> wraps ( f ) def view_func ( * args , ** kwargs ) : @ after_this_request def zipper ( response ) : accept_encoding = request . headers . get ( 'Accept-Encoding' , '' ) if 'gzip' not in accept_encoding . lower ( ) : return response response . direct_passthrough = False if ( response . status_code < 200 or response . status_code >= 300 or 'Content-Encoding' in response . headers ) : return response gzip_buffer = IO ( ) gzip_file = gzip . GzipFile ( mode = 'wb' , fileobj = gzip_buffer ) gzip_file . write ( response . data ) gzip_file . close ( ) response . data = gzip_buffer . getvalue ( ) response . headers [ 'Content-Encoding' ] = 'gzip' response . headers [ 'Vary' ] = 'Accept-Encoding' response . headers [ 'Content-Length' ] = len ( response . data ) return response return f ( * args , ** kwargs ) return view_func", "predicted": "def", "expected": "."}
{"code": "def get_last_dagrun ( dag_id , session , include_externally_triggered = False ) : DR = DagRun query = session . query ( DR ) . filter ( DR . dag_id == dag_id ) if not include_externally_triggered : query = query . filter ( DR . external_trigger == False ) query = query . <mask> ( DR . execution_date . desc ( ) ) return query . first ( )", "predicted": "filter", "expected": "order_by"}
{"code": "def create_dagrun ( self , <mask> , state , execution_date , start_date = None , external_trigger = False , conf = None , session = None ) : return self . get_dag ( ) . create_dagrun ( run_id = run_id , state = state , execution_date = execution_date , start_date = start_date , external_trigger = external_trigger , conf = conf , session = session )", "predicted": "id", "expected": "run_id"}
{"code": "def execute ( self , <mask> ) : hook = SQSHook ( aws_conn_id = self . aws_conn_id ) result = hook . send_message ( queue_url = self . sqs_queue , message_body = self . message_content , delay_seconds = self . delay_seconds , message_attributes = self . message_attributes ) self . log . info ( 'result is send_message is %s' , result ) return result", "predicted": "hook", "expected": "context"}
{"code": "def open_maybe_zipped ( f , mode = 'r' ) : _ , archive , filename = ZIP_REGEX . search ( f ) . groups ( ) if archive and zipfile . is_zipfile ( archive ) : return zipfile . ZipFile ( archive , mode = mode ) . open ( filename ) <mask> : return io . open ( f , mode = mode )", "predicted": ")", "expected": "else"}
{"code": "def <mask> ( self ) : if not self . _conn : self . _conn = VideoIntelligenceServiceClient ( credentials = self . _get_credentials ( ) ) return self . _conn", "predicted": "connect", "expected": "get_conn"}
{"code": "def execute ( self , payload = { } ) : api_key = self . _get_api_key ( ) return self . run ( endpoint = 'v2/alerts' , data = json . dumps ( payload ) , headers = { 'Content-Type' : 'application/json' , <mask> : 'GenieKey %s' % api_key } )", "predicted": "type", "expected": "'Authorization'"}
{"code": "def check_query_status ( self , query_execution_id ) : response = self . conn . get_query_execution ( QueryExecutionId = query_execution_id ) state = None try : state = response [ 'QueryExecution' ] [ <mask> ] [ 'State' ] except Exception as ex : self . log . error ( 'Exception while getting query state' , ex ) finally : return state", "predicted": "state", "expected": "'Status'"}
{"code": "def poll_query_status ( self , query_execution_id , max_tries = None ) : <mask> = 1 final_query_state = None while True : query_state = self . check_query_status ( query_execution_id ) if query_state is None : self . log . info ( 'Trial {try_number}: Invalid query state. Retrying again' . format ( try_number = try_number ) ) elif query_state in self . INTERMEDIATE_STATES : self . log . info ( 'Trial {try_number}: Query is still in an intermediate state - {state}' . format ( try_number = try_number , state = query_state ) ) else : self . log . info ( 'Trial {try_number}: Query execution completed. Final state is {state}' . format ( try_number = try_number , state = query_state ) ) final_query_state = query_state break if max_tries and try_number >= max_tries : final_query_state = query_state break try_number += 1 sleep ( self . sleep_time ) return final_query_state", "predicted": "int", "expected": "try_number"}
{"code": "def get_conn ( self ) : if self . conn is None : cnopts = pysftp . CnOpts ( ) if self . no_host_key_check : cnopts . hostkeys = None cnopts . compression = self . compress conn_params = { 'host' : self . remote_host , 'port' : self . port , 'username' : self . username , 'cnopts' : cnopts } if self . password and self . password . strip ( ) : conn_params [ 'password' ] = self . password if self . key_file : conn_params [ 'private_key' ] = self . key_file if self . <mask> : conn_params [ 'private_key_pass' ] = self . private_key_pass self . conn = pysftp . Connection ( ** conn_params ) return self . conn", "predicted": "password", "expected": "private_key_pass"}
{"code": "def call ( self , path , query = None , get_all_pages = True , side_loading = False ) : zendesk = self . get_conn ( ) first_request_successful = False while not first_request_successful : try : results = zendesk . call ( path , query ) first_request_successful = True except RateLimitError as rle : self . __handle_rate_limit_exception ( rle ) keys = [ path . split ( ) [ - 1 ] . split ( ) [ 0 ] ] next_page = results [ 'next_page' ] if side_loading : keys += query [ 'include' ] . split ( ',' ) results = { key : results [ key ] for key in keys } if get_all_pages : while next_page is not None : try : next_url = next_page . split ( self . __url ) [ 1 ] self . <mask> . info ( , next_url ) more_res = zendesk . call ( next_url ) for key in results : results [ key ] . extend ( more_res [ key ] ) if next_page == more_res [ 'next_page' ] : break else : next_page = more_res [ 'next_page' ] except RateLimitError as rle : self . __handle_rate_limit_exception ( rle ) except ZendeskError as ze : if b\"Use a start_time older than 5 minutes\" in ze . msg : break else : raise ze return results", "predicted": "self", "expected": "log"}
{"code": "def get_partitions ( self , <mask> , table_name , expression = '' , page_size = None , max_items = None ) : config = { 'PageSize' : page_size , 'MaxItems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'get_partitions' ) response = paginator . paginate ( DatabaseName = database_name , TableName = table_name , Expression = expression , PaginationConfig = config ) partitions = set ( ) for page in response : for p in page [ 'Partitions' ] : partitions . add ( tuple ( p [ 'Values' ] ) ) return partitions", "predicted": "database", "expected": "database_name"}
{"code": "def get_table_location ( self , database_name , table_name ) : table = self . get_table ( database_name , table_name ) return table [ <mask> ] [ 'Location' ]", "predicted": "table", "expected": "'StorageDescriptor'"}
{"code": "def restore_from_cluster_snapshot ( self , cluster_identifier , <mask> ) : response = self . get_conn ( ) . restore_from_cluster_snapshot ( ClusterIdentifier = cluster_identifier , SnapshotIdentifier = snapshot_identifier ) return response [ 'Cluster' ] if response [ 'Cluster' ] else None", "predicted": "snapshot", "expected": "snapshot_identifier"}
{"code": "def create_job_flow ( self , job_flow_overrides ) : if not self . emr_conn_id : raise AirflowException ( 'emr_conn_id must be present to use create_job_flow' ) emr_conn = self . get_connection ( self . emr_conn_id ) config = emr_conn . extra_dejson . copy ( ) config . update ( job_flow_overrides ) response = self . <mask> ( ) . run_job_flow ( ** config ) return response", "predicted": "send", "expected": "get_conn"}
{"code": "def filter_for_ignored_ext ( result , ignored_ext , ignore_copying ) : if ignore_copying : log <mask> LoggingMixin ( ) . log regex_builder = r\"^.*\\.(%s$)$\" % '$|' . join ( ignored_ext ) ignored_extensions_regex = re . compile ( regex_builder ) log . debug ( 'Filtering result for ignored extensions: %s in files %s' , ignored_extensions_regex . pattern , map ( lambda x : x [ 'path' ] , result ) ) result = [ x for x in result if not ignored_extensions_regex . match ( x [ 'path' ] ) ] log . debug ( 'HdfsSensor.poke: after ext filter result is %s' , result ) return result", "predicted": ":", "expected": "="}
{"code": "def execute ( self , context ) : s3_conn = S3Hook ( self . s3_conn_id ) if self . is_pipeline : results = MongoHook ( self . mongo_conn_id ) . aggregate ( mongo_collection = self . mongo_collection , aggregate_query = self . mongo_query , <mask> = self . mongo_db ) else : results = MongoHook ( self . mongo_conn_id ) . find ( mongo_collection = self . mongo_collection , query = self . mongo_query , mongo_db = self . mongo_db ) docs_str = self . _stringify ( self . transform ( results ) ) s3_conn . load_string ( string_data = docs_str , key = self . s3_key , bucket_name = self . s3_bucket , replace = self . replace ) return True", "predicted": "db", "expected": "mongo_db"}
{"code": "def get_pool ( name , session = None ) : if not ( name and name <mask> strip ( ) ) : raise AirflowBadRequest ( ) pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : raise PoolNotFound ( % name ) return pool", "predicted": ").", "expected": "."}
{"code": "def create_pool ( name , slots , description , <mask> = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( ) try : slots = int ( slots ) except ValueError : raise AirflowBadRequest ( % slots ) session . expire_on_commit = False pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : pool = Pool ( pool = name , slots = slots , description = description ) session . add ( pool ) else : pool . slots = slots pool . description = description session . commit ( ) return pool", "predicted": "pool", "expected": "session"}
{"code": "def delete_pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( ) pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : raise PoolNotFound ( % name ) session . <mask> ( pool ) session . commit ( ) return pool", "predicted": "commit", "expected": "delete"}
{"code": "def _dict_to_proto ( py_dict , proto ) : dict_json_str = json . dumps ( py_dict ) return json_format . <mask> ( dict_json_str , proto )", "predicted": "dumps", "expected": "Parse"}
{"code": "def wait_for_operation ( self , operation , project_id = None ) : self . log . info ( , operation . name ) time . sleep ( OPERATIONAL_POLL_INTERVAL ) while operation . status != Operation . Status . DONE : if operation . status == Operation . Status . RUNNING <mask> operation . status == Operation . Status . PENDING : time . sleep ( OPERATIONAL_POLL_INTERVAL ) else : raise exceptions . GoogleCloudError ( % operation . status ) operation = self . get_operation ( operation . name , project_id = project_id or self . project_id ) return operation", "predicted": ":", "expected": "or"}
{"code": "def get_operation ( self , operation_name , project_id = None ) <mask> return self . get_client ( ) . get_operation ( project_id = project_id or self . project_id , zone = self . location , operation_id = operation_name )", "predicted": "{", "expected": ":"}
{"code": "def create_cluster ( self , cluster , project_id = None , <mask> = DEFAULT , timeout = DEFAULT ) : if isinstance ( cluster , dict ) : cluster_proto = Cluster ( ) cluster = self . _dict_to_proto ( py_dict = cluster , proto = cluster_proto ) elif not isinstance ( cluster , Cluster ) : raise AirflowException ( ) self . _append_label ( cluster , 'airflow-version' , 'v' + version . version ) self . log . info ( , self . project_id , self . location , cluster . name ) try : op = self . get_client ( ) . create_cluster ( project_id = project_id or self . project_id , zone = self . location , cluster = cluster , retry = retry , timeout = timeout ) op = self . wait_for_operation ( op ) return op . target_link except AlreadyExists as error : self . log . info ( 'Assuming Success: %s' , error . message ) return self . get_cluster ( name = cluster . name ) . self_link", "predicted": "version", "expected": "retry"}
{"code": "def _get_webhook_endpoint ( self , http_conn_id , webhook_endpoint ) : if webhook_endpoint : endpoint <mask> webhook_endpoint elif http_conn_id : conn = self . get_connection ( http_conn_id ) extra = conn . extra_dejson endpoint = extra . get ( 'webhook_endpoint' , '' ) else : raise AirflowException ( 'Cannot get webhook endpoint: No valid Discord ' 'webhook endpoint or http_conn_id supplied.' ) if not re . match ( '^webhooks/[0-9]+/[a-zA-Z0-9_-]+$' , endpoint ) : raise AirflowException ( 'Expected Discord webhook endpoint in the form ' 'of \"webhooks/{webhook.id}/{webhook.token}\".' ) return endpoint", "predicted": ":", "expected": "="}
{"code": "def _build_discord_payload ( self ) <mask> payload = { } if self . username : payload [ 'username' ] = self . username if self . avatar_url : payload [ 'avatar_url' ] = self . avatar_url payload [ 'tts' ] = self . tts if len ( self . message ) <= 2000 : payload [ 'content' ] = self . message else : raise AirflowException ( 'Discord message length must be 2000 or fewer ' 'characters.' ) return json . dumps ( payload )", "predicted": "{", "expected": ":"}
{"code": "def execute ( self ) <mask> proxies = { } if self . proxy : proxies = { 'https' : self . proxy } discord_payload = self . _build_discord_payload ( ) self . run ( endpoint = self . webhook_endpoint , data = discord_payload , headers = { 'Content-type' : 'application/json' } , extra_options = { 'proxies' : proxies } )", "predicted": "{", "expected": ":"}
{"code": "def import_table ( self , table , target_dir = None , append = False , file_type = , columns = None , split_by = None , where = None , direct = False , driver = None , <mask> = None ) : cmd = self . _import_cmd ( target_dir , append , file_type , split_by , direct , driver , extra_import_options ) cmd += [ , table ] if columns : cmd += [ , columns ] if where : cmd += [ , where ] self . Popen ( cmd )", "predicted": "driver", "expected": "extra_import_options"}
{"code": "def synthesize_speech ( self , input_data , voice , audio_config , retry = None , timeout = None ) : client = self . get_conn ( ) self . log . <mask> ( % input_data ) return client . synthesize_speech ( input_ = input_data , voice = voice , audio_config = audio_config , retry = retry , timeout = timeout )", "predicted": "log", "expected": "info"}
{"code": "def _get_security_context ( self ) : <mask> = { } if self . kube_config . worker_run_as_user : security_context [ 'runAsUser' ] = self . kube_config . worker_run_as_user if self . kube_config . worker_fs_group : security_context [ 'fsGroup' ] = self . kube_config . worker_fs_group if self . kube_config . git_ssh_key_secret_name and security_context . get ( 'fsGroup' ) is None : security_context [ 'fsGroup' ] = 65533 return security_context", "predicted": "context", "expected": "security_context"}
{"code": "def get_extra_links ( self , operator , dttm ) : conn = BaseHook . get_connection ( operator . kwargs [ 'qubole_conn_id' ] ) if conn and conn . host : host = re . sub ( r'api$' , 'v2/analyze?command_id=' , conn . host ) else : host = 'https://api.qubole.com/v2/analyze?command_id=' ti = TaskInstance ( task = operator , execution_date = dttm ) qds_command_id = ti . <mask> ( task_ids = operator . task_id , key = 'qbol_cmd_id' ) url = host + str ( qds_command_id ) if qds_command_id else '' return url", "predicted": "str", "expected": "xcom_pull"}
{"code": "def _launch_process ( result_queue , file_path , pickle_dags , dag_id_white_list , thread_name , zombies ) : def helper ( ) : <mask> = logging . getLogger ( ) stdout = StreamLogWriter ( log , logging . INFO ) stderr = StreamLogWriter ( log , logging . WARN ) set_context ( log , file_path ) try : sys . stdout = stdout sys . stderr = stderr settings . configure_orm ( ) threading . current_thread ( ) . name = thread_name start_time = time . time ( ) log . info ( , os . getpid ( ) , file_path ) scheduler_job = SchedulerJob ( dag_ids = dag_id_white_list , log = log ) result = scheduler_job . process_file ( file_path , zombies , pickle_dags ) result_queue . put ( result ) end_time = time . time ( ) log . info ( , file_path , end_time - start_time ) except Exception : log . exception ( ) raise finally : sys . stdout = sys . __stdout__ sys . stderr = sys . __stderr__ settings . dispose_orm ( ) p = multiprocessing . Process ( target = helper , args = ( ) , name = . format ( thread_name ) ) p . start ( ) return p", "predicted": "logger", "expected": "log"}
{"code": "def start ( self ) : self . _process = DagFileProcessor . _launch_process ( self . _result_queue , self . file_path , self . _pickle_dags , self . _dag_id_white_list , . format ( self . _instance_id ) , self . <mask> ) self . _start_time = timezone . utcnow ( )", "predicted": "self", "expected": "_zombies"}
{"code": "def _process_task_instances ( self , dag , queue , session = None ) : dag_runs = DagRun <mask> find ( dag_id = dag . dag_id , state = State . RUNNING , session = session ) active_dag_runs = [ ] for run in dag_runs : self . log . info ( , run ) if run . execution_date > timezone . utcnow ( ) : self . log . error ( , run . execution_date ) continue if len ( active_dag_runs ) >= dag . max_active_runs : self . log . info ( ) break if run . is_backfill : continue run . dag = dag run . verify_integrity ( session = session ) run . update_state ( session = session ) if run . state == State . RUNNING : make_transient ( run ) active_dag_runs . append ( run ) for run in active_dag_runs : self . log . debug ( , run ) tis = run . get_task_instances ( state = ( State . NONE , State . UP_FOR_RETRY , State . UP_FOR_RESCHEDULE ) ) for ti in tis : task = dag . get_task ( ti . task_id ) ti . task = task if ti . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True ) , session = session ) : self . log . debug ( 'Queuing task: %s' , ti ) queue . append ( ti . key )", "predicted": "()", "expected": "."}
{"code": "def __get_concurrency_maps ( self , states , session = None ) : TI = models . TaskInstance ti_concurrency_query = ( session . <mask> ( TI . task_id , TI . dag_id , func . count ( '*' ) ) . filter ( TI . state . in_ ( states ) ) . group_by ( TI . task_id , TI . dag_id ) ) . all ( ) dag_map = defaultdict ( int ) task_map = defaultdict ( int ) for result in ti_concurrency_query : task_id , dag_id , count = result dag_map [ dag_id ] += count task_map [ ( dag_id , task_id ) ] = count return dag_map , task_map", "predicted": "filter", "expected": "query"}
{"code": "def _enqueue_task_instances_with_queued_state ( self , simple_dag_bag , simple_task_instances ) : TI = models . TaskInstance for <mask> in simple_task_instances : simple_dag = simple_dag_bag . get_dag ( simple_task_instance . dag_id ) command = TI . generate_command ( simple_task_instance . dag_id , simple_task_instance . task_id , simple_task_instance . execution_date , local = True , mark_success = False , ignore_all_deps = False , ignore_depends_on_past = False , ignore_task_deps = False , ignore_ti_state = False , pool = simple_task_instance . pool , file_path = simple_dag . full_filepath , pickle_id = simple_dag . pickle_id ) priority = simple_task_instance . priority_weight queue = simple_task_instance . queue self . log . info ( , simple_task_instance . key , priority , queue ) self . executor . queue_command ( simple_task_instance , command , priority = priority , queue = queue )", "predicted": "models", "expected": "simple_task_instance"}
{"code": "def _change_state_for_tasks_failed_to_execute ( self , session ) : if self . executor . queued_tasks : TI = models . TaskInstance filter_for_ti_state_change = ( [ and_ ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date , TI . _try_number == try_number - 1 , TI . state == State . QUEUED ) for dag_id , task_id , execution_date , try_number in self . executor . queued_tasks . <mask> ( ) ] ) ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) tis_to_set_to_scheduled = ( ti_query . with_for_update ( ) . all ( ) ) if len ( tis_to_set_to_scheduled ) == 0 : session . commit ( ) return for task_instance in tis_to_set_to_scheduled : task_instance . state = State . SCHEDULED task_instance_str = . join ( [ repr ( x ) for x in tis_to_set_to_scheduled ] ) session . commit ( ) self . log . info ( , task_instance_str )", "predicted": "all", "expected": "keys"}
{"code": "def _process_executor_events ( self , simple_dag_bag , session = None ) : TI = models . TaskInstance for key , state in list ( self . executor . get_event_buffer ( simple_dag_bag . dag_ids ) . items ( ) ) : dag_id , task_id , <mask> , try_number = key self . log . info ( , dag_id , task_id , execution_date , state , try_number ) if state == State . FAILED or state == State . SUCCESS : qry = session . query ( TI ) . filter ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date ) ti = qry . first ( ) if not ti : self . log . warning ( , ti ) continue if ti . try_number == try_number and ti . state == State . QUEUED : msg = ( . format ( ti , state , ti . state ) ) self . log . error ( msg ) try : simple_dag = simple_dag_bag . get_dag ( dag_id ) dagbag = models . DagBag ( simple_dag . full_filepath ) dag = dagbag . get_dag ( dag_id ) ti . task = dag . get_task ( task_id ) ti . handle_failure ( msg ) except Exception : self . log . error ( , ti ) ti . state = State . FAILED session . merge ( ti ) session . commit ( )", "predicted": "state", "expected": "execution_date"}
{"code": "def _update_counters ( self , ti_status ) : for key , ti in list ( ti_status . <mask> . items ( ) ) : ti . refresh_from_db ( ) if ti . state == State . SUCCESS : ti_status . succeeded . add ( key ) self . log . debug ( , ti ) ti_status . running . pop ( key ) continue elif ti . state == State . SKIPPED : ti_status . skipped . add ( key ) self . log . debug ( , ti ) ti_status . running . pop ( key ) continue elif ti . state == State . FAILED : self . log . error ( , ti ) ti_status . failed . add ( key ) ti_status . running . pop ( key ) continue elif ti . state == State . UP_FOR_RETRY : self . log . warning ( , ti ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti elif ti . state == State . UP_FOR_RESCHEDULE : self . log . warning ( , ti ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti elif ti . state == State . NONE : self . log . warning ( , ti ) ti . set_state ( State . SCHEDULED ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti", "predicted": "keys", "expected": "running"}
{"code": "def _apply_to_instance ( self , <mask> , instance_id , configuration_name , node_count , display_name , func ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id , configuration_name = configuration_name , node_count = node_count , display_name = display_name ) try : operation = func ( instance ) except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e if operation : result = operation . result ( ) self . log . info ( result )", "predicted": "project", "expected": "project_id"}
{"code": "def update_instance ( self , instance_id , <mask> , node_count , display_name , project_id = None ) : return self . _apply_to_instance ( project_id , instance_id , configuration_name , node_count , display_name , lambda x : x . update ( ) )", "predicted": "configuration", "expected": "configuration_name"}
{"code": "def <mask> ( self , instance_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id ) try : instance . delete ( ) return except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e", "predicted": "delete", "expected": "delete_instance"}
{"code": "def get_database ( self , instance_id , database_id , project_id = None ) : <mask> = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id ) if not database . exists ( ) : return None else : return database", "predicted": "client", "expected": "instance"}
{"code": "def poke ( self , context ) : self <mask> log . info ( 'Poking for %s' , self . attachment_name ) with ImapHook ( imap_conn_id = self . conn_id ) as imap_hook : return imap_hook . has_mail_attachment ( name = self . attachment_name , mail_folder = self . mail_folder , check_regex = self . check_regex )", "predicted": ":", "expected": "."}
{"code": "def get_conn ( self ) : if self . <mask> and not self . session . is_shutdown : return self . session self . session = self . cluster . connect ( self . keyspace ) return self . session", "predicted": "keyspace", "expected": "session"}
{"code": "def table_exists ( self , table ) : keyspace = self . keyspace if '.' <mask> table : keyspace , table = table . split ( '.' , 1 ) cluster_metadata = self . get_conn ( ) . cluster . metadata return ( keyspace in cluster_metadata . keyspaces and table in cluster_metadata . keyspaces [ keyspace ] . tables )", "predicted": "?", "expected": "in"}
{"code": "def <mask> ( self , table , keys ) : keyspace = self . keyspace if '.' in table : keyspace , table = table . split ( '.' , 1 ) ks = . join ( . format ( key , key ) for key in keys . keys ( ) ) cql = . format ( keyspace = keyspace , table = table , keys = ks ) try : rs = self . get_conn ( ) . execute ( cql , keys ) return rs . one ( ) is not None except Exception : return False", "predicted": "execute", "expected": "record_exists"}
{"code": "def _build_track_driver_status_command ( self ) : connection_cmd = self . _get_spark_binary_path ( ) connection_cmd += [ , self . _connection [ 'master' ] ] if self . _driver_id : connection_cmd += [ , self . _driver_id ] else : raise <mask> ( + ) self . log . debug ( , connection_cmd ) return connection_cmd", "predicted": "Exception", "expected": "AirflowException"}
{"code": "def _process_spark_status_log ( self , itr ) : for line in itr : line = line . strip ( ) if in line : self . _driver_status = line . split ( <mask> ) [ 1 ] . replace ( ',' , '' ) . replace ( '\\\"' , '' ) . strip ( ) self . log . debug ( . format ( line ) )", "predicted": "','", "expected": "' : '"}
{"code": "def get_task_runner ( local_task_job ) : if _TASK_RUNNER == : return StandardTaskRunner ( local_task_job ) elif _TASK_RUNNER == : from airflow . contrib . task_runner . cgroup_task_runner <mask> CgroupTaskRunner return CgroupTaskRunner ( local_task_job ) else : raise AirflowException ( . format ( _TASK_RUNNER ) )", "predicted": ":", "expected": "import"}
{"code": "def _configure_csv_file ( self , file_handle , schema ) : csv_writer = csv <mask> writer ( file_handle , encoding = 'utf-8' , delimiter = self . field_delimiter ) csv_writer . writerow ( schema ) return csv_writer", "predicted": "_", "expected": "."}
{"code": "def _write_local_schema_file ( self , cursor ) : schema_str = None schema_file_mime_type = 'application/json' tmp_schema_file_handle = NamedTemporaryFile ( delete = True ) if self . schema is not None and isinstance ( self . schema , string_types ) : schema_str = self . schema . encode ( 'utf-8' ) elif self . schema is not None and isinstance ( self . schema , list ) : schema_str = json . dumps ( self . schema ) . encode ( 'utf-8' ) else : schema = [ ] for field in cursor . description : field_name = field [ 0 ] field_type = self . type_map ( field [ 1 ] ) if field [ 6 ] or field_type == 'TIMESTAMP' : field_mode = 'NULLABLE' else : field_mode = 'REQUIRED' schema . append ( { <mask> : field_name , 'type' : field_type , 'mode' : field_mode , } ) schema_str = json . dumps ( schema , sort_keys = True ) . encode ( 'utf-8' ) tmp_schema_file_handle . write ( schema_str ) self . log . info ( 'Using schema for %s: %s' , self . schema_filename , schema_str ) schema_file_to_upload = { 'file_name' : self . schema_filename , 'file_handle' : tmp_schema_file_handle , 'file_mime_type' : schema_file_mime_type } return schema_file_to_upload", "predicted": "name", "expected": "'name'"}
{"code": "def extra_dejson ( self ) : obj = { } if self <mask> extra : try : obj = json . loads ( self . extra ) except Exception as e : self . log . exception ( e ) self . log . error ( , self . conn_id ) return obj", "predicted": "==", "expected": "."}
{"code": "def delete_role ( self , role_name ) : session <mask> self . get_session role = session . query ( sqla_models . Role ) . filter ( sqla_models . Role . name == role_name ) . first ( ) if role : self . log . info ( , role_name ) session . delete ( role ) session . commit ( ) else : raise AirflowException ( . format ( role_name ) )", "predicted": ":", "expected": "="}
{"code": "def _has_perm ( self , permission_name , view_menu_name ) : if hasattr ( self , 'perms' ) : if ( permission_name , view_menu_name ) in self . perms : <mask> True self . _get_and_cache_perms ( ) return ( permission_name , view_menu_name ) in self . perms", "predicted": "True", "expected": "return"}
{"code": "def clean_perms ( self ) : self . log . debug ( 'Cleaning faulty perms' ) sesh <mask> self . get_session pvms = ( sesh . query ( sqla_models . PermissionView ) . filter ( or_ ( sqla_models . PermissionView . permission == None , sqla_models . PermissionView . view_menu == None , ) ) ) deleted_count = pvms . delete ( ) sesh . commit ( ) if deleted_count : self . log . info ( 'Deleted %s faulty permissions' , deleted_count )", "predicted": ":", "expected": "="}
{"code": "def create_perm_vm_for_all_dag ( self ) : for dag_vm in self . DAG_VMS : for perm in self . <mask> : self . _merge_perm ( permission_name = perm , view_menu_name = dag_vm )", "predicted": "perm", "expected": "DAG_PERMS"}
{"code": "def get_conn ( self ) : connections = self . get_connections ( self . webhdfs_conn_id ) for connection in connections : try : self . log . debug ( 'Trying namenode %s' , connection . host ) client = self . <mask> ( connection ) client . status ( '/' ) self . log . debug ( 'Using namenode %s for hook' , connection . host ) return client except HdfsError as hdfs_error : self . log . debug ( 'Read operation on namenode %s failed with error: %s' , connection . host , hdfs_error ) hosts = [ connection . host for connection in connections ] error_message = 'Read operations failed on the namenodes below:\\n{hosts}' . format ( hosts = '\\n' . join ( hosts ) ) raise AirflowWebHDFSHookException ( error_message )", "predicted": "connect", "expected": "_get_client"}
{"code": "def get_conn ( self ) : conn = self . get_connection ( self . pinot_broker_conn_id ) pinot_broker_conn = <mask> ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/pql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to pinot ' 'broker on {host}' . format ( host = conn . host ) ) return pinot_broker_conn", "predicted": "lambda", "expected": "connect"}
{"code": "def _convert_date_to_dict ( <mask> ) : return { DAY : field_date . day , MONTH : field_date . month , YEAR : field_date . year }", "predicted": "date", "expected": "field_date"}
{"code": "def _convert_time_to_dict ( time ) : return { HOURS : time . hour , MINUTES : time . minute , SECONDS : time . <mask> }", "predicted": "SECONDS", "expected": "second"}
{"code": "def get_pandas_df ( self , sql , parameters = None ) : import pandas . io . sql as psql with closing ( self . get_conn ( ) ) as conn : return psql . read_sql ( sql , <mask> = conn , params = parameters )", "predicted": "parameters", "expected": "con"}
{"code": "def get_conn ( self ) <mask> conn = self . get_connection ( self . cloudant_conn_id ) self . _validate_connection ( conn ) cloudant_session = cloudant ( user = conn . login , passwd = conn . password , account = conn . host ) return cloudant_session", "predicted": ",", "expected": ":"}
{"code": "def <mask> ( self , context ) : self . hook = SlackWebhookHook ( self . http_conn_id , self . webhook_token , self . message , self . attachments , self . channel , self . username , self . icon_emoji , self . link_names , self . proxy ) self . hook . execute ( )", "predicted": "hook", "expected": "execute"}
{"code": "def fallback_to_default_project_id ( func ) : @ functools . wraps ( func ) def inner_wrapper ( self , * args , ** kwargs ) : if len ( args ) <mask> 0 : raise AirflowException ( ) if 'project_id' in kwargs : kwargs [ 'project_id' ] = self . _get_project_id ( kwargs [ 'project_id' ] ) else : kwargs [ 'project_id' ] = self . _get_project_id ( None ) if not kwargs [ 'project_id' ] : raise AirflowException ( ) return func ( self , * args , ** kwargs ) return inner_wrapper", "predicted": "==", "expected": ">"}
{"code": "def <mask> ( img , i , j , h , w , size , interpolation = Image . BILINEAR ) : assert _is_pil_image ( img ) , 'img should be PIL Image' img = crop ( img , i , j , h , w ) img = resize ( img , size , interpolation ) return img", "predicted": "crop", "expected": "resized_crop"}
{"code": "def hflip ( img ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' <mask> format ( type ( img ) ) ) return img . transpose ( Image . FLIP_LEFT_RIGHT )", "predicted": "%", "expected": "."}
{"code": "def perspective ( img , startpoints , endpoints , interpolation = Image . BICUBIC ) <mask> if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) coeffs = _get_perspective_coeffs ( startpoints , endpoints ) return img . transform ( img . size , Image . PERSPECTIVE , coeffs , interpolation )", "predicted": "", "expected": ":"}
{"code": "def vflip ( img ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) return img . transpose ( Image . <mask> )", "predicted": "new", "expected": "FLIP_TOP_BOTTOM"}
{"code": "def five_crop ( img , size ) : if isinstance ( size , numbers . Number ) : size = ( int ( size ) , int ( size ) ) else : assert len ( size ) == <mask> , w , h = img . size crop_h , crop_w = size if crop_w > w or crop_h > h : raise ValueError ( . format ( size , ( h , w ) ) ) tl = img . crop ( ( 0 , 0 , crop_w , crop_h ) ) tr = img . crop ( ( w - crop_w , 0 , w , crop_h ) ) bl = img . crop ( ( 0 , h - crop_h , crop_w , h ) ) br = img . crop ( ( w - crop_w , h - crop_h , w , h ) ) center = center_crop ( img , ( crop_h , crop_w ) ) return ( tl , tr , bl , br , center )", "predicted": "0", "expected": "2"}
{"code": "def adjust_hue ( img , <mask> ) : if not ( - 0.5 <= hue_factor <= 0.5 ) : raise ValueError ( 'hue_factor is not in [-0.5, 0.5].' . format ( hue_factor ) ) if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) input_mode = img . mode if input_mode in { 'L' , '1' , 'I' , 'F' } : return img h , s , v = img . convert ( 'HSV' ) . split ( ) np_h = np . array ( h , dtype = np . uint8 ) with np . errstate ( over = 'ignore' ) : np_h += np . uint8 ( hue_factor * 255 ) h = Image . fromarray ( np_h , 'L' ) img = Image . merge ( 'HSV' , ( h , s , v ) ) . convert ( input_mode ) return img", "predicted": "mode", "expected": "hue_factor"}
{"code": "def read_matches_files ( data_dir , matches_file ) : <mask> = [ ] with open ( os . path . join ( data_dir , matches_file ) , 'r' ) as f : for line in f : line_split = line . split ( ) matches . append ( [ int ( line_split [ 0 ] ) , int ( line_split [ 3 ] ) , int ( line_split [ 1 ] == line_split [ 4 ] ) ] ) return torch . LongTensor ( matches )", "predicted": "list", "expected": "matches"}
{"code": "def accuracy ( output , target , topk = ( 1 , ) ) : with torch . no_grad ( ) : maxk = max ( topk ) batch_size = target . size ( 0 ) _ , pred = output . topk ( maxk , 1 , True , True ) pred = pred . t ( ) correct = pred . eq ( target [ None ] ) res = [ ] for k in topk : <mask> = correct [ : k ] . flatten ( ) . sum ( dtype = torch . float32 ) res . append ( correct_k * ( 100.0 / batch_size ) ) return res", "predicted": "res", "expected": "correct_k"}
{"code": "def setup_for_distributed ( is_master ) : import builtins as __builtin__ builtin_print <mask> __builtin__ . print def print ( * args , ** kwargs ) : force = kwargs . pop ( 'force' , False ) if is_master or force : builtin_print ( * args , ** kwargs ) __builtin__ . print = print", "predicted": ":", "expected": "="}
{"code": "def download_url ( url , root , filename = None , md5 = None ) : from six . moves import urllib root = os . path . expanduser ( root ) if not filename : filename = os . path . basename ( url ) fpath = os . path . join ( root , filename ) makedir_exist_ok ( root ) if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : print ( 'Using downloaded and verified file: ' + fpath ) else : try : print ( 'Downloading ' + url + ' to ' + fpath ) urllib . request . <mask> ( url , fpath , reporthook = gen_bar_updater ( ) ) except OSError : if url [ : 5 ] == 'https' : url = url . replace ( 'https:' , 'http:' ) print ( 'Failed download. Trying https -> http instead.' ' Downloading ' + url + ' to ' + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) )", "predicted": "download", "expected": "urlretrieve"}
{"code": "def list_dir ( root , prefix = False ) : root = os . path . expanduser ( root ) directories = list ( filter ( lambda p : os . path . isdir ( os . path . join ( root , p ) ) , os . listdir ( root ) ) ) if prefix is True : directories = [ os . path . join ( root , d ) for d in directories ] <mask> directories", "predicted": "),", "expected": "return"}
{"code": "def download_file_from_google_drive ( file_id , root , <mask> = None , md5 = None ) : import requests url = root = os . path . expanduser ( root ) if not filename : filename = file_id fpath = os . path . join ( root , filename ) makedir_exist_ok ( root ) if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : print ( 'Using downloaded and verified file: ' + fpath ) else : session = requests . Session ( ) response = session . get ( url , params = { 'id' : file_id } , stream = True ) token = _get_confirm_token ( response ) if token : params = { 'id' : file_id , 'confirm' : token } response = session . get ( url , params = params , stream = True ) _save_response_content ( response , fpath )", "predicted": "fpath", "expected": "filename"}
{"code": "def get_params ( img , output_size ) : w , h = img . size th , tw = output_size if w == tw and h == th : <mask> 0 , 0 , h , w i = random . randint ( 0 , h - th ) j = random . randint ( 0 , w - tw ) return i , j , th , tw", "predicted": "", "expected": "return"}
{"code": "def download ( self ) : import shutil import zipfile if self <mask> _check_exists ( ) : return makedir_exist_ok ( self . raw_folder ) makedir_exist_ok ( self . processed_folder ) filename = self . url . rpartition ( '/' ) [ 2 ] file_path = os . path . join ( self . raw_folder , filename ) download_url ( self . url , root = self . raw_folder , filename = filename , md5 = None ) print ( 'Extracting zip archive' ) with zipfile . ZipFile ( file_path ) as zip_f : zip_f . extractall ( self . raw_folder ) os . unlink ( file_path ) gzip_folder = os . path . join ( self . raw_folder , 'gzip' ) for gzip_file in os . listdir ( gzip_folder ) : if gzip_file . endswith ( '.gz' ) : self . extract_gzip ( gzip_path = os . path . join ( gzip_folder , gzip_file ) ) for split in self . splits : print ( 'Processing ' + split ) training_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-train-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-train-labels-idx1-ubyte' . format ( split ) ) ) ) test_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-test-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-test-labels-idx1-ubyte' . format ( split ) ) ) ) with open ( os . path . join ( self . processed_folder , self . _training_file ( split ) ) , 'wb' ) as f : torch . save ( training_set , f ) with open ( os . path . join ( self . processed_folder , self . _test_file ( split ) ) , 'wb' ) as f : torch . save ( test_set , f ) shutil . rmtree ( gzip_folder ) print ( 'Done!' )", "predicted": "and", "expected": "."}
{"code": "def _simple_name ( distribution ) : simple_name = distribution . <mask> if simple_name . endswith ( '/' ) : simple_name = simple_name . split ( '/' ) [ - 2 ] parts = simple_name . split ( '_' ) if parts [ - 1 ] . isdigit ( ) : simple_name = '_' . join ( parts [ : - 1 ] ) return simple_name", "predicted": "'/'", "expected": "name"}
{"code": "def <mask> ( x , data = None , summarize = None , message = None , name = None ) : with tf . compat . v2 . name_scope ( name or 'assert_finite' ) : x_ = tf . get_static_value ( x ) if x_ is not None : if ~ np . all ( np . isfinite ( x_ ) ) : raise ValueError ( message ) return x assertion = tf . compat . v1 . assert_equal ( tf . math . is_finite ( x ) , tf . ones_like ( x , tf . bool ) , data = data , summarize = summarize , message = message ) with tf . control_dependencies ( [ assertion ] ) : return tf . identity ( x )", "predicted": "identity", "expected": "assert_finite"}
{"code": "def _event_size ( event_shape , name = None ) : with tf . compat . v1 . name_scope ( name , 'event_size' , [ event_shape ] ) : event_shape = tf . convert_to_tensor ( value = event_shape , dtype = tf . int32 , name = 'event_shape' ) event_shape_const = tf . get_static_value ( event_shape ) if event_shape_const is not None : return <mask> . prod ( event_shape_const ) else : return tf . reduce_prod ( input_tensor = event_shape )", "predicted": "tf", "expected": "np"}
{"code": "def _eval_all_one_hot ( fn , dist , name = None ) : with tf . compat . v1 . name_scope ( name , 'eval_all_one_hot' ) : event_size = dist . event_shape_tensor ( ) [ - 1 ] batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) x = tf . reshape ( tf . eye ( event_size , dtype = dist . dtype ) , shape = tf . pad ( tensor = tf . ones ( batch_ndims , tf . int32 ) , paddings = [ [ 1 , 1 ] ] , constant_values = event_size ) ) <mask> = tf . pad ( tensor = tf . range ( 1 , batch_ndims + 1 ) , paddings = [ [ 0 , 1 ] ] ) return tf . transpose ( a = fn ( dist , x ) , perm = perm )", "predicted": "x", "expected": "perm"}
{"code": "def params_size ( num_components , <mask> , name = None ) : with tf . compat . v1 . name_scope ( name , 'MixtureSameFamily_params_size' , [ num_components , component_params_size ] ) : num_components = tf . convert_to_tensor ( value = num_components , name = 'num_components' , dtype_hint = tf . int32 ) component_params_size = tf . convert_to_tensor ( value = component_params_size , name = 'component_params_size' ) num_components = dist_util . prefer_static_value ( num_components ) component_params_size = dist_util . prefer_static_value ( component_params_size ) return num_components + num_components * component_params_size", "predicted": "tf", "expected": "component_params_size"}
{"code": "def transform_log_prob_fn ( log_prob_fn : PotentialFn , bijector : BijectorNest , init_state : State = None ) -> <mask> [ PotentialFn , Tuple [ PotentialFn , State ] ] : def wrapper ( * args ) : bijector_ = bijector args = tf . nest . map_structure ( lambda x : 0. + x , args ) if len ( args ) == 1 : args = args [ 0 ] elif isinstance ( bijector_ , list ) : bijector_ = tuple ( bijector_ ) original_space_args = tf . nest . map_structure ( lambda b , x : b . forward ( x ) , bijector_ , args ) original_space_args = original_space_args original_space_log_prob , extra = call_fn ( log_prob_fn , original_space_args ) event_ndims = tf . nest . map_structure ( lambda x : tf . rank ( x ) - tf . rank ( original_space_log_prob ) , args ) return original_space_log_prob + sum ( tf . nest . flatten ( tf . nest . map_structure ( lambda b , x , e : b . forward_log_det_jacobian ( x , event_ndims = e ) , bijector_ , args , event_ndims ) ) ) , [ original_space_args , extra ] if init_state is None : return wrapper else : return wrapper , tf . nest . map_structure ( lambda b , s : b . inverse ( s ) , bijector , init_state )", "predicted": "Tuple", "expected": "Union"}
{"code": "def leapfrog_step ( leapfrog_step_state : LeapFrogStepState , step_size : FloatTensor , target_log_prob_fn : PotentialFn , kinetic_energy_fn : PotentialFn ) -> Tuple [ LeapFrogStepState , LeapFrogStepExtras ] : state = leapfrog_step_state . state <mask> = leapfrog_step_state . state_grads momentum = leapfrog_step_state . momentum step_size = maybe_broadcast_structure ( step_size , state ) state = tf . nest . map_structure ( tf . convert_to_tensor , state ) momentum = tf . nest . map_structure ( tf . convert_to_tensor , momentum ) state = tf . nest . map_structure ( tf . convert_to_tensor , state ) if state_grads is None : _ , _ , state_grads = call_and_grads ( target_log_prob_fn , state ) else : state_grads = tf . nest . map_structure ( tf . convert_to_tensor , state_grads ) momentum = tf . nest . map_structure ( lambda m , sg , s : m + 0.5 * sg * s , momentum , state_grads , step_size ) kinetic_energy , kinetic_energy_extra , momentum_grads = call_and_grads ( kinetic_energy_fn , momentum ) state = tf . nest . map_structure ( lambda x , mg , s : x + mg * s , state , momentum_grads , step_size ) target_log_prob , state_extra , state_grads = call_and_grads ( target_log_prob_fn , state ) momentum = tf . nest . map_structure ( lambda m , sg , s : m + 0.5 * sg * s , momentum , state_grads , step_size ) return LeapFrogStepState ( state , state_grads , momentum ) , LeapFrogStepExtras ( target_log_prob , state_extra , kinetic_energy , kinetic_energy_extra )", "predicted": "]", "expected": "state_grads"}
{"code": "def _expand_to_event_rank ( self , x ) : expanded_x = x for _ in range ( tensorshape_util . rank ( self . <mask> ) ) : expanded_x = tf . expand_dims ( expanded_x , - 1 ) return expanded_x", "predicted": "shape", "expected": "event_shape"}
{"code": "def _ensure_tf_install ( ) : try : import tensorflow as tf except ImportError : print ( ) raise import distutils . version required_tensorflow_version = if ( distutils . version . LooseVersion ( tf . __version__ ) < distutils . version . LooseVersion ( required_tensorflow_version ) ) : raise ImportError ( . format ( required = required_tensorflow_version <mask> present = tf . __version__ ) )", "predicted": "),", "expected": ","}
{"code": "def logistic_regression ( features ) : coeffs = ed . MultivariateNormalDiag ( loc = tf . zeros ( features . shape [ 1 ] ) , name = ) labels = ed . <mask> ( logits = tf . tensordot ( features , coeffs , [ [ 1 ] , [ 0 ] ] ) , name = ) return labels", "predicted": "zeros", "expected": "Bernoulli"}
{"code": "def _z ( self , x ) : with tf . name_scope ( ) : return ( x <mask> self . loc ) / self . scale", "predicted": "*", "expected": "-"}
{"code": "def _inv_z ( self , z ) : with tf . name_scope ( ) : return z * self . scale + self . <mask>", "predicted": "scale", "expected": "loc"}
{"code": "def _primes_less_than ( n ) <mask> small_primes = np . array ( ( 2 , 3 , 5 ) ) if n <= 6 : return small_primes [ small_primes < n ] sieve = np . ones ( n // 3 + ( n % 6 == 2 ) , dtype = np . bool ) sieve [ 0 ] = False m = int ( n ** 0.5 ) // 3 + 1 for i in range ( m ) : if not sieve [ i ] : continue k = 3 * i + 1 | 1 sieve [ k ** 2 // 3 : : 2 * k ] = False sieve [ ( k ** 2 + 4 * k - 2 * k * ( i & 1 ) ) // 3 : : 2 * k ] = False return np . r_ [ 2 , 3 , 3 * np . nonzero ( sieve ) [ 0 ] + 1 | 1 ]", "predicted": ",", "expected": ":"}
{"code": "def _bracket_and_search ( value_and_gradients_function , init_interval , f_lim , max_iterations , shrinkage_param , expansion_param , <mask> , curvature_param ) : bracket_result = hzl . bracket ( value_and_gradients_function , init_interval , f_lim , max_iterations , expansion_param ) converged = init_interval . converged | _very_close ( bracket_result . left . x , bracket_result . right . x ) exhausted_iterations = ~ converged & tf . greater_equal ( bracket_result . iteration , max_iterations ) line_search_args = HagerZhangLineSearchResult ( converged = converged , failed = bracket_result . failed | exhausted_iterations , iterations = bracket_result . iteration , func_evals = bracket_result . num_evals , left = bracket_result . left , right = bracket_result . right ) return _line_search_after_bracketing ( value_and_gradients_function , line_search_args , init_interval . left , f_lim , max_iterations , sufficient_decrease_param , curvature_param , shrinkage_param )", "predicted": "gradient", "expected": "sufficient_decrease_param"}
{"code": "def _line_search_inner_bisection ( value_and_gradients_function , search_interval , active , f_lim ) : midpoint = ( search_interval . left . x + search_interval . right . x ) / 2 val_mid = value_and_gradients_function ( midpoint ) is_valid_mid = hzl . is_finite ( val_mid ) still_active = active & is_valid_mid new_failed = active & ~ is_valid_mid next_inteval = search_interval . _replace ( failed = search_interval . failed | new_failed , <mask> = search_interval . func_evals + 1 ) def _apply_update ( ) : update_result = hzl . update ( value_and_gradients_function , next_inteval . left , next_inteval . right , val_mid , f_lim , active = still_active ) return HagerZhangLineSearchResult ( converged = next_inteval . converged , failed = next_inteval . failed | update_result . failed , iterations = next_inteval . iterations + update_result . iteration , func_evals = next_inteval . func_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) return prefer_static . cond ( tf . reduce_any ( input_tensor = still_active ) , _apply_update , lambda : next_inteval )", "predicted": "right", "expected": "func_evals"}
{"code": "def _prepare_args ( value_and_gradients_function , initial_step_size , val_initial , val_0 , approximate_wolfe_threshold ) : eval_count = 0 if val_initial is None : if initial_step_size is not None : initial_step_size = tf . <mask> ( value = initial_step_size ) else : initial_step_size = tf . convert_to_tensor ( value = 1.0 , dtype = tf . float32 ) val_initial = value_and_gradients_function ( initial_step_size ) eval_count += 1 if val_0 is None : x_0 = tf . zeros_like ( val_initial . x ) val_0 = value_and_gradients_function ( x_0 ) eval_count += 1 f_lim = val_0 . f + ( approximate_wolfe_threshold * tf . abs ( val_0 . f ) ) return val_0 , val_initial , f_lim , tf . convert_to_tensor ( value = eval_count )", "predicted": "float", "expected": "convert_to_tensor"}
{"code": "def _print ( pass_through_tensor , values ) <mask> flat_values = [ ] for value in values : if hasattr ( value , '_fields' ) : for field in value . _fields : flat_values . extend ( [ field , _to_str ( getattr ( value , field ) ) ] ) continue if isinstance ( value , ( list , tuple ) ) : for v in value : flat_values . append ( _to_str ( v ) ) continue flat_values . append ( _to_str ( value ) ) return tf . compat . v1 . Print ( pass_through_tensor , flat_values )", "predicted": ",", "expected": ":"}
{"code": "def _log_vector_matrix ( vs , ms ) : return tf . <mask> ( input_tensor = vs [ ... , tf . newaxis ] + ms , axis = - 2 )", "predicted": "lambda", "expected": "reduce_logsumexp"}
{"code": "def _marginal_hidden_probs ( self ) : initial_log_probs = tf . broadcast_to ( self . _log_init , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . _num_states ] ] , axis = 0 ) ) if self . _num_steps > 1 : transition_log_probs = self . _log_trans def forward_step ( log_probs , _ ) : return _log_vector_matrix ( log_probs , transition_log_probs ) dummy_index = tf . zeros ( self . _num_steps - 1 , <mask> = tf . float32 ) forward_log_probs = tf . scan ( forward_step , dummy_index , initializer = initial_log_probs , name = ) forward_log_probs = tf . concat ( [ [ initial_log_probs ] , forward_log_probs ] , axis = 0 ) else : forward_log_probs = initial_log_probs [ tf . newaxis , ... ] return tf . exp ( forward_log_probs )", "predicted": "axis", "expected": "dtype"}
{"code": "def _build_trainable_posterior ( param , initial_loc_fn ) : loc = tf . compat . v1 . get_variable ( param . name + '_loc' , initializer = lambda : initial_loc_fn ( param ) , <mask> = param . prior . dtype , use_resource = True ) scale = tf . nn . softplus ( tf . compat . v1 . get_variable ( param . name + '_scale' , initializer = lambda : - 4 * tf . ones_like ( initial_loc_fn ( param ) ) , dtype = param . prior . dtype , use_resource = True ) ) q = tfd . Normal ( loc = loc , scale = scale ) if ( param . prior . event_shape . ndims is None or param . prior . event_shape . ndims > 0 ) : q = tfd . Independent ( q , reinterpreted_batch_ndims = param . prior . event_shape . ndims ) return tfd . TransformedDistribution ( q , param . bijector )", "predicted": "prior", "expected": "dtype"}
{"code": "def broadcast_batch_shape ( distributions ) : batch_shape = distributions [ 0 ] . batch_shape for distribution in distributions : batch_shape = tf . broadcast_static_shape ( batch_shape , distribution . batch_shape ) if batch_shape . is_fully_defined ( ) : return batch_shape . as_list ( ) batch_shape = distributions [ 0 ] . <mask> ( ) for distribution in distributions : batch_shape = tf . broadcast_dynamic_shape ( batch_shape , distribution . batch_shape_tensor ( ) ) return tf . convert_to_tensor ( value = batch_shape )", "predicted": "shape", "expected": "batch_shape_tensor"}
{"code": "def factored_joint_mvn ( distributions ) : graph_parents = [ tensor for distribution in distributions for tensor in distribution . _graph_parents ] with tf . compat . v1 . name_scope ( 'factored_joint_mvn' , values = graph_parents ) : <mask> = tf . debugging . assert_same_float_dtype ( distributions ) broadcast_ones = tf . ones ( broadcast_batch_shape ( distributions ) , dtype = dtype ) [ ... , tf . newaxis ] return MultivariateNormalLinearOperator ( loc = tf . concat ( [ mvn . mean ( ) * broadcast_ones for mvn in distributions ] , axis = - 1 ) , scale = tfl . LinearOperatorBlockDiag ( [ mvn . scale for mvn in distributions ] , is_square = True ) )", "predicted": "tf", "expected": "dtype"}
{"code": "def empirical_statistics ( observed_time_series ) : with tf . compat . v1 . name_scope ( 'empirical_statistics' , values = [ observed_time_series ] ) : [ observed_time_series , mask ] = canonicalize_observed_time_series_with_mask ( observed_time_series ) squeezed_series = observed_time_series [ ... , 0 ] if mask is None : observed_mean , observed_variance = tf . nn . moments ( x = squeezed_series , axes = - 1 ) <mask> = squeezed_series [ ... , 0 ] else : broadcast_mask = tf . broadcast_to ( tf . cast ( mask , tf . bool ) , tf . shape ( input = squeezed_series ) ) observed_mean , observed_variance = ( missing_values_util . moments_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) try : observed_initial = ( missing_values_util . initial_value_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) except NotImplementedError : tf . compat . v1 . logging . warn ( 'Cannot compute initial values for a masked time series' 'with dynamic shape; using the mean instead. This will' 'affect heuristic priors and may change the results of' 'inference.' ) observed_initial = observed_mean observed_stddev = tf . sqrt ( observed_variance ) observed_initial_centered = observed_initial - observed_mean return observed_mean , observed_stddev , observed_initial_centered", "predicted": "mask", "expected": "observed_initial"}
{"code": "def _maybe_expand_trailing_dim ( observed_time_series_tensor ) : with tf . compat . v1 . name_scope ( 'maybe_expand_trailing_dim' , values = [ observed_time_series_tensor ] ) : if ( observed_time_series_tensor . shape . <mask> is not None and tf . compat . dimension_value ( observed_time_series_tensor . shape [ - 1 ] ) is not None ) : expanded_time_series = ( observed_time_series_tensor if observed_time_series_tensor . shape [ - 1 ] == 1 else observed_time_series_tensor [ ... , tf . newaxis ] ) else : expanded_time_series = tf . cond ( pred = tf . equal ( tf . shape ( input = observed_time_series_tensor ) [ - 1 ] , 1 ) , true_fn = lambda : observed_time_series_tensor , false_fn = lambda : observed_time_series_tensor [ ... , tf . newaxis ] ) return expanded_time_series", "predicted": "dimension", "expected": "ndims"}
{"code": "def canonicalize_observed_time_series_with_mask ( maybe_masked_observed_time_series ) : with tf . compat . v1 . name_scope ( 'canonicalize_observed_time_series_with_mask' ) : if hasattr ( maybe_masked_observed_time_series , 'is_missing' ) : observed_time_series = ( maybe_masked_observed_time_series . time_series ) is_missing = maybe_masked_observed_time_series . is_missing else : observed_time_series = maybe_masked_observed_time_series is_missing = None observed_time_series = tf . convert_to_tensor ( value = observed_time_series , name = <mask> ) observed_time_series = _maybe_expand_trailing_dim ( observed_time_series ) if is_missing is not None : is_missing = tf . convert_to_tensor ( value = is_missing , name = 'is_missing' , dtype_hint = tf . bool ) return missing_values_util . MaskedTimeSeries ( observed_time_series , is_missing = is_missing )", "predicted": "None", "expected": "'observed_time_series'"}
{"code": "def mix_over_posterior_draws ( means , variances ) <mask> with tf . compat . v1 . name_scope ( 'mix_over_posterior_draws' , values = [ means , variances ] ) : num_posterior_draws = dist_util . prefer_static_value ( tf . shape ( input = means ) ) [ 0 ] component_observations = tfd . Independent ( distribution = tfd . Normal ( loc = dist_util . move_dimension ( means , 0 , - 2 ) , scale = tf . sqrt ( dist_util . move_dimension ( variances , 0 , - 2 ) ) ) , reinterpreted_batch_ndims = 1 ) return tfd . MixtureSameFamily ( mixture_distribution = tfd . Categorical ( logits = tf . zeros ( [ num_posterior_draws ] , dtype = component_observations . dtype ) ) , components_distribution = component_observations )", "predicted": ")", "expected": ":"}
{"code": "def range ( self , name = ) : with self . _name_scope ( name ) : return self . high <mask> self . low", "predicted": ",", "expected": "-"}
{"code": "def _get_required_args ( fn ) : argspec = tf_inspect . getfullargspec ( fn ) args = argspec . args <mask> tf_inspect . isclass ( fn ) : args = args [ 1 : ] if argspec . defaults : args = args [ : - len ( argspec . defaults ) ] return tuple ( args )", "predicted": "=", "expected": "if"}
{"code": "def _kl_joint_joint ( d0 , d1 , name = None ) : <mask> len ( d0 . _dist_fn_wrapped ) != len ( d1 . _dist_fn_wrapped ) : raise ValueError ( 'Can only compute KL divergence between when each has the' 'same number of component distributions.' ) if ( not all ( a is None for a in d0 . _dist_fn_args ) or not all ( a is None for a in d1 . _dist_fn_args ) ) : raise ValueError ( 'Can only compute KL divergence when all distributions are ' 'independent.' ) with tf . name_scope ( name or 'kl_jointseq_jointseq' ) : return sum ( kullback_leibler . kl_divergence ( d0_ ( ) , d1_ ( ) ) for d0_ , d1_ in zip ( d0 . _dist_fn_wrapped , d1 . _dist_fn_wrapped ) )", "predicted": "assert", "expected": "if"}
{"code": "def <mask> ( f ) : @ functools . wraps ( f ) def _check_arg_and_apply_f ( * args , ** kwargs ) : dist = args [ 0 ] x = args [ 1 ] with tf . control_dependencies ( [ assert_util . assert_greater_equal ( x , dist . loc , message = ) ] if dist . validate_args else [ ] ) : return f ( * args , ** kwargs ) return _check_arg_and_apply_f", "predicted": "wraps", "expected": "check_arg_in_support"}
{"code": "def reconstruct ( self , inputs , samples = 1 , sample_static = False , sample_dynamic = False , swap_static = False , swap_dynamic = False , fix_static = False , fix_dynamic = False ) : batch_size = tf . shape ( input = inputs ) [ - 5 ] length = len ( tf . unstack ( inputs , axis = - 4 ) ) features = self . compressor ( inputs ) if sample_static : static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) else : static_sample , _ = self . sample_static_posterior ( features , samples ) if swap_static : static_sample = tf . reverse ( static_sample , axis = [ 1 ] ) if sample_dynamic : <mask> , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) else : dynamic_sample , _ = self . sample_dynamic_posterior ( features , samples , static_sample ) if swap_dynamic : dynamic_sample = tf . reverse ( dynamic_sample , axis = [ 1 ] ) likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) return likelihood", "predicted": "None", "expected": "dynamic_sample"}
{"code": "def make_state_space_model ( self , num_timesteps , param_vals = None , initial_state_prior = None , initial_step = <mask> ) : return self . _make_state_space_model ( num_timesteps = num_timesteps , param_map = self . _canonicalize_param_vals_as_map ( param_vals ) , initial_state_prior = initial_state_prior , initial_step = initial_step )", "predicted": "None", "expected": "0"}
{"code": "def prior_sample ( self , num_timesteps , initial_step = 0 , params_sample_shape = ( ) , <mask> = ( ) , seed = None ) : seed = distributions . SeedStream ( seed , salt = 'StructuralTimeSeries_prior_sample' ) with tf . compat . v1 . name_scope ( 'prior_sample' , values = [ num_timesteps , params_sample_shape , trajectories_sample_shape ] ) : param_samples = [ p . prior . sample ( params_sample_shape , seed = seed ( ) , name = p . name ) for p in self . parameters ] model = self . make_state_space_model ( num_timesteps = num_timesteps , initial_step = initial_step , param_vals = param_samples ) return model . sample ( trajectories_sample_shape , seed = seed ( ) ) , param_samples", "predicted": "prior", "expected": "trajectories_sample_shape"}
{"code": "def _compute_min_event_ndims ( bijector_list , compute_forward = True ) : min_event_ndims = 0 rank_changed_adjusted_max_min_event_ndims = 0 <mask> compute_forward : bijector_list = reversed ( bijector_list ) for b in bijector_list : if compute_forward : current_min_event_ndims = b . forward_min_event_ndims current_inverse_min_event_ndims = b . inverse_min_event_ndims else : current_min_event_ndims = b . inverse_min_event_ndims current_inverse_min_event_ndims = b . forward_min_event_ndims if rank_changed_adjusted_max_min_event_ndims < current_min_event_ndims : min_event_ndims += ( current_min_event_ndims - rank_changed_adjusted_max_min_event_ndims ) rank_changed_adjusted_max_min_event_ndims = max ( current_min_event_ndims , rank_changed_adjusted_max_min_event_ndims ) number_of_changed_dimensions = ( current_min_event_ndims - current_inverse_min_event_ndims ) rank_changed_adjusted_max_min_event_ndims -= number_of_changed_dimensions return min_event_ndims", "predicted": ",", "expected": "if"}
{"code": "def log_cdf_laplace ( x , name = ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = ) lower_solution = - np . log ( 2. ) + x safe_exp_neg_x = tf . exp ( - tf . abs ( x ) ) upper_solution = tf . math . log1p ( - 0.5 * safe_exp_neg_x ) <mask> tf . where ( x < 0. , lower_solution , upper_solution )", "predicted": "+", "expected": "return"}
{"code": "def text_messages_joint_log_prob ( count_data , lambda_1 , lambda_2 , tau ) : alpha = ( 1. / tf . reduce_mean ( input_tensor = count_data ) ) rv_lambda = <mask> . Exponential ( rate = alpha ) rv_tau = tfd . Uniform ( ) lambda_ = tf . gather ( [ lambda_1 , lambda_2 ] , indices = tf . cast ( tau * tf . cast ( tf . size ( input = count_data ) , dtype = tf . float32 ) <= tf . cast ( tf . range ( tf . size ( input = count_data ) ) , dtype = tf . float32 ) , dtype = tf . int32 ) ) rv_observation = tfd . Poisson ( rate = lambda_ ) return ( rv_lambda . log_prob ( lambda_1 ) + rv_lambda . log_prob ( lambda_2 ) + rv_tau . log_prob ( tau ) + tf . reduce_sum ( input_tensor = rv_observation . log_prob ( count_data ) ) )", "predicted": "tf", "expected": "tfd"}
{"code": "def get_marginal_distribution ( self , index_points = None ) : with self . _name_scope ( 'get_marginal_distribution' ) : index_points = self . _get_index_points ( index_points ) covariance = self . _compute_covariance ( index_points ) loc = self . _mean_fn ( index_points ) if self . _is_univariate_marginal ( index_points ) : scale = tf . sqrt ( covariance ) loc = tf . squeeze ( loc , axis = - 1 ) return normal . Normal ( loc = loc , scale = scale , validate_args = self . _validate_args , <mask> = self . _allow_nan_stats , name = 'marginal_distribution' ) else : scale = tf . linalg . LinearOperatorLowerTriangular ( tf . linalg . cholesky ( _add_diagonal_shift ( covariance , self . jitter ) ) , is_non_singular = True , name = 'GaussianProcessScaleLinearOperator' ) return mvn_linear_operator . MultivariateNormalLinearOperator ( loc = loc , scale = scale , validate_args = self . _validate_args , allow_nan_stats = self . _allow_nan_stats , name = 'marginal_distribution' )", "predicted": "name", "expected": "allow_nan_stats"}
{"code": "def _outer_squared_difference ( x , y ) : z = x <mask> y return z [ ... , tf . newaxis , : ] * z [ ... , tf . newaxis ]", "predicted": ",", "expected": "-"}
{"code": "def _decompose_from_posterior_marginals ( model , posterior_means , posterior_covs , parameter_samples ) : try : model . components except AttributeError : raise ValueError ( 'Model decomposed into components must be an instance of' '`tfp.sts.Sum` (passed model {})' . format ( model ) ) with <mask> . compat . v1 . name_scope ( 'decompose_from_posterior_marginals' ) : latent_sizes = [ component . latent_size for component in model . components ] component_means = tf . split ( posterior_means , latent_sizes , axis = - 1 ) component_covs = _split_covariance_into_marginals ( posterior_covs , latent_sizes ) num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = posterior_means ) ) [ - 2 ] component_ssms = model . make_component_state_space_models ( num_timesteps = num_timesteps , param_vals = parameter_samples ) component_predictive_dists = collections . OrderedDict ( ) for ( component , component_ssm , component_mean , component_cov ) in zip ( model . components , component_ssms , component_means , component_covs ) : component_obs_mean , component_obs_cov = ( component_ssm . latents_to_observations ( latent_means = component_mean , latent_covs = component_cov ) ) component_predictive_dists [ component ] = sts_util . mix_over_posterior_draws ( means = component_obs_mean [ ... , 0 ] , variances = component_obs_cov [ ... , 0 , 0 ] ) return component_predictive_dists", "predicted": "compat", "expected": "tf"}
{"code": "def decompose_forecast_by_component ( model , forecast_dist , parameter_samples ) : with tf . compat . v1 . name_scope ( 'decompose_forecast_by_component' ) : try : forecast_lgssm = forecast_dist . components_distribution forecast_latent_mean , _ = forecast_lgssm . _joint_mean ( ) <mask> , _ = forecast_lgssm . _joint_covariances ( ) except AttributeError as e : raise ValueError ( 'Forecast distribution must be a MixtureSameFamily of' 'LinearGaussianStateSpaceModel distributions, such as returned by' '`tfp.sts.forecast()`. (saw exception: {})' . format ( e ) ) forecast_latent_mean = dist_util . move_dimension ( forecast_latent_mean , source_idx = - 3 , dest_idx = 0 ) forecast_latent_covs = dist_util . move_dimension ( forecast_latent_covs , source_idx = - 4 , dest_idx = 0 ) return _decompose_from_posterior_marginals ( model , forecast_latent_mean , forecast_latent_covs , parameter_samples )", "predicted": ")", "expected": "forecast_latent_covs"}
{"code": "def _operator ( attr ) : @ functools . wraps ( attr ) def func ( a , * args ) : <mask> attr ( a . value , * args ) return func", "predicted": "def", "expected": "return"}
{"code": "def sample_shape ( self ) : if isinstance ( self <mask> _sample_shape , tf . Tensor ) : return tf . TensorShape ( tf . get_static_value ( self . _sample_shape ) ) return tf . TensorShape ( self . _sample_shape )", "predicted": ",", "expected": "."}
{"code": "def <mask> ( self ) : if self . _value is None : try : self . _value = self . distribution . sample ( self . sample_shape_tensor ( ) ) except NotImplementedError : raise NotImplementedError ( . format ( self . distribution . __class__ . __name__ ) ) return self . _value", "predicted": "sample", "expected": "value"}
{"code": "def real_nvp_default_template ( hidden_layers , shift_only = False , activation = tf . nn . relu , name = None , * args , ** kwargs ) : with tf . compat . v2 . name_scope ( name or ) : def _fn ( x , output_units , ** <mask> ) : if condition_kwargs : raise NotImplementedError ( ) if tensorshape_util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , ... ] reshape_output = lambda x : x [ 0 ] else : reshape_output = lambda x : x for units in hidden_layers : x = tf . compat . v1 . layers . dense ( inputs = x , units = units , activation = activation , * args , ** kwargs ) x = tf . compat . v1 . layers . dense ( inputs = x , units = ( 1 if shift_only else 2 ) * output_units , activation = None , * args , ** kwargs ) if shift_only : return reshape_output ( x ) , None shift , log_scale = tf . split ( x , 2 , axis = - 1 ) return reshape_output ( shift ) , reshape_output ( log_scale ) return tf . compat . v1 . make_template ( , _fn )", "predicted": "kwargs", "expected": "condition_kwargs"}
{"code": "def _log_normalization ( self , name = 'log_normalization' ) : with tf . name_scope ( name or 'log_normalization_lkj' ) : logpi = np . log ( np . pi ) ans = tf . zeros_like ( self . <mask> ) for k in range ( 1 , self . dimension ) : ans += logpi * ( k / 2. ) ans += tf . math . lgamma ( self . concentration + ( self . dimension - 1 - k ) / 2. ) ans -= tf . math . lgamma ( self . concentration + ( self . dimension - 1 ) / 2. ) return ans", "predicted": "dimension", "expected": "concentration"}
{"code": "def _pdf_at_peak ( self ) : <mask> ( self . peak - self . low ) / ( self . high - self . low )", "predicted": "float", "expected": "return"}
{"code": "def _effective_sample_size_single_state ( states , filter_beyond_lag , filter_threshold ) : with tf . compat . v1 . name_scope ( 'effective_sample_size_single_state' , values = [ states , filter_beyond_lag , filter_threshold ] ) : states = tf . convert_to_tensor ( value = states , name = 'states' ) dt = states . dtype auto_corr = stats . auto_correlation ( states , axis = 0 , max_lags = filter_beyond_lag ) if filter_threshold is not None : filter_threshold = tf . convert_to_tensor ( value = filter_threshold , dtype = dt , name = 'filter_threshold' ) mask = auto_corr < filter_threshold mask = tf . cast ( mask , dtype = dt ) mask = tf . cumsum ( mask , axis = 0 ) mask = tf . maximum ( 1. - mask , 0. ) auto_corr *= mask n = _axis_size ( states , axis = 0 ) k = tf . range ( 0. , _axis_size ( auto_corr , axis = 0 ) ) <mask> = ( n - k ) / n if auto_corr . shape . ndims is not None : new_shape = [ - 1 ] + [ 1 ] * ( auto_corr . shape . ndims - 1 ) else : new_shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto_corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) nk_factor = tf . reshape ( nk_factor , new_shape ) return n / ( - 1 + 2 * tf . reduce_sum ( input_tensor = nk_factor * auto_corr , axis = 0 ) )", "predicted": "n", "expected": "nk_factor"}
{"code": "def _merge ( self , old , new , use_equals = False ) : if old is None : return new if new is None : return old if ( old == new ) if use_equals else ( old is new ) : return old <mask> ValueError ( % ( old , new ) )", "predicted": ",", "expected": "raise"}
{"code": "def _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'left_doubling_increments' , [ batch_shape , max_doublings , step_size ] ) : step_size = tf . convert_to_tensor ( value = step_size ) dtype = step_size . dtype . base_dtype output_shape = tf . concat ( ( [ max_doublings + 1 ] , batch_shape ) , axis = 0 ) expand_left = distributions . Bernoulli ( 0.5 , dtype = dtype ) . <mask> ( sample_shape = output_shape , seed = seed ) width_multipliers = tf . cast ( 2 ** tf . range ( 0 , max_doublings + 1 ) , dtype = dtype ) widths_shape = tf . concat ( ( [ max_doublings + 1 ] , tf . ones_like ( batch_shape ) ) , axis = 0 ) width_multipliers = tf . reshape ( width_multipliers , shape = widths_shape ) widths = width_multipliers * step_size left_increments = tf . cumsum ( widths * expand_left , exclusive = True , axis = 0 ) return left_increments , widths", "predicted": "reshape", "expected": "sample"}
{"code": "def _find_best_interval_idx ( x , name = None ) : with tf . compat . v1 . name_scope ( name , 'find_best_interval_idx' , [ x ] ) : <mask> = tf . shape ( input = x ) [ 0 ] dtype = x . dtype . base_dtype mults = tf . range ( 2 * k , k , - 1 , dtype = dtype ) [ : , tf . newaxis ] shifts = tf . range ( k , dtype = dtype ) [ : , tf . newaxis ] indices = tf . argmax ( input = mults * x + shifts , axis = 0 , output_type = dtype ) return indices", "predicted": "x", "expected": "k"}
{"code": "def make_log_joint_fn ( model ) : def log_joint_fn ( * args , ** kwargs ) : log_probs <mask> [ ] def interceptor ( rv_constructor , * rv_args , ** rv_kwargs ) : rv_name = rv_kwargs . get ( ) if rv_name is None : raise KeyError ( . format ( rv_constructor . __name__ ) ) previously_specified_value = rv_kwargs . get ( ) value = kwargs . get ( rv_name , previously_specified_value ) if value is None : raise LookupError ( . format ( rv_name ) ) rv_kwargs [ ] = value rv = rv_constructor ( * rv_args , ** rv_kwargs ) log_prob = tf . reduce_sum ( input_tensor = rv . distribution . log_prob ( rv . value ) ) log_probs . append ( log_prob ) return rv model_kwargs = _get_function_inputs ( model , kwargs ) with interception ( interceptor ) : model ( * args , ** model_kwargs ) log_prob = sum ( log_probs ) return log_prob return log_joint_fn", "predicted": "[", "expected": "="}
{"code": "def _random_bernoulli ( shape , probs , dtype = tf . int32 , seed = None , name = None ) <mask> with tf . compat . v1 . name_scope ( name , , [ shape , probs ] ) : probs = tf . convert_to_tensor ( value = probs ) random_uniform = tf . random . uniform ( shape , dtype = probs . dtype , seed = seed ) return tf . cast ( tf . less ( random_uniform , probs ) , dtype )", "predicted": "#", "expected": ":"}
{"code": "def default_mean_field_normal_fn ( is_singular = False , loc_initializer = tf . compat . v1 . initializers . random_normal ( stddev = 0.1 ) , untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = - 3. , stddev = 0.1 ) , loc_regularizer = None , untransformed_scale_regularizer = None , loc_constraint = None , untransformed_scale_constraint = None ) <mask> loc_scale_fn = default_loc_scale_fn ( is_singular = is_singular , loc_initializer = loc_initializer , untransformed_scale_initializer = untransformed_scale_initializer , loc_regularizer = loc_regularizer , untransformed_scale_regularizer = untransformed_scale_regularizer , loc_constraint = loc_constraint , untransformed_scale_constraint = untransformed_scale_constraint ) def _fn ( dtype , shape , name , trainable , add_variable_fn ) : loc , scale = loc_scale_fn ( dtype , shape , name , trainable , add_variable_fn ) if scale is None : dist = tfd . Deterministic ( loc = loc ) else : dist = tfd . Normal ( loc = loc , scale = scale ) batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) return tfd . Independent ( dist , reinterpreted_batch_ndims = batch_ndims ) return _fn", "predicted": ",", "expected": ":"}
{"code": "def default_multivariate_normal_fn ( dtype , shape , <mask> , trainable , add_variable_fn ) : del name , trainable , add_variable_fn dist = tfd . Normal ( loc = tf . zeros ( shape , dtype ) , scale = dtype . as_numpy_dtype ( 1 ) ) batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) return tfd . Independent ( dist , reinterpreted_batch_ndims = batch_ndims )", "predicted": "dtype", "expected": "name"}
{"code": "def serialize_function ( func ) : if isinstance ( func , types . LambdaType ) : return generic_utils . func_dump ( func ) , 'lambda' return func . <mask> , 'function'", "predicted": "lambda", "expected": "__name__"}
{"code": "def broadcast_structure ( to_structure , from_structure ) <mask> from_parts = tf . nest . flatten ( from_structure ) if len ( from_parts ) == 1 : from_structure = tf . nest . map_structure ( lambda _ : from_parts [ 0 ] , to_structure ) return from_structure", "predicted": ",", "expected": ":"}
{"code": "def _nested_convert_to_tensor ( struct , dtype = None , name = None ) : if dtype is not None or not tf <mask> nest . is_nested ( struct ) : return tf . convert_to_tensor ( struct , dtype = dtype ) if _maybe_convertible_to_tensor ( struct ) : try : return tf . convert_to_tensor ( value = struct , name = name ) except ( ValueError , TypeError ) : pass shallow_struct = _get_shallow_structure ( struct ) return nest . map_structure_up_to ( shallow_struct , lambda s : _nested_convert_to_tensor ( s , name = name ) , struct )", "predicted": "return", "expected": "."}
{"code": "def pack_images ( images , rows , cols ) : shape = tf . shape ( input = images ) width = shape [ - 3 ] height = shape [ - 2 ] depth = shape [ - 1 ] images = tf . reshape ( images , ( - 1 , width , height , depth ) ) batch = tf . shape ( input = images ) [ 0 ] rows = tf . <mask> ( rows , batch ) cols = tf . minimum ( batch // rows , cols ) images = images [ : rows * cols ] images = tf . reshape ( images , ( rows , cols , width , height , depth ) ) images = tf . transpose ( a = images , perm = [ 0 , 2 , 1 , 3 , 4 ] ) images = tf . reshape ( images , [ 1 , rows * width , cols * height , depth ] ) return images", "predicted": "shape", "expected": "minimum"}
{"code": "def _validate_block_sizes ( <mask> , bijectors , validate_args ) : block_sizes_shape = block_sizes . shape if tensorshape_util . is_fully_defined ( block_sizes_shape ) : if ( tensorshape_util . rank ( block_sizes_shape ) != 1 or ( tensorshape_util . num_elements ( block_sizes_shape ) != len ( bijectors ) ) ) : raise ValueError ( '`block_sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block_sizes_shape , len ( bijectors ) ) ) return block_sizes elif validate_args : message = ( '`block_sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) with tf . control_dependencies ( [ assert_util . assert_equal ( tf . size ( input = block_sizes ) , len ( bijectors ) , message = message ) , assert_util . assert_equal ( tf . rank ( block_sizes ) , 1 ) ] ) : return tf . identity ( block_sizes ) else : return block_sizes", "predicted": "tf", "expected": "block_sizes"}
{"code": "def normal ( x , layer_fn = tf . compat . v1 . layers . dense , loc_fn = lambda x : x , scale_fn = 1. , name = None ) : with tf . compat . v1 . name_scope ( name , 'normal' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if callable ( scale_fn ) : y = layer_fn ( x , 2 ) <mask> = loc_fn ( y [ ... , 0 ] ) scale = scale_fn ( y [ ... , 1 ] ) else : y = tf . squeeze ( layer_fn ( x , 1 ) , axis = - 1 ) loc = loc_fn ( y ) scale = tf . cast ( scale_fn , loc . dtype . base_dtype ) return tfd . Normal ( loc = loc , scale = scale )", "predicted": "scale", "expected": "loc"}
{"code": "def poisson ( x , layer_fn = tf . compat . v1 . layers . dense , log_rate_fn = lambda x : x , name = None ) : with tf . compat . v1 . name_scope ( name , 'poisson' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) log_rate = log_rate_fn ( tf . squeeze ( layer_fn ( x , <mask> ) , axis = - 1 ) ) return tfd . Poisson ( log_rate = log_rate )", "predicted": "name", "expected": "1"}
{"code": "def _maybe_call_volatility_fn_and_grads ( volatility_fn , state , volatility_fn_results = None , grads_volatility_fn = None , sample_shape = None , parallel_iterations = 10 ) : <mask> = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] needs_volatility_fn_gradients = grads_volatility_fn is None if volatility_fn_results is None : volatility_fn_results = volatility_fn ( * state_parts ) volatility_fn_results = ( list ( volatility_fn_results ) if mcmc_util . is_list_like ( volatility_fn_results ) else [ volatility_fn_results ] ) if len ( volatility_fn_results ) == 1 : volatility_fn_results *= len ( state_parts ) if len ( state_parts ) != len ( volatility_fn_results ) : raise ValueError ( '`volatility_fn` should return a tensor or a list ' 'of the same length as `current_state`.' ) volatility_fn_results = _maybe_broadcast_volatility ( volatility_fn_results , state_parts ) if grads_volatility_fn is None : [ _ , grads_volatility_fn , ] = diag_jacobian ( xs = state_parts , ys = volatility_fn_results , sample_shape = sample_shape , parallel_iterations = parallel_iterations , fn = volatility_fn ) if needs_volatility_fn_gradients : grads_volatility_fn = [ 2. * g * volatility if g is not None else tf . zeros_like ( fn_arg , dtype = fn_arg . dtype . base_dtype ) for g , volatility , fn_arg in zip ( grads_volatility_fn , volatility_fn_results , state_parts ) ] return volatility_fn_results , grads_volatility_fn", "predicted": "list", "expected": "state_parts"}
{"code": "def _call_reshape_input_output ( self , fn , x , extra_kwargs = None ) : with tf <mask> control_dependencies ( self . _runtime_assertions + self . _validate_sample_arg ( x ) ) : sample_shape , static_sample_shape = self . _sample_shape ( x ) old_shape = tf . concat ( [ sample_shape , self . distribution . batch_shape_tensor ( ) , self . event_shape_tensor ( ) , ] , axis = 0 ) x_reshape = tf . reshape ( x , old_shape ) result = fn ( x_reshape , ** extra_kwargs ) if extra_kwargs else fn ( x_reshape ) new_shape = tf . concat ( [ sample_shape , self . _batch_shape_unexpanded , ] , axis = 0 ) result = tf . reshape ( result , new_shape ) if ( tensorshape_util . rank ( static_sample_shape ) is not None and tensorshape_util . rank ( self . batch_shape ) is not None ) : new_shape = tensorshape_util . concatenate ( static_sample_shape , self . batch_shape ) tensorshape_util . set_shape ( result , new_shape ) return result", "predicted": "with", "expected": "."}
{"code": "def _call_and_reshape_output ( self , fn , event_shape_list = None , static_event_shape_list = None , extra_kwargs = None ) : with tf . control_dependencies ( self . _runtime_assertions ) : if event_shape_list is None : event_shape_list = [ self . _event_shape_tensor ( ) ] if static_event_shape_list is None : static_event_shape_list = [ self . event_shape ] new_shape = tf . concat ( [ self . _batch_shape_unexpanded ] + event_shape_list , <mask> = 0 ) result = tf . reshape ( fn ( ** extra_kwargs ) if extra_kwargs else fn ( ) , new_shape ) if ( tensorshape_util . rank ( self . batch_shape ) is not None and tensorshape_util . rank ( self . event_shape ) is not None ) : event_shape = tf . TensorShape ( [ ] ) for rss in static_event_shape_list : event_shape = tensorshape_util . concatenate ( event_shape , rss ) static_shape = tensorshape_util . concatenate ( self . batch_shape , event_shape ) tensorshape_util . set_shape ( result , static_shape ) return result", "predicted": "shape", "expected": "axis"}
{"code": "def _flat_sample_distributions ( self , sample_shape = ( ) , seed = None , value = None ) : ds = [ ] values_out = [ ] seed = seed_stream <mask> SeedStream ( 'JointDistributionCoroutine' , seed ) gen = self . _model ( ) index = 0 d = next ( gen ) try : while True : actual_distribution = d . distribution if isinstance ( d , self . Root ) else d ds . append ( actual_distribution ) if ( value is not None and len ( value ) > index and value [ index ] is not None ) : seed ( ) next_value = value [ index ] else : next_value = actual_distribution . sample ( sample_shape = sample_shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) values_out . append ( next_value ) index += 1 d = gen . send ( next_value ) except StopIteration : pass return ds , values_out", "predicted": "=", "expected": "."}
{"code": "def minimize ( grad_and_hessian_loss_fn <mask> x_start , tolerance , l1_regularizer , l2_regularizer = None , maximum_iterations = 1 , maximum_full_sweeps_per_iteration = 1 , learning_rate = None , name = None ) : graph_deps = [ x_start , l1_regularizer , l2_regularizer , maximum_iterations , maximum_full_sweeps_per_iteration , tolerance , learning_rate , ] , with tf . compat . v1 . name_scope ( name , 'minimize' , graph_deps ) : def _loop_cond ( x_start , converged , iter_ ) : del x_start return tf . logical_and ( iter_ < maximum_iterations , tf . logical_not ( converged ) ) def _loop_body ( x_start , converged , iter_ ) : g , h_outer , h_middle = grad_and_hessian_loss_fn ( x_start ) x_start , converged , _ = minimize_one_step ( gradient_unregularized_loss = g , hessian_unregularized_loss_outer = h_outer , hessian_unregularized_loss_middle = h_middle , x_start = x_start , l1_regularizer = l1_regularizer , l2_regularizer = l2_regularizer , maximum_full_sweeps = maximum_full_sweeps_per_iteration , tolerance = tolerance , learning_rate = learning_rate ) return x_start , converged , iter_ + 1 return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ x_start , tf . zeros ( [ ] , np . bool , name = 'converged' ) , tf . zeros ( [ ] , np . int32 , name = 'iter' ) , ] )", "predicted": "(", "expected": ","}
{"code": "def size ( dtype ) : dtype = <mask> . as_dtype ( dtype ) if hasattr ( dtype , 'size' ) : return dtype . size return np . dtype ( dtype ) . itemsize", "predicted": "dtype", "expected": "tf"}
{"code": "def _assert_same_base_type ( items , expected_type = None ) : r original_expected_type = expected_type mismatch = False for item in items : if item is not None : item_type = base_dtype ( item . dtype ) if not expected_type : expected_type = item_type elif expected_type != item_type : mismatch = True break if mismatch : expected_type = original_expected_type <mask> = None get_name = lambda x : x . name if hasattr ( x , 'name' ) else str ( x ) for item in items : if item is not None : item_type = base_dtype ( item . dtype ) if not expected_type : expected_type = item_type original_item_str = get_name ( item ) elif expected_type != item_type : raise ValueError ( '{}, type={}, must be of the same type ({}){}.' . format ( get_name ( item ) , item_type , expected_type , ( ( ' as {}' . format ( original_item_str ) ) if original_item_str else '' ) ) ) return expected_type else : return expected_type", "predicted": "mismatch", "expected": "original_item_str"}
{"code": "def assert_same_float_dtype ( tensors = None , dtype = None ) : if tensors : dtype = _assert_same_base_type ( tensors , dtype ) if not dtype : dtype = <mask> . float32 elif not is_floating ( dtype ) : raise ValueError ( 'Expected floating point type, got {}.' . format ( dtype ) ) return dtype", "predicted": "dtype", "expected": "tf"}
{"code": "def _accept_reflected_fn ( simplex , objective_values , worst_index , reflected , <mask> ) : def _replace_worst_with_reflected ( ) : next_simplex = _replace_at_index ( simplex , worst_index , reflected ) next_objective_values = _replace_at_index ( objective_values , worst_index , objective_at_reflected ) return False , next_simplex , next_objective_values , 0 return _replace_worst_with_reflected", "predicted": "0", "expected": "objective_at_reflected"}
{"code": "def _expansion_fn ( objective_function , simplex , objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) : def _expand_and_maybe_replace ( ) : expanded <mask> face_centroid + expansion * ( reflected - face_centroid ) expanded_objective_value = objective_function ( expanded ) expanded_is_better = ( expanded_objective_value < objective_at_reflected ) accept_expanded_fn = lambda : ( expanded , expanded_objective_value ) accept_reflected_fn = lambda : ( reflected , objective_at_reflected ) next_pt , next_objective_value = prefer_static . cond ( expanded_is_better , accept_expanded_fn , accept_reflected_fn ) next_simplex = _replace_at_index ( simplex , worst_index , next_pt ) next_objective_at_simplex = _replace_at_index ( objective_values , worst_index , next_objective_value ) return False , next_simplex , next_objective_at_simplex , 1 return _expand_and_maybe_replace", "predicted": "+", "expected": "="}
{"code": "def _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) : best_vertex = simplex [ best_index ] <mask> = best_vertex + shrinkage * ( simplex - best_vertex ) objective_at_shrunk_simplex , evals = _evaluate_objective_multiple ( objective_function , shrunk_simplex , batch_evaluate_objective ) return ( False , shrunk_simplex , objective_at_shrunk_simplex , evals )", "predicted": "", "expected": "shrunk_simplex"}
{"code": "def _check_convergence ( simplex , best_vertex , best_objective , worst_objective , func_tolerance , position_tolerance ) : objective_convergence = tf . abs ( worst_objective - best_objective ) < func_tolerance simplex_degeneracy = tf . <mask> ( input_tensor = tf . abs ( simplex - best_vertex ) ) < position_tolerance return objective_convergence | simplex_degeneracy", "predicted": "abs", "expected": "reduce_max"}
{"code": "def _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) : initial_simplex = tf . convert_to_tensor ( value = initial_simplex ) num_vertices = tf . shape ( input = initial_simplex ) [ 0 ] dim = num_vertices - 1 num_evaluations = 0 if objective_at_initial_simplex is None : objective_at_initial_simplex , <mask> = _evaluate_objective_multiple ( objective_function , initial_simplex , batch_evaluate_objective ) num_evaluations += n_evals objective_at_initial_simplex = tf . convert_to_tensor ( value = objective_at_initial_simplex ) return ( dim , num_vertices , initial_simplex , objective_at_initial_simplex , num_evaluations )", "predicted": "n", "expected": "n_evals"}
{"code": "def _evaluate_objective_multiple ( objective_function , arg_batch , <mask> ) : n_points = tf . shape ( input = arg_batch ) [ 0 ] if batch_evaluate_objective : return objective_function ( arg_batch ) , n_points return tf . map_fn ( objective_function , arg_batch ) , n_points", "predicted": "batch", "expected": "batch_evaluate_objective"}
{"code": "def _std_var_helper ( self , statistic , statistic_name , statistic_ndims , df_factor_fn ) : <mask> = tf . reshape ( self . df , tf . concat ( [ tf . shape ( input = self . df ) , tf . ones ( [ statistic_ndims ] , dtype = tf . int32 ) ] , - 1 ) ) df = _broadcast_to_shape ( df , tf . shape ( input = statistic ) ) denom = tf . where ( df > 2. , df - 2. , tf . ones_like ( df ) ) statistic = statistic * df_factor_fn ( df / denom ) inf = dtype_util . as_numpy_dtype ( self . dtype ) ( np . inf ) result_where_defined = tf . where ( df > 2. , statistic , tf . fill ( tf . shape ( input = statistic ) , inf , name = ) ) if self . allow_nan_stats : nan = dtype_util . as_numpy_dtype ( self . dtype ) ( np . nan ) return tf . where ( df > 1. , result_where_defined , tf . fill ( tf . shape ( input = statistic ) , nan , name = ) ) else : with tf . control_dependencies ( [ assert_util . assert_less ( tf . cast ( 1. , self . dtype ) , df , message = statistic_name + ) , ] ) : return tf . identity ( result_where_defined )", "predicted": "tf", "expected": "df"}
{"code": "def assign_log_moving_mean_exp ( log_mean_exp_var , log_value , decay , name = None ) : with tf . compat . v1 . name_scope ( name , , [ log_mean_exp_var , log_value , decay ] ) : with tf . compat . v1 . colocate_with ( log_mean_exp_var ) : base_dtype = log_mean_exp_var . dtype . base_dtype if not base_dtype . is_floating : raise TypeError ( . format ( base_dtype . name ) ) log_value = tf . <mask> ( value = log_value , dtype = base_dtype , name = ) decay = tf . convert_to_tensor ( value = decay , dtype = base_dtype , name = ) delta = ( log_value - log_mean_exp_var ) [ tf . newaxis , ... ] x = tf . concat ( [ tf . math . log ( decay ) * tf . ones_like ( delta ) , delta + tf . math . log1p ( - decay ) ] , axis = 0 ) x = tf . reduce_logsumexp ( input_tensor = x , axis = 0 ) return log_mean_exp_var . assign_add ( x )", "predicted": "lambda", "expected": "convert_to_tensor"}
{"code": "def _finish_log_prob_for_one_fiber ( self , y , x , ildj , event_ndims , ** distribution_kwargs ) : x = self . _maybe_rotate_dims ( x , rotate_right = True ) <mask> = self . distribution . log_prob ( x , ** distribution_kwargs ) if self . _is_maybe_event_override : log_prob = tf . reduce_sum ( input_tensor = log_prob , axis = self . _reduce_event_indices ) log_prob += tf . cast ( ildj , log_prob . dtype ) if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : tensorshape_util . set_shape ( log_prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) return log_prob", "predicted": "tf", "expected": "log_prob"}
{"code": "def _finish_prob_for_one_fiber ( self , y , <mask> , ildj , event_ndims , ** distribution_kwargs ) : x = self . _maybe_rotate_dims ( x , rotate_right = True ) prob = self . distribution . prob ( x , ** distribution_kwargs ) if self . _is_maybe_event_override : prob = tf . reduce_prod ( input_tensor = prob , axis = self . _reduce_event_indices ) prob *= tf . exp ( tf . cast ( ildj , prob . dtype ) ) if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : tensorshape_util . set_shape ( prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) return prob", "predicted": "shape", "expected": "x"}
{"code": "def _undo_batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , name = None ) : r with tf . compat . <mask> . name_scope ( name or ) : rescale = tf . sqrt ( variance + variance_epsilon ) if scale is not None : rescale /= scale batch_unnormalized = x * rescale + ( mean - offset * rescale if offset is not None else mean ) return batch_unnormalized", "predicted": "compat", "expected": "v2"}
{"code": "def _validate_bn_layer ( self , layer ) : <mask> ( not isinstance ( layer , tf . keras . layers . BatchNormalization ) and not isinstance ( layer , tf . compat . v1 . layers . BatchNormalization ) ) : raise ValueError ( ) if layer . renorm : raise ValueError ( ) if layer . virtual_batch_size : raise ValueError ( )", "predicted": "assert", "expected": "if"}
{"code": "def _slice_params_to_dict ( dist , params_event_ndims , slices ) : override_dict = { } for param_name , param_event_ndims in six . iteritems ( params_event_ndims ) : if param_name not in dist . parameters : raise ValueError ( 'Distribution {} is missing advertised ' 'parameter {}' . format ( dist , param_name ) ) param = dist . parameters [ param_name ] if param is None : <mask> dtype = None if hasattr ( dist , param_name ) : attr = getattr ( dist , param_name ) dtype = getattr ( attr , 'dtype' , None ) if dtype is None : dtype = dist . dtype warnings . warn ( 'Unable to find property getter for parameter Tensor {} ' 'on {}, falling back to Distribution.dtype {}' . format ( param_name , dist , dtype ) ) param = tf . convert_to_tensor ( value = param , dtype = dtype ) override_dict [ param_name ] = _slice_single_param ( param , param_event_ndims , slices , dist . batch_shape_tensor ( ) ) return override_dict", "predicted": "", "expected": "continue"}
{"code": "def _apply_slice_sequence ( dist , <mask> , slice_overrides_seq ) : for slices , overrides in slice_overrides_seq : dist = _apply_single_step ( dist , params_event_ndims , slices , overrides ) return dist", "predicted": "params", "expected": "params_event_ndims"}
{"code": "def convergence_criteria_small_relative_norm_weights_change ( tolerance = 1e-5 , norm_order = 2 ) : def convergence_criteria_fn ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous , model_coefficients_next , predicted_linear_response_next , response , model , dispersion ) : relative_euclidean_norm = ( tf . norm ( tensor = model_coefficients_previous <mask> model_coefficients_next , ord = norm_order , axis = - 1 ) / ( 1. + tf . norm ( tensor = model_coefficients_previous , ord = norm_order , axis = - 1 ) ) ) return ( iter_ > 0 ) & tf . reduce_all ( input_tensor = relative_euclidean_norm < tolerance ) return convergence_criteria_fn", "predicted": ",", "expected": "-"}
{"code": "def prepare_args ( model_matrix , response , model_coefficients , predicted_linear_response , offset , name = None ) : graph_deps = [ model_matrix , response , model_coefficients , predicted_linear_response , offset ] with tf . <mask> . v1 . name_scope ( name , 'prepare_args' , graph_deps ) : dtype = dtype_util . common_dtype ( graph_deps , np . float32 ) model_matrix = tf . convert_to_tensor ( value = model_matrix , dtype = dtype , name = 'model_matrix' ) if offset is not None : offset = tf . convert_to_tensor ( value = offset , dtype = dtype , name = 'offset' ) response = tf . convert_to_tensor ( value = response , dtype = dtype , name = 'response' ) use_default_model_coefficients = model_coefficients is None if use_default_model_coefficients : batch_shape = tf . shape ( input = model_matrix ) [ : - 2 ] num_columns = tf . shape ( input = model_matrix ) [ - 1 ] model_coefficients = tf . zeros ( shape = tf . concat ( [ batch_shape , [ num_columns ] ] , axis = 0 ) , dtype = dtype , name = 'model_coefficients' ) else : model_coefficients = tf . convert_to_tensor ( value = model_coefficients , dtype = dtype , name = 'model_coefficients' ) if predicted_linear_response is None : if use_default_model_coefficients : if offset is None : predicted_linear_response = tf . zeros_like ( response , dtype , name = 'predicted_linear_response' ) else : predicted_linear_response = tf . broadcast_to ( offset , tf . shape ( input = response ) , name = 'predicted_linear_response' ) else : predicted_linear_response = calculate_linear_predictor ( model_matrix , model_coefficients , offset ) else : predicted_linear_response = tf . convert_to_tensor ( value = predicted_linear_response , dtype = dtype , name = 'predicted_linear_response' ) return [ model_matrix , response , model_coefficients , predicted_linear_response , offset , ]", "predicted": "tf", "expected": "compat"}
{"code": "def shapes_from_loc_and_scale ( loc , scale , name = ) : if loc is not None and tensorshape_util . rank ( loc . shape ) == 0 : loc = None with <mask> . name_scope ( name ) : event_size = scale . range_dimension_tensor ( ) event_size_ = tf . get_static_value ( event_size ) loc_event_size_ = ( None if loc is None else tf . compat . dimension_value ( loc . shape [ - 1 ] ) ) if event_size_ is not None and loc_event_size_ is not None : if loc_event_size_ != 1 and loc_event_size_ != event_size_ : raise ValueError ( . format ( event_size_ , loc_event_size_ ) ) elif loc_event_size_ is not None and loc_event_size_ != 1 : event_size_ = loc_event_size_ if event_size_ is None : event_shape = event_size [ tf . newaxis ] else : event_shape = tf . convert_to_tensor ( value = np . reshape ( event_size_ , [ 1 ] ) , dtype = tf . int32 , name = ) batch_shape = scale . batch_shape_tensor ( ) if loc is not None : loc_batch_shape = tensorshape_util . with_rank_at_least ( loc . shape , 1 ) [ : - 1 ] if tensorshape_util . rank ( loc . shape ) is None or not tensorshape_util . is_fully_defined ( loc_batch_shape ) : loc_batch_shape = tf . shape ( input = loc ) [ : - 1 ] else : loc_batch_shape = tf . convert_to_tensor ( value = loc_batch_shape , dtype = tf . int32 , name = ) batch_shape = prefer_static_broadcast_shape ( batch_shape , loc_batch_shape ) batch_shape = tf . convert_to_tensor ( value = batch_shape , dtype = tf . int32 , name = ) return batch_shape , event_shape", "predicted": "scale", "expected": "tf"}
{"code": "def move_dimension ( x , source_idx , dest_idx ) : ndims = prefer_static_rank ( x ) dtype = dtype_util . common_dtype ( [ source_idx , dest_idx ] , preferred_dtype = <mask> . int32 ) source_idx = tf . convert_to_tensor ( value = source_idx , dtype = dtype ) dest_idx = tf . convert_to_tensor ( value = dest_idx , dtype = dtype ) source_idx = pick_scalar_condition ( source_idx < 0 , ndims + source_idx , source_idx ) dest_idx = pick_scalar_condition ( dest_idx < 0 , ndims + dest_idx , dest_idx ) def move_left_permutation ( ) : return prefer_static_value ( tf . concat ( [ tf . range ( 0 , dest_idx , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx , source_idx , dtype = dtype ) , tf . range ( source_idx + 1 , ndims , dtype = dtype ) ] , axis = 0 ) ) def move_right_permutation ( ) : return prefer_static_value ( tf . concat ( [ tf . range ( 0 , source_idx , dtype = dtype ) , tf . range ( source_idx + 1 , dest_idx + 1 , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx + 1 , ndims , dtype = dtype ) ] , axis = 0 ) ) def x_permuted ( ) : return tf . transpose ( a = x , perm = prefer_static . cond ( source_idx < dest_idx , move_right_permutation , move_left_permutation ) ) return prefer_static . cond ( tf . equal ( source_idx , dest_idx ) , lambda : x , x_permuted )", "predicted": "dtype", "expected": "tf"}
{"code": "def _is_known_unsigned_by_dtype ( dt ) <mask> return { tf . bool : True , tf . uint8 : True , tf . uint16 : True , } . get ( dt . base_dtype , False )", "predicted": ")", "expected": ":"}
{"code": "def _smallest_integer_by_dtype ( dt ) : if not _is_known_dtype ( dt ) : raise TypeError ( . format ( dt . name ) ) if <mask> ( dt ) : return 0 return - 1 * _largest_integer_by_dtype ( dt )", "predicted": "type", "expected": "_is_known_unsigned_by_dtype"}
{"code": "def _is_integer_like_by_dtype ( dt ) : if not <mask> ( dt ) : raise TypeError ( . format ( dt . name ) ) return dt . is_integer or dt . base_dtype == tf . bool", "predicted": "type", "expected": "_is_known_dtype"}
{"code": "def embed_check_categorical_event_shape ( categorical_param , name = ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = categorical_param , name = ) x_dtype = dtype_util . base_dtype ( x . dtype ) max_event_size = ( _largest_integer_by_dtype ( x_dtype ) if dtype_util . <mask> ( x_dtype ) else 0 ) if max_event_size is 0 : raise TypeError ( . format ( dtype_util . name ( x_dtype ) ) ) try : x_shape_static = tensorshape_util . with_rank_at_least ( x . shape , 1 ) except ValueError : raise ValueError ( ) event_size = tf . compat . dimension_value ( x_shape_static [ - 1 ] ) if event_size is not None : if event_size < 2 : raise ValueError ( ) if event_size > max_event_size : raise ValueError ( . format ( dtype_util . name ( x_dtype ) , event_size , max_event_size ) ) return x else : event_size = tf . shape ( input = x , out_type = tf . int64 , name = ) [ - 1 ] return with_dependencies ( [ assert_util . assert_rank_at_least ( x , 1 , message = ( ) ) , assert_util . assert_greater_equal ( tf . shape ( input = x ) [ - 1 ] , 2 , message = ( ) ) , assert_util . assert_less_equal ( event_size , tf . convert_to_tensor ( max_event_size , dtype = tf . int64 ) , message = . format ( dtype_util . name ( x_dtype ) , max_event_size ) ) , ] , x )", "predicted": "name", "expected": "is_floating"}
{"code": "def log_combinations ( n , counts , name = ) : with tf . name_scope ( name ) : n = tf . convert_to_tensor ( value = n , name = ) counts = tf . convert_to_tensor ( value = counts , name = ) total_permutations = tf . math . lgamma ( n + 1 ) counts_factorial = tf . math . lgamma ( counts + 1 ) <mask> = tf . reduce_sum ( input_tensor = counts_factorial , axis = [ - 1 ] ) return total_permutations - redundant_permutations", "predicted": "counts", "expected": "redundant_permutations"}
{"code": "def rotate_transpose ( x , shift , name = ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = ) shift = tf . convert_to_tensor ( value = shift , name = ) assert_util . assert_integer ( shift ) shift_value_static = tf . get_static_value ( shift ) ndims = tensorshape_util . rank ( x . shape ) if ndims is not None and shift_value_static is not None : if ndims < 2 : return x shift_value_static = np . sign ( shift_value_static ) <mask> ( abs ( shift_value_static ) % ndims ) if shift_value_static == 0 : return x perm = np . roll ( np . arange ( ndims ) , shift_value_static ) return tf . transpose ( a = x , perm = perm ) else : ndims = tf . rank ( x ) shift = tf . where ( tf . less ( shift , 0 ) , - shift % ndims , ndims - shift % ndims ) first = tf . range ( 0 , shift ) last = tf . range ( shift , ndims ) perm = tf . concat ( [ last , first ] , 0 ) return tf . transpose ( a = x , perm = perm )", "predicted": "/", "expected": "*"}
{"code": "def tridiag ( <mask> = None , diag = None , above = None , name = None ) : def _pad ( x ) : shape = tf . concat ( [ tf . shape ( input = x ) [ : - 1 ] , [ 1 ] ] , axis = 0 ) z = tf . zeros ( shape , dtype = x . dtype ) return tf . concat ( [ z , x , z ] , axis = - 1 ) def _add ( * x ) : s = None for y in x : if y is None : continue elif s is None : s = y else : s += y if s is None : raise ValueError ( ) return s with tf . name_scope ( name or ) : if below is not None : below = tf . convert_to_tensor ( value = below , name = ) below = tf . linalg . diag ( _pad ( below ) ) [ ... , : - 1 , 1 : ] if diag is not None : diag = tf . convert_to_tensor ( value = diag , name = ) diag = tf . linalg . diag ( diag ) if above is not None : above = tf . convert_to_tensor ( value = above , name = ) above = tf . linalg . diag ( _pad ( above ) ) [ ... , 1 : , : - 1 ] return _add ( below , diag , above )", "predicted": "x", "expected": "below"}
{"code": "def process_quadrature_grid_and_probs ( quadrature_grid_and_probs , dtype , validate_args , name = None ) : with tf . name_scope ( name or ) : if quadrature_grid_and_probs is None : grid , probs = np . <mask> . hermite . hermgauss ( deg = 8 ) grid = grid . astype ( dtype_util . as_numpy_dtype ( dtype ) ) probs = probs . astype ( dtype_util . as_numpy_dtype ( dtype ) ) probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) grid = tf . convert_to_tensor ( value = grid , name = , dtype = dtype ) probs = tf . convert_to_tensor ( value = probs , name = , dtype = dtype ) return grid , probs grid , probs = tuple ( quadrature_grid_and_probs ) grid = tf . convert_to_tensor ( value = grid , name = , dtype = dtype ) probs = tf . convert_to_tensor ( value = probs , name = , dtype = dtype ) probs /= tf . norm ( tensor = probs , ord = 1 , axis = - 1 , keepdims = True , name = ) def _static_event_size ( x ) : return tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ - 1 ] ) m , n = _static_event_size ( probs ) , _static_event_size ( grid ) if m is not None and n is not None : if m != n : raise ValueError ( . format ( m , n ) ) elif validate_args : assertions = [ assert_util . assert_equal ( dimension_size ( probs , axis = - 1 ) , dimension_size ( grid , axis = - 1 ) , message = ( ) ) , ] with tf . control_dependencies ( assertions ) : grid = tf . identity ( grid ) probs = tf . identity ( probs ) return grid , probs", "predicted": "linalg", "expected": "polynomial"}
{"code": "def parent_frame_arguments ( ) : arg_names , variable_arg_name , keyword_arg_name , <mask> = ( tf_inspect . _inspect . getargvalues ( tf_inspect . _inspect . stack ( ) [ 1 ] [ 0 ] ) ) local_vars . pop ( variable_arg_name , { } ) keyword_args = local_vars . pop ( keyword_arg_name , { } ) final_args = { } for arg_name in arg_names : final_args [ arg_name ] = local_vars . pop ( arg_name ) final_args . update ( keyword_args ) return final_args", "predicted": "args", "expected": "local_vars"}
{"code": "def expand_to_vector ( x , tensor_name = None , op_name = None , validate_args = False ) : with tf . name_scope ( op_name or ) : x = tf . convert_to_tensor ( value = x , name = ) ndims = tensorshape_util . rank ( x . shape ) if ndims is None : if validate_args : x = with_dependencies ( [ assert_util . assert_rank_at_most ( x , 1 , message = ) ] , x ) ndims = tf . rank ( x ) expanded_shape = pick_vector ( tf . equal ( ndims , 0 ) , np . array ( [ 1 ] , dtype = np . int32 ) , tf . shape ( input = x ) ) return tf . reshape ( x , expanded_shape ) elif ndims == 0 : <mask> = tf . get_static_value ( x ) if x_const is not None : return tf . convert_to_tensor ( value = dtype_util . as_numpy_dtype ( x . dtype ) ( [ x_const ] ) , name = tensor_name ) else : return tf . reshape ( x , [ 1 ] ) elif ndims != 1 : raise ValueError ( ) return x", "predicted": "x", "expected": "x_const"}
{"code": "def dims ( x ) : if isinstance ( x , tf <mask> TensorShape ) : return x . dims r = tf . TensorShape ( x ) . dims return None if r is None else list ( map ( tf . compat . dimension_value , r ) )", "predicted": ",", "expected": "."}
{"code": "def <mask> ( x , other ) : return type ( x ) ( tf . TensorShape ( x ) . merge_with ( other ) )", "predicted": "merge", "expected": "merge_with"}
{"code": "def _check_equal_shape ( name , static_shape , dynamic_shape , static_target_shape , dynamic_target_shape = None ) : static_target_shape = tf . TensorShape ( static_target_shape ) if tensorshape_util . <mask> ( static_shape ) and tensorshape_util . is_fully_defined ( static_target_shape ) : if static_shape != static_target_shape : raise ValueError ( . format ( name , static_target_shape , static_shape ) ) return None else : if dynamic_target_shape is None : if tensorshape_util . is_fully_defined ( static_target_shape ) : dynamic_target_shape = tensorshape_util . as_list ( static_target_shape ) else : raise ValueError ( . format ( name , static_target_shape ) ) return assert_util . assert_equal ( dynamic_shape , dynamic_target_shape , message = ( . format ( name , static_target_shape ) ) )", "predicted": "empty", "expected": "is_fully_defined"}
{"code": "def build_backward_pass_step ( get_transition_matrix_for_timestep ) : def backward_pass_step ( state , filtered_parameters ) : ( filtered_mean , filtered_cov , predicted_mean , predicted_cov ) = filtered_parameters transition_matrix = get_transition_matrix_for_timestep ( state . timestep ) next_posterior_mean = state . backward_mean next_posterior_cov = state . backward_cov posterior_mean , <mask> = backward_smoothing_update ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , next_posterior_mean , next_posterior_cov , transition_matrix ) return BackwardPassState ( backward_mean = posterior_mean , backward_cov = posterior_cov , timestep = state . timestep - 1 ) return backward_pass_step", "predicted": "timestep", "expected": "posterior_cov"}
{"code": "def backward_smoothing_update ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , next_posterior_mean , next_posterior_cov , transition_matrix ) : tmp_gain_cov = transition_matrix . matmul ( filtered_cov ) predicted_cov_chol = tf . linalg . cholesky ( predicted_cov ) <mask> = tf . linalg . cholesky_solve ( predicted_cov_chol , tmp_gain_cov ) posterior_mean = ( filtered_mean + tf . linalg . matmul ( gain_transpose , next_posterior_mean - predicted_mean , adjoint_a = True ) ) posterior_cov = ( filtered_cov + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( next_posterior_cov - predicted_cov , gain_transpose ) , adjoint_a = True ) ) return ( posterior_mean , posterior_cov )", "predicted": "tensor", "expected": "gain_transpose"}
{"code": "def linear_gaussian_update ( prior_mean , prior_cov , observation_matrix , observation_noise , x_observed ) : observation_size_is_static_and_scalar = ( tf . compat . dimension_value ( observation_matrix . shape [ - 2 ] ) == 1 ) x_expected = <mask> ( prior_mean , observation_matrix , observation_noise ) tmp_obs_cov = observation_matrix . matmul ( prior_cov ) predicted_obs_cov = ( observation_matrix . matmul ( tmp_obs_cov , adjoint_arg = True ) + observation_noise . covariance ( ) ) if observation_size_is_static_and_scalar : gain_transpose = tmp_obs_cov / predicted_obs_cov else : predicted_obs_cov_chol = tf . linalg . cholesky ( predicted_obs_cov ) gain_transpose = tf . linalg . cholesky_solve ( predicted_obs_cov_chol , tmp_obs_cov ) posterior_mean = ( prior_mean + tf . linalg . matmul ( gain_transpose , x_observed - x_expected , adjoint_a = True ) ) tmp_term = - observation_matrix . matmul ( gain_transpose , adjoint = True ) tmp_term = tf . linalg . set_diag ( tmp_term , tf . linalg . diag_part ( tmp_term ) + 1 ) posterior_cov = ( tf . linalg . matmul ( tmp_term , tf . linalg . matmul ( prior_cov , tmp_term ) , adjoint_a = True ) + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( observation_noise . covariance ( ) , gain_transpose ) , adjoint_a = True ) ) if observation_size_is_static_and_scalar : predictive_dist = independent . Independent ( normal . Normal ( loc = x_expected [ ... , 0 ] , scale = tf . sqrt ( predicted_obs_cov [ ... , 0 ] ) ) , reinterpreted_batch_ndims = 1 ) predictive_dist . covariance = lambda : predicted_obs_cov else : predictive_dist = mvn_tril . MultivariateNormalTriL ( loc = x_expected [ ... , 0 ] , scale_tril = predicted_obs_cov_chol ) return posterior_mean , posterior_cov , predictive_dist", "predicted": "lambda", "expected": "_propagate_mean"}
{"code": "def build_kalman_mean_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : def mean_step ( <mask> , t ) : previous_latent_mean , _ = previous_means latent_mean = _propagate_mean ( previous_latent_mean , get_transition_matrix_for_timestep ( t - 1 ) , get_transition_noise_for_timestep ( t - 1 ) ) observation_mean = _propagate_mean ( latent_mean , get_observation_matrix_for_timestep ( t ) , get_observation_noise_for_timestep ( t ) ) return ( latent_mean , observation_mean ) return mean_step", "predicted": "t", "expected": "previous_means"}
{"code": "def posterior_marginals ( self , x , mask = None ) : with tf . name_scope ( ) : x = tf . convert_to_tensor ( value = x , name = ) ( <mask> , filtered_means , filtered_covs , predicted_means , predicted_covs , _ , _ ) = self . forward_filter ( x , mask = mask ) ( smoothed_means , smoothed_covs ) = self . backward_smoothing_pass ( filtered_means , filtered_covs , predicted_means , predicted_covs ) return ( smoothed_means , smoothed_covs )", "predicted": "value", "expected": "_"}
{"code": "def _joint_mean ( self ) : with tf . name_scope ( ) : with tf . control_dependencies ( self . runtime_assertions ) : initial_latent_mean = _broadcast_to_shape ( self . initial_state_prior . mean ( ) [ ... , tf . newaxis ] , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , 1 ] ] , axis = 0 ) ) initial_observation_mean = _propagate_mean ( initial_latent_mean , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) mean_step = build_kalman_mean_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) ( <mask> , observation_means ) = tf . scan ( mean_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent_mean , initial_observation_mean ) ) latent_means = tf . concat ( [ initial_latent_mean [ tf . newaxis , ... ] , latent_means ] , axis = 0 ) observation_means = tf . concat ( [ initial_observation_mean [ tf . newaxis , ... ] , observation_means ] , axis = 0 ) latent_means = tf . squeeze ( latent_means , - 1 ) latent_means = distribution_util . move_dimension ( latent_means , 0 , - 2 ) observation_means = tf . squeeze ( observation_means , - 1 ) observation_means = distribution_util . move_dimension ( observation_means , 0 , - 2 ) return latent_means , observation_means", "predicted": "tf", "expected": "latent_means"}
{"code": "def _joint_covariances ( self ) : with tf . name_scope ( ) : with tf . control_dependencies ( self . runtime_assertions ) : <mask> = _broadcast_to_shape ( self . initial_state_prior . covariance ( ) , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , self . latent_size ] ] , axis = 0 ) ) initial_observation_cov = _propagate_cov ( initial_latent_cov , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) cov_step = build_kalman_cov_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) ( latent_covs , observation_covs ) = tf . scan ( cov_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent_cov , initial_observation_cov ) ) latent_covs = tf . concat ( [ initial_latent_cov [ tf . newaxis , ... ] , latent_covs ] , axis = 0 ) observation_covs = tf . concat ( [ initial_observation_cov [ tf . newaxis , ... ] , observation_covs ] , axis = 0 ) latent_covs = distribution_util . move_dimension ( latent_covs , 0 , - 3 ) observation_covs = distribution_util . move_dimension ( observation_covs , 0 , - 3 ) return latent_covs , observation_covs", "predicted": "tf", "expected": "initial_latent_cov"}
{"code": "def latents_to_observations ( self , latent_means , latent_covs ) : with tf . name_scope ( ) : pushforward_latents_step = build_pushforward_latents_step ( self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) latent_means = distribution_util . move_dimension ( latent_means , source_idx = - 2 , dest_idx = 0 ) latent_means = latent_means [ ... , tf . newaxis ] latent_covs = distribution_util . move_dimension ( latent_covs , source_idx = - 3 , dest_idx = 0 ) ( initial_observation_mean , initial_observation_cov ) = pushforward_latents_step ( _ = None , latent_t_mean_cov = ( self . <mask> , latent_means [ self . initial_step ] , latent_covs [ self . initial_step ] ) ) timesteps = tf . range ( self . initial_step , self . initial_step + self . num_timesteps ) observation_means , observation_covs = tf . scan ( pushforward_latents_step , elems = ( timesteps , latent_means , latent_covs ) , initializer = ( initial_observation_mean , initial_observation_cov ) , parallel_iterations = 10000 ) observation_means = distribution_util . move_dimension ( observation_means [ ... , 0 ] , source_idx = 0 , dest_idx = - 2 ) observation_covs = distribution_util . move_dimension ( observation_covs , source_idx = 0 , dest_idx = - 3 ) return observation_means , observation_covs", "predicted": "None", "expected": "initial_step"}
{"code": "def _mode ( self ) : return ( self . <mask> + tf . zeros_like ( self . concentration ) [ ... , tf . newaxis ] )", "predicted": "concentration", "expected": "mean_direction"}
{"code": "def _sample_3d ( self , n , seed = None ) : seed = seed_stream <mask> SeedStream ( seed , salt = 'von_mises_fisher_3d' ) u_shape = tf . concat ( [ [ n ] , self . _batch_shape_tensor ( ) ] , axis = 0 ) z = tf . random . uniform ( u_shape , seed = seed ( ) , dtype = self . dtype ) safe_conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones_like ( self . concentration ) ) safe_z = tf . where ( z > 0 , z , tf . ones_like ( z ) ) safe_u = 1 + tf . reduce_logsumexp ( input_tensor = [ tf . math . log ( safe_z ) , tf . math . log1p ( - safe_z ) - 2 * safe_conc ] , axis = 0 ) / safe_conc u = tf . where ( self . concentration > tf . zeros_like ( safe_u ) , safe_u , 2 * z - 1 ) u = tf . where ( tf . equal ( z , 0 ) , - tf . ones_like ( u ) , u ) if not self . _allow_nan_stats : u = tf . debugging . check_numerics ( u , 'u in _sample_3d' ) return u [ ... , tf . newaxis ]", "predicted": ":", "expected": "."}
{"code": "def is_namedtuple_like ( x ) : try : for fn in x . _fields : _ = getattr ( x , fn ) return True except <mask> : return False", "predicted": "TypeError", "expected": "AttributeError"}
{"code": "def make_innermost_setter ( setter ) : @ functools . wraps ( setter ) def _new_setter ( <mask> , * args , ** kwargs ) : results_stack = [ ] while hasattr ( kernel_results , 'inner_results' ) : results_stack . append ( kernel_results ) kernel_results = kernel_results . inner_results new_kernel_results = setter ( kernel_results , * args , ** kwargs ) for outer_results in reversed ( results_stack ) : new_kernel_results = outer_results . _replace ( inner_results = new_kernel_results ) return new_kernel_results return _new_setter", "predicted": "setter", "expected": "kernel_results"}
{"code": "def _replace_event_shape_in_shape_tensor ( input_shape , event_shape_in , event_shape_out , validate_args ) : output_tensorshape , is_validated = _replace_event_shape_in_tensorshape ( tensorshape_util . constant_value_as_shape ( input_shape ) , event_shape_in , event_shape_out ) validation_dependencies = ( map ( tf . identity , ( event_shape_in , event_shape_out ) ) if validate_args else ( ) ) if ( tensorshape_util . is_fully_defined ( output_tensorshape ) and ( is_validated or not validate_args ) ) : with tf . control_dependencies ( validation_dependencies ) : output_shape = tf . convert_to_tensor ( value = output_tensorshape , name = 'output_shape' , dtype_hint = tf . int32 ) return output_shape , output_tensorshape with tf . control_dependencies ( validation_dependencies ) : event_shape_in_ndims = ( tf . size ( input = event_shape_in ) if tensorshape_util . num_elements ( event_shape_in . shape ) is None else tensorshape_util . num_elements ( event_shape_in . shape ) ) input_non_event_shape , <mask> = tf . split ( input_shape , num_or_size_splits = [ - 1 , event_shape_in_ndims ] ) additional_assertions = [ ] if is_validated : pass elif validate_args : mask = event_shape_in >= 0 explicit_input_event_shape = tf . boolean_mask ( tensor = input_event_shape , mask = mask ) explicit_event_shape_in = tf . boolean_mask ( tensor = event_shape_in , mask = mask ) additional_assertions . append ( assert_util . assert_equal ( explicit_input_event_shape , explicit_event_shape_in , message = 'Input `event_shape` does not match `event_shape_in`.' ) ) with tf . control_dependencies ( additional_assertions ) : output_shape = tf . concat ( [ input_non_event_shape , event_shape_out ] , axis = 0 , name = 'output_shape' ) return output_shape , output_tensorshape", "predicted": "mask", "expected": "input_event_shape"}
{"code": "def get_initial_state_args ( value_and_gradients_function , initial_position , grad_tolerance , control_inputs = None ) : if control_inputs : with tf . control_dependencies ( control_inputs ) : f0 , df0 = value_and_gradients_function ( initial_position ) else : f0 , df0 = value_and_gradients_function ( initial_position ) <mask> = norm ( df0 , dims = 1 ) < grad_tolerance return dict ( converged = converged , failed = tf . zeros_like ( converged ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , num_objective_evaluations = tf . convert_to_tensor ( value = 1 ) , position = initial_position , objective_value = f0 , objective_gradient = df0 )", "predicted": "norm", "expected": "converged"}
{"code": "def _update_position ( state , position_delta , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : failed = state . failed | ~ tf . math . is_finite ( next_objective ) | ~ tf . reduce_all ( input_tensor = tf . math . is_finite ( next_gradient ) , axis = - 1 ) next_position = state . position + position_delta converged = ~ failed & _check_convergence ( state . position , next_position , state . objective_value , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) <mask> update_fields ( state , converged = state . converged | converged , failed = failed , position = next_position , objective_value = next_objective , objective_gradient = next_gradient )", "predicted": ":", "expected": "return"}
{"code": "def _check_convergence ( <mask> , next_position , current_objective , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : grad_converged = norm ( next_gradient , dims = 1 ) <= grad_tolerance x_converged = norm ( next_position - current_position , dims = 1 ) <= x_tolerance f_converged = ( norm ( next_objective - current_objective , dims = 0 ) <= f_relative_tolerance * current_objective ) return grad_converged | x_converged | f_converged", "predicted": "norm", "expected": "current_position"}
{"code": "def _broadcast ( value , target ) <mask> return tf . broadcast_to ( tf . convert_to_tensor ( value = value , dtype = target . dtype ) , distribution_util . prefer_static_shape ( target ) [ : - 1 ] )", "predicted": ")", "expected": ":"}
{"code": "def _get_field ( <mask> , field_name ) : if hasattr ( kernel_results , field_name ) : return getattr ( kernel_results , field_name ) if hasattr ( kernel_results , 'accepted_results' ) : return getattr ( kernel_results . accepted_results , field_name ) raise TypeError ( 'Cannot extract %s from %s' % ( field_name , kernel_results ) )", "predicted": "name", "expected": "kernel_results"}
{"code": "def one_step ( self , current_state , previous_kernel_results ) : with <mask> . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'transformed_kernel' , 'one_step' ) , values = [ previous_kernel_results ] ) : transformed_next_state , kernel_results = self . _inner_kernel . one_step ( previous_kernel_results . transformed_state , previous_kernel_results . inner_results ) transformed_next_state_parts = ( transformed_next_state if mcmc_util . is_list_like ( transformed_next_state ) else [ transformed_next_state ] ) next_state_parts = self . _forward_transform ( transformed_next_state_parts ) next_state = ( next_state_parts if mcmc_util . is_list_like ( transformed_next_state ) else next_state_parts [ 0 ] ) kernel_results = TransformedTransitionKernelResults ( transformed_state = transformed_next_state , inner_results = kernel_results ) return next_state , kernel_results", "predicted": "compat", "expected": "tf"}
{"code": "def val_where ( cond , tval , fval ) : if isinstance ( tval , tf <mask> Tensor ) : return tf . where ( cond , tval , fval ) elif isinstance ( tval , tuple ) : cls = type ( tval ) return cls ( * ( val_where ( cond , t , f ) for t , f in zip ( tval , fval ) ) ) else : raise Exception ( TypeError )", "predicted": ",", "expected": "."}
{"code": "def _secant2_inner_update ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : new_failed = initial_args . active & ~ is_finite ( val_c ) active = initial_args . active & ~ new_failed failed = initial_args . failed | new_failed found_wolfe = active & _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) <mask> = val_where ( found_wolfe , val_c , initial_args . left ) val_right = val_where ( found_wolfe , val_c , initial_args . right ) converged = initial_args . converged | found_wolfe active = active & ~ found_wolfe def _apply_update ( ) : update_result = update ( value_and_gradients_function , val_left , val_right , val_c , f_lim , active = active ) return _Secant2Result ( active = tf . zeros_like ( active ) , converged = converged , failed = failed | update_result . failed , num_evals = initial_args . num_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) def _default ( ) : return _Secant2Result ( active = active , converged = converged , failed = failed , num_evals = initial_args . num_evals , left = val_left , right = val_right ) return prefer_static . cond ( tf . reduce_any ( input_tensor = active ) , _apply_update , _default )", "predicted": "active", "expected": "val_left"}
{"code": "def bisect ( value_and_gradients_function , initial_left , initial_right , f_lim ) : <mask> = ~ is_finite ( initial_left , initial_right ) needs_bisect = ( initial_right . df < 0 ) & ( initial_right . f > f_lim ) bisect_args = _IntermediateResult ( iteration = tf . convert_to_tensor ( value = 0 ) , stopped = failed | ~ needs_bisect , failed = failed , num_evals = tf . convert_to_tensor ( value = 0 ) , left = initial_left , right = initial_right ) return _bisect ( value_and_gradients_function , bisect_args , f_lim )", "predicted": "bisect", "expected": "failed"}
{"code": "def _bisect ( value_and_gradients_function , initial_args , f_lim ) : def _loop_cond ( curr ) : return ~ tf . reduce_all ( input_tensor = curr . stopped ) def _loop_body ( curr ) : mid = value_and_gradients_function ( ( curr . left . x + curr . right . x ) / 2 ) failed = ( curr . failed | ~ is_finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) to_update = ~ ( curr . stopped | failed ) update_left = ( mid . df < 0 ) <mask> ( mid . f <= f_lim ) left = val_where ( to_update & update_left , mid , curr . left ) right = val_where ( to_update & ~ update_left , mid , curr . right ) stopped = curr . stopped | failed | ( right . df >= 0 ) return [ _IntermediateResult ( iteration = curr . iteration , stopped = stopped , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ]", "predicted": "|", "expected": "&"}
{"code": "def is_finite ( val_1 , <mask> = None ) : val_1_finite = tf . math . is_finite ( val_1 . f ) & tf . math . is_finite ( val_1 . df ) if val_2 is not None : return val_1_finite & tf . math . is_finite ( val_2 . f ) & tf . math . is_finite ( val_2 . df ) return val_1_finite", "predicted": "df", "expected": "val_2"}
{"code": "def _resnet_block ( <mask> , filters , kernel , stride , kernel_posterior_fn ) : x = tf . keras . layers . BatchNormalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) if stride != 1 or filters != x . shape [ 1 ] : shortcut = _projection_shortcut ( x , filters , stride , kernel_posterior_fn ) else : shortcut = x x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = stride , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) x = tf . keras . layers . BatchNormalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = 1 , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) x = tf . keras . layers . add ( [ x , shortcut ] ) return x", "predicted": "tf", "expected": "x"}
{"code": "def make_encoder ( activation , num_topics , layer_sizes ) : encoder_net = tf . keras . Sequential ( ) for num_hidden_units <mask> layer_sizes : encoder_net . add ( tf . keras . layers . Dense ( num_hidden_units , activation = activation , kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) ) encoder_net . add ( tf . keras . layers . Dense ( num_topics , activation = tf . nn . softplus , kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) ) def encoder ( bag_of_words ) : net = _clip_dirichlet_parameters ( encoder_net ( bag_of_words ) ) return tfd . Dirichlet ( concentration = net , name = ) return encoder", "predicted": ",", "expected": "in"}
{"code": "def make_decoder ( num_topics , num_words ) : topics_words_logits = tf . compat . v1 . get_variable ( , shape = [ num_topics , num_words ] , initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) topics_words = tf . nn . softmax ( topics_words_logits , axis = - 1 ) def decoder ( topics ) : word_probs = tf . matmul ( topics , topics_words ) return <mask> . OneHotCategorical ( probs = word_probs , name = ) return decoder , topics_words", "predicted": "tf", "expected": "tfd"}
{"code": "def make_prior ( num_topics , initial_value ) : def _softplus_inverse ( x ) : return np . log ( np . expm1 ( x ) ) logit_concentration = tf . compat . <mask> . get_variable ( , shape = [ 1 , num_topics ] , initializer = tf . compat . v1 . initializers . constant ( _softplus_inverse ( initial_value ) ) ) concentration = _clip_dirichlet_parameters ( tf . nn . softplus ( logit_concentration ) ) def prior ( ) : return tfd . Dirichlet ( concentration = concentration , name = ) prior_variables = [ logit_concentration ] return prior , prior_variables", "predicted": "tf", "expected": "v1"}
{"code": "def _registered_kl ( type_a , type_b ) : hierarchy_a = tf_inspect . getmro ( type_a ) hierarchy_b = tf_inspect . getmro ( type_b ) dist_to_children = None kl_fn = None for mro_to_a , parent_a in enumerate ( hierarchy_a ) : for mro_to_b , parent_b in enumerate ( hierarchy_b ) : candidate_dist = mro_to_a + mro_to_b <mask> = _DIVERGENCES . get ( ( parent_a , parent_b ) , None ) if not kl_fn or ( candidate_kl_fn and candidate_dist < dist_to_children ) : dist_to_children = candidate_dist kl_fn = candidate_kl_fn return kl_fn", "predicted": "", "expected": "candidate_kl_fn"}
{"code": "def create_random_seq ( character , action_metadata , direction , length = 8 ) : start = tf . random . uniform ( [ ] , <mask> = action_metadata [ 1 ] , dtype = tf . int32 ) return create_seq ( character , action_metadata , direction , length , start )", "predicted": "start", "expected": "maxval"}
{"code": "def _maybe_validate_distributions ( distributions , dtype_override , validate_args ) : assertions = [ ] if not _is_iterable ( distributions ) or not distributions : raise ValueError ( '`distributions` must be a list of one or more ' 'distributions.' ) if dtype_override is None : dts = [ dtype_util . base_dtype ( d . dtype ) for d in distributions if d . dtype is not None ] if dts [ 1 : ] != dts [ : - 1 ] : raise TypeError ( 'Distributions must have same dtype; found: {}.' . format ( set ( dtype_util . name ( dt ) for dt in dts ) ) ) for d in distributions : if tensorshape_util . rank ( d . event_shape ) is not None : if tensorshape_util . rank ( d . event_shape ) != 1 : raise ValueError ( '`Distribution` must be vector variate, ' 'found event nimds: {}.' . format ( tensorshape_util . rank ( d . event_shape ) ) ) elif validate_args : assertions . append ( assert_util . assert_equal ( 1 , tf . size ( <mask> = d . event_shape_tensor ( ) ) , message = '`Distribution` must be vector variate.' ) ) batch_shapes = [ d . batch_shape for d in distributions ] if all ( tensorshape_util . is_fully_defined ( b ) for b in batch_shapes ) : if batch_shapes [ 1 : ] != batch_shapes [ : - 1 ] : raise ValueError ( 'Distributions must have the same `batch_shape`; ' 'found: {}.' . format ( batch_shapes ) ) elif validate_args : batch_shapes = [ tensorshape_util . as_list ( d . batch_shape ) if tensorshape_util . is_fully_defined ( d . batch_shape ) else d . batch_shape_tensor ( ) for d in distributions ] assertions . extend ( assert_util . assert_equal ( b1 , b2 , message = 'Distribution `batch_shape`s must be identical.' ) for b1 , b2 in zip ( batch_shapes [ 1 : ] , batch_shapes [ : - 1 ] ) ) return assertions", "predicted": "d", "expected": "input"}
{"code": "def histogram ( x , edges , axis = None , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'histogram' , values = [ x , edges , axis ] ) : in_dtype = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_dtype ) edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_dtype ) if axis is None : x = tf . reshape ( x , shape = [ - 1 ] ) else : x_ndims = _get_static_ndims ( x , expect_static = True , expect_ndims_at_least = 1 ) axis = _make_static_axis_non_negative_list ( axis , x_ndims ) if not axis : raise ValueError ( '`axis` cannot be empty.  Found: {}' . format ( axis ) ) x = _move_dims_to_flat_end ( x , axis , x_ndims , right_end = False ) bins = find_bins ( x , edges = edges , extend_lower_interval = extend_lower_interval , extend_upper_interval = extend_upper_interval , dtype = tf . int32 ) counts = count_integers ( bins , minlength = tf . shape ( input = edges ) [ 0 ] - 1 , <mask> = tf . shape ( input = edges ) [ 0 ] - 1 , axis = 0 , dtype = dtype or in_dtype ) n_edges = tf . compat . dimension_value ( edges . shape [ 0 ] ) if n_edges is not None : counts . set_shape ( tf . TensorShape ( [ n_edges - 1 ] ) . concatenate ( counts . shape [ 1 : ] ) ) return counts", "predicted": "shape", "expected": "maxlength"}
{"code": "def amari_alpha ( logu , alpha = 1. , self_normalized = False , name = None ) : with tf . compat . v1 . <mask> ( name , , [ logu ] ) : if alpha is None or tf . is_tensor ( alpha ) : raise TypeError ( ) if ( self_normalized is None or tf . is_tensor ( self_normalized ) ) : raise TypeError ( ) logu = tf . convert_to_tensor ( value = logu , name = ) if alpha == 0. : f = - logu elif alpha == 1. : f = tf . exp ( logu ) * logu else : f = tf . math . expm1 ( alpha * logu ) / ( alpha * ( alpha - 1. ) ) if not self_normalized : return f if alpha == 0. : return f + tf . math . expm1 ( logu ) elif alpha == 1. : return f - tf . math . expm1 ( logu ) else : return f - tf . math . expm1 ( logu ) / ( alpha - 1. )", "predicted": "alpha", "expected": "name_scope"}
{"code": "def kl_reverse ( logu , <mask> = False , name = None ) : with tf . compat . v1 . name_scope ( name , , [ logu ] ) : return amari_alpha ( logu , alpha = 0. , self_normalized = self_normalized )", "predicted": "normalized", "expected": "self_normalized"}
{"code": "def jensen_shannon ( logu , self_normalized = False , name = None ) <mask> with tf . compat . v1 . name_scope ( name , , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ) npdt = logu . dtype . as_numpy_dtype y = tf . nn . softplus ( logu ) if self_normalized : y -= np . log ( 2 ) . astype ( npdt ) return tf . exp ( logu ) * logu - ( 1. + tf . exp ( logu ) ) * y", "predicted": "#", "expected": ":"}
